<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>大数据 | 阿花花花deCSNotes</title><meta name="author" content="JH"><meta name="copyright" content="JH"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="大数据概述大数据概念和影响大数据特点： 大量化（volume）：大数据摩尔定律 快速化（velocity）：数据处理速度快 多样化（variety）：大数据由结构化和非结构化数据组成 价值密度低（value）  大数据影响四种数据范式：实验、理论、计算、数据 在思维方式方面，大数据完全颠覆了传统的思维方式： 全样而非抽样（抽取数据计算分析）众多服务器的集群出现使得不需要抽样 效率而非精确：抽样分析">
<meta property="og:type" content="article">
<meta property="og:title" content="大数据">
<meta property="og:url" content="https://www.gtxhjh.cn/2021/01/28/BigData/bigdata/index.html">
<meta property="og:site_name" content="阿花花花deCSNotes">
<meta property="og:description" content="大数据概述大数据概念和影响大数据特点： 大量化（volume）：大数据摩尔定律 快速化（velocity）：数据处理速度快 多样化（variety）：大数据由结构化和非结构化数据组成 价值密度低（value）  大数据影响四种数据范式：实验、理论、计算、数据 在思维方式方面，大数据完全颠覆了传统的思维方式： 全样而非抽样（抽取数据计算分析）众多服务器的集群出现使得不需要抽样 效率而非精确：抽样分析">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg">
<meta property="article:published_time" content="2021-01-28T05:46:30.000Z">
<meta property="article:modified_time" content="2021-02-03T08:03:04.507Z">
<meta property="article:author" content="JH">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://www.gtxhjh.cn/2021/01/28/BigData/bigdata/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2021-02-03 16:03:04'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    })(window)</script><meta name="generator" content="Hexo 5.3.0"><link rel="alternate" href="/atom.xml" title="阿花花花deCSNotes" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/img/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">22</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">16</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">阿花花花deCSNotes</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">大数据</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-01-28T05:46:30.000Z" title="发表于 2021-01-28 13:46:30">2021-01-28</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-02-03T08:03:04.507Z" title="更新于 2021-02-03 16:03:04">2021-02-03</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="大数据概述"><a href="#大数据概述" class="headerlink" title="大数据概述"></a>大数据概述</h1><h2 id="大数据概念和影响"><a href="#大数据概念和影响" class="headerlink" title="大数据概念和影响"></a>大数据概念和影响</h2><h3 id="大数据特点："><a href="#大数据特点：" class="headerlink" title="大数据特点："></a>大数据特点：</h3><ul>
<li>大量化（volume）：大数据摩尔定律</li>
<li>快速化（velocity）：数据处理速度快</li>
<li>多样化（variety）：大数据由结构化和非结构化数据组成</li>
<li>价值密度低（value）</li>
</ul>
<h3 id="大数据影响"><a href="#大数据影响" class="headerlink" title="大数据影响"></a>大数据影响</h3><p>四种数据范式：实验、理论、计算、数据</p>
<p>在思维方式方面，大数据完全颠覆了传统的思维方式：</p>
<p>全样而非抽样（抽取数据计算分析）众多服务器的集群出现使得不需要抽样</p>
<p>效率而非精确：抽样分析追求精确度是因为如果抽样精确度不高放在全样将被放大</p>
<p>相关而非因果：</p>
<h2 id="大数据关键技术"><a href="#大数据关键技术" class="headerlink" title="大数据关键技术"></a>大数据关键技术</h2><p>大数据技术层次：</p>
<ul>
<li>数据采集</li>
<li>数据存储与管理</li>
<li>数据处理与分析</li>
<li>数据隐私与安全</li>
</ul>
<blockquote>
<p>分布式存储：解决海量数据存储问题</p>
<p>分布式处理：解决海量数据处理问题</p>
</blockquote>
<p>不同的计算模式需要不同的产品如：</p>
<p>批处理：时效性要求无法满足</p>
<ul>
<li>MapReduce</li>
<li>Spark—时效性比mapreduce高，可以高效迭代计算，适合数据挖掘</li>
</ul>
<p>流计算：实时处理给出实时响应</p>
<p>图计算：高效处理图结构数据比如社交网络数据</p>
<p>查询分析计算：交互式查询，大数据查询分析软件</p>
<ul>
<li>Hive</li>
</ul>
<img src="images/image-20201024171750098.png" alt="image-20201024171750098" style="zoom:50%;">

<h2 id="大数据与云计算、物联网"><a href="#大数据与云计算、物联网" class="headerlink" title="大数据与云计算、物联网"></a>大数据与云计算、物联网</h2><h3 id="云计算"><a href="#云计算" class="headerlink" title="云计算"></a>云计算</h3><blockquote>
<p>通过网络以服务的方式为用户提供非常廉价的IT资源</p>
</blockquote>
<p>云计算解决两个核心问题：海量数据分布式存储和处理</p>
<p>云计算特征：虚拟化、多租户</p>
<ol>
<li>公有云：面向所有用户</li>
<li>混合云</li>
<li>私有云：面向企业内部</li>
</ol>
<p>云计算层次模型：</p>
<ul>
<li><p>基础设施层 IaaS：面向网络架构师</p>
<p>将基础设施作为服务出租</p>
</li>
<li><p>平台层PaaS：面向应用开发者，平台即服务</p>
<p>开发云计算产品，平台作为服务</p>
</li>
<li><p>应用层SaaS：面向用户，软件即服务</p>
<p>云财务软件</p>
</li>
</ul>
<h4 id="虚拟化技术"><a href="#虚拟化技术" class="headerlink" title="虚拟化技术"></a>虚拟化技术</h4><img src="images/image-20201024172609608.png" alt="image-20201024172609608" style="zoom:50%;">

<blockquote>
<p>VPN</p>
</blockquote>
<h4 id="云计算数据中心"><a href="#云计算数据中心" class="headerlink" title="云计算数据中心"></a>云计算数据中心</h4><h3 id="物联网-IoT"><a href="#物联网-IoT" class="headerlink" title="物联网 IoT"></a>物联网 IoT</h3><p>物物相连互联网</p>
<h4 id="层次架构"><a href="#层次架构" class="headerlink" title="层次架构"></a>层次架构</h4><img src="images/image-20201024173233515.png" alt="image-20201024173233515" style="zoom:50%;">

<h3 id="关系"><a href="#关系" class="headerlink" title="关系"></a>关系</h3><img src="images/image-20201024173652900.png" alt="image-20201024173652900" style="zoom:50%;">

<h1 id="大数据处理架构Hadoop"><a href="#大数据处理架构Hadoop" class="headerlink" title="大数据处理架构Hadoop"></a>大数据处理架构Hadoop</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><blockquote>
<p>分布式计算平台，java语言开发，跨平台，支持多种编程语言</p>
</blockquote>
<p><strong>两大核心：HDFS（分布式存储） + MapReduce（分布式处理）</strong></p>
<h3 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h3><ul>
<li>高可靠性：多台机器构成集群，部分机器发生故障，剩余机器可以继续对外提供服务</li>
<li>高效性</li>
<li>高可扩展性</li>
<li>成本低：采用普通pc机构成一个集群</li>
</ul>
<p>架构：</p>
 <img src="images/image-20201024175515267.png" alt="image-20201024175515267" style="zoom:50%;">

<h3 id="版本"><a href="#版本" class="headerlink" title="版本"></a>版本</h3><ul>
<li><p>1.0：</p>
<p><img src="images/image-20201024175758834.png" alt="image-20201024175758834"></p>
<p>mapreduce:需要数据处理分析以及集群资源调度</p>
<p>因此2.0把任务分解，使得mapreduce只进行数据处理，由yarn来进行资源调度</p>
</li>
<li><p>2.0</p>
<p>HDFS在1.0可扩展性并不好，因此2.0实现多个namenode分区管理</p>
<p><img src="images/image-20201024175918318.png" alt="image-20201024175918318"></p>
</li>
</ul>
<h2 id="各组件、功能"><a href="#各组件、功能" class="headerlink" title="各组件、功能"></a>各组件、功能</h2><h3 id="项目结构"><a href="#项目结构" class="headerlink" title="项目结构"></a>项目结构</h3><p>HDFS + MapReduce</p>
<p><img src="images/image-20201024180808668.png" alt="image-20201024180808668"></p>
<blockquote>
<p>HDFS：分布式文件存储；YARN：计算资源调度；MapReduce：离线计算，基于磁盘；Tez：MapReduce作业进行分析优化，构建有向无环图，实现高效；Spark：通用并行框架，基于内存计算，性能优于MapReduce；Hive：数据仓库功能，用于企业决策分析，可以把sql语句转成MapReduce作业；Pig：实现流数据处理，基于大规模数据分析平台，轻量级分析；Oozie：作业流调度系统，工作流管理；zookeeper：提供分布式协调一致性服务，协同工作系统，分布式锁；HBase：列族数据库，支持随机读写，非关系型分布式数据库；Flume：日志收集分析框架；Sqoop：数据导入导出，用于在Hadoop与传统数据库之间进行数据传递；Ambari：部署工具</p>
</blockquote>
<h1 id="分布式文件系统HDFS"><a href="#分布式文件系统HDFS" class="headerlink" title="分布式文件系统HDFS"></a>分布式文件系统HDFS</h1><h2 id="概述-1"><a href="#概述-1" class="headerlink" title="概述"></a>概述</h2><p>HDFS-Hadoop Distributed File System</p>
<blockquote>
<p>大数据时代，海量数据需要集群分布式存储</p>
</blockquote>
<p><strong>集群基本架构：</strong></p>
<img src="images/image-20201024211057840.png" alt="image-20201024211057840" style="zoom:50%;">

<blockquote>
<p>机架内部机器通过光纤高速交换机连接，机架与机架之间由带宽更高的光纤交换机连接</p>
</blockquote>
<p><strong>文件系统结构：</strong></p>
<p>文件分布式存储</p>
<img src="images/image-20201024211350548.png" alt="image-20201024211350548" style="zoom:50%;">

<h3 id="HDFS实现目标"><a href="#HDFS实现目标" class="headerlink" title="HDFS实现目标"></a>HDFS实现目标</h3><ul>
<li>兼容廉价的硬件设备（需要成本低，普通的pc机）</li>
<li>流数据读写（传统文件系统一般块为单位，HDFS是全部数据全部读写，批量处理）</li>
<li>支持大数据集</li>
<li>支持简单的文件模型（对文件简化，牺牲相关性能，获取批量处理特性，只允许追加不允许修改）</li>
<li>跨平台兼容性</li>
</ul>
<h3 id="局限性"><a href="#局限性" class="headerlink" title="局限性"></a>局限性</h3><ul>
<li>不适合低延迟数据访问（面向大规模数据流式读写，无法精确定位某个数据，不适合实时处理需求，而hbase可以）</li>
<li>无法高效储存大量小文件（使用元数据指引，其保存在的namenode中，内存中检索，需要建立索引结构，小文件多索引结构将变庞大从而耗时大）</li>
<li>不支持多用户写入及任意修改文件（只允许追加）</li>
</ul>
<h3 id="相关概念"><a href="#相关概念" class="headerlink" title="相关概念"></a>相关概念</h3><h4 id="块"><a href="#块" class="headerlink" title="块"></a>块</h4><p>为了分摊磁盘读写开销，即在大量数据间分摊磁盘寻址开销</p>
<p>但是HDFS的一个块比普通文件系统的块大很多，一般64MB甚至128MB</p>
<blockquote>
<p>这样可以支持面向大规模数据存储、降低分布式节点的寻址开销（三级寻址：元数据、数据节点、取数据）</p>
<p>缺点：块过大将导致mapreduce只有一两个任务执行，牺牲了并行度发挥不了分布式并行处理效果</p>
<p>好处：支持大规模文件存储；简化系统设计；适合数据备份</p>
</blockquote>
<h4 id="两大组件"><a href="#两大组件" class="headerlink" title="两大组件"></a>两大组件</h4><p>—名称节点namenode、 数据节点datanode</p>
<img src="images/image-20201024213142752.png" alt="image-20201024213142752" style="zoom:50%;">

<p>名称节点承担整个HDFS集群的管家任务</p>
<p>数据节点具体负责存储实际数据，存在磁盘中，本地的linux文件系统中</p>
<h5 id="元数据"><a href="#元数据" class="headerlink" title="元数据"></a>元数据</h5><ol>
<li>文件是什么</li>
<li>文件被分成多少块</li>
<li>块和文件的映射关系</li>
<li>每个块被存储在哪个服务器上</li>
</ol>
<h5 id="名称节点"><a href="#名称节点" class="headerlink" title="名称节点"></a>名称节点</h5><h6 id="FsImage"><a href="#FsImage" class="headerlink" title="FsImage"></a>FsImage</h6><p>保存系统文件树以及文件树中所有文件数据</p>
<p>文件复制等级、修改和访问时间、访问权限、块大小以及组成文件的块（不记录在哪块数据节点，而是单独维护）</p>
<blockquote>
<p> 数据节点会向管家汇报保存了什么数据块，即实时沟通维护保存在内存中</p>
</blockquote>
<h6 id="EditLog"><a href="#EditLog" class="headerlink" title="EditLog"></a>EditLog</h6><p>记录对数据进行的操作，也在内存中，规模小，操作效率高</p>
<h6 id="如何处理"><a href="#如何处理" class="headerlink" title="如何处理"></a>如何处理</h6><img src="images/image-20201024214455169.png" alt="image-20201024214455169" style="zoom:50%;">

<p>每次启动，后台将FsImage从底层磁盘读到内存，元数据信息都要保存到内存，与EditLog各项操作合并，FsImage记录历史数据结构信息，而修改由EditLog记录，合并后才能得到最新的元数据，从而得到新版的FsImage，然后创建一个空的EditLog</p>
<blockquote>
<p>因此每次更新用EditLog记录，如果直接修改FsImage将会运行很慢</p>
</blockquote>
<h6 id="第二名称节点"><a href="#第二名称节点" class="headerlink" title="第二名称节点"></a>第二名称节点</h6><p>对于不断的操作EditLog会不断增大，第二名称节点可以解决这个问题，当然它也有对名称节点冷备份的功能</p>
<img src="images/image-20201024214906025.png" alt="image-20201024214906025" style="zoom: 67%;">

<blockquote>
<p>EditLog会不断增大，第二名称节点定期和名称节点通信，某个阶段让名称节点停止使用EditLog文件，将EditLog、Fsimage存入自己机器，名称节点会建立新的Edits（记录维护期间操作），第二名称节点把editlog和fsimage导入本地后合并成新的fsimage再发送给名称节点，名称节点得到合并后的大的fsimage，然后把edit.new更改为editlog，维护期间editnew记录了维护期间操作，从而实现了edit fsimage合并也实现了冷备份效果</p>
</blockquote>
<h5 id="数据节点"><a href="#数据节点" class="headerlink" title="数据节点"></a>数据节点</h5><p>负责具体数据存储，每个数据节点数据保存到各自本地的linux文件系统中</p>
<h2 id="体系结构"><a href="#体系结构" class="headerlink" title="体系结构"></a>体系结构</h2><p>HDFS采用主从架构</p>
<h3 id="HDFS命名空间管理"><a href="#HDFS命名空间管理" class="headerlink" title="HDFS命名空间管理"></a>HDFS命名空间管理</h3><p>包含目录、文件、块</p>
<p>访问HDFS文件系统/ + 目录名称</p>
<h3 id="通信协议"><a href="#通信协议" class="headerlink" title="通信协议"></a>通信协议</h3><p>所有HDFS通信协议都构建在TCP/IP基础之上</p>
<p>客户端使用客户端协议和名称节点交互</p>
<p>名称节点和数据节点交互用数据节点协议</p>
<p>客户端和数据节点通过远程调用RPC实现</p>
<h3 id="局限性（1-0）"><a href="#局限性（1-0）" class="headerlink" title="局限性（1.0）"></a>局限性（1.0）</h3><ul>
<li>命名空间限制：名称节点保存在内存中，名称节点能够容纳对象个数受到空间大小限制</li>
<li>性能瓶颈：整个分布式文件吞吐量受限于单个名称节点吞吐量</li>
<li>隔离问题：由于集群中只有一个名称节点一个命名空间，因此无法对不同应用程序隔离</li>
<li>集群的可用性：一旦唯一的名称节点发生故障整个集群无法使用（第二名称节点是冷备份），单点故障（2.0解决，设置多个名称节点并且加入热备）</li>
</ul>
<h2 id="存储原理"><a href="#存储原理" class="headerlink" title="存储原理"></a>存储原理</h2><h3 id="冗余数据保存"><a href="#冗余数据保存" class="headerlink" title="冗余数据保存"></a>冗余数据保存</h3><p>廉价机器集群容易不断出现故障，那么需要冗余数据，每个数据以快为单位被冗余保存（默认保存3份）</p>
<p>伪分布时：冗余只能为1</p>
<p><strong>好处：</strong></p>
<ul>
<li>加快数据传输速度（因为多个客户端发起访问，可以实现并行操作）</li>
<li>容易检查数据错误（互为备份做参照）</li>
<li>保证数据可靠性</li>
</ul>
<h3 id="数据保存策略"><a href="#数据保存策略" class="headerlink" title="数据保存策略"></a>数据保存策略</h3><img src="images/image-20201024230045366.png" alt="image-20201024230045366" style="zoom:50%;">

<blockquote>
<p>每个节点都是廉价机器</p>
<p><strong>存储</strong>，多份副本的放置：</p>
<ol>
<li><p>第一份放上传文件的数据节点（不需要通过网络）</p>
<p>如果提交数据的请求不在集群内部，则随机挑选磁盘不满CPU不忙节点</p>
</li>
<li><p>第二份放与第一份所在机架不同的节点上</p>
</li>
<li><p>第三份放与第一份所在机架相同机架的不同节点上</p>
</li>
<li><p>后续多份随机</p>
</li>
</ol>
<p><strong>读取</strong>，<em>就近</em>读取，网络开销小</p>
<p>确定就近：</p>
<img src="images/image-20201024230608275.png" alt="image-20201024230608275" style="zoom:50%;">

<p>当客户端读取数据，从名称节点获得数据块不同副本的存放位置列表，列表中包含了副本所在数据节点，调用API确定节点所属机架ID，当发现数据块副本对应机架ID和客户端对应机架ID相同就优先选择该副本读取数据，否则随机选择</p>
</blockquote>
<h3 id="数据恢复"><a href="#数据恢复" class="headerlink" title="数据恢复"></a>数据恢复</h3><h4 id="名称节点的出错"><a href="#名称节点的出错" class="headerlink" title="名称节点的出错"></a>名称节点的出错</h4><p>名称节点会做冷备份，当出错时，整个HDFS实例将失效，出了问题将会暂停服务一段时间，从第二名称节点做恢复，恢复后再开始服务(1.0，2.0马上热备)</p>
<h4 id="数据节点的出错"><a href="#数据节点的出错" class="headerlink" title="数据节点的出错"></a>数据节点的出错</h4><p>数据节点具体负责存储相关数据，那么<strong>如何知道出错</strong>：因为数据节点会定期（远程调用）对名称节点发送**<em>心跳信息**</em>示意自己的状态（收不到则发生故障）。</p>
<p><strong>处理</strong>：当发生故障名称节点对其标记宕机，把存储在故障机上数据重新分发到可用机上，因为之前冗余备份过，所以可以实现重新分发（可以调整冗余数据位置，当负载不均衡时也可以调整）</p>
<h4 id="数据本身的出错"><a href="#数据本身的出错" class="headerlink" title="数据本身的出错"></a>数据本身的出错</h4><p>数据块存储到不同服务器上会出现磁盘损坏的情况，导致数据错误</p>
<p><strong>如何知道</strong>：使用校验码，客户端读取数据后对数据进行校验码校验，校验码是在文件被创建时客户端写文件时为数据块分配的，读出数据后进行校验码计算，从而进行比较</p>
<p>出错后进行冗余数据的恢复</p>
<h2 id="数据读写"><a href="#数据读写" class="headerlink" title="数据读写"></a>数据读写</h2><h3 id="读过程"><a href="#读过程" class="headerlink" title="读过程"></a>读过程</h3><ul>
<li>打开文件</li>
</ul>
<img src="images/image-20201024233012593.png" alt="image-20201024233012593" style="zoom:50%;">

<ul>
<li>获取数据块信息</li>
</ul>
<img src="images/image-20201024233153701.png" alt="image-20201024233153701" style="zoom:50%;">

<blockquote>
<p>DFSInputStream询问名称节点，通过接口，名称节点返回信息</p>
</blockquote>
<ul>
<li>读取请求</li>
</ul>
<p>read函数读取，读后关闭与数据节点的连接</p>
<ul>
<li>获取数据块信息(可能发生)</li>
</ul>
<p>数据节点读后可能有剩余数据没有读完，因此可能还需要获取下一个数据的存储信息</p>
<img src="images/image-20201024233441535.png" alt="image-20201024233441535" style="zoom:50%;">

<ul>
<li><p>再次读取数据</p>
</li>
<li><p>关闭文件</p>
</li>
</ul>
<p>循环读完后close关闭</p>
<h3 id="写过程"><a href="#写过程" class="headerlink" title="写过程"></a>写过程</h3><img src="images/image-20201024233734809.png" alt="image-20201024233734809" style="zoom:50%;">

<ul>
<li><p>创建文件请求</p>
<p>DFSOutputStream与名称节点交流</p>
</li>
<li><p>创建文件元数据</p>
<p>询问名称节点文件写在什么数据节点</p>
<p>DFSOutputStream执行RPC远程调用，让名称节点在文件系统命名空间新建文件，名称节点检查文件是否存在以及权限检查，然后创建文件</p>
</li>
<li><p>写入数据、写入数据包</p>
<p>通过输出流，高效的方式是流水线复制：把数据分包，分包放入输出流的内部队列，放入后输出流会向名称节点申请数据节点。<strong>数据节点构成数据流管道</strong>，内部队列的分包会打包为数据包发送到整个数据流管道第一个节点，由第一个节点发给下一个节点，以此类推，形成流水线</p>
<p><img src="images/image-20201024234401842.png" alt="image-20201024234401842"></p>
</li>
<li><p>接受确认包</p>
<p>由最后一个数据节点开始往前传</p>
<p><img src="images/image-20201024234428900.png" alt="image-20201024234428900"></p>
</li>
<li><p>关闭文件、写操作完成</p>
</li>
</ul>
<h2 id="HDFS常用命令"><a href="#HDFS常用命令" class="headerlink" title="HDFS常用命令"></a>HDFS常用命令</h2><p>启动: ./sbin/start-dfs.sh</p>
<p>hadoop dfs / hadoop fd / ./bin/hdfs dfs </p>
<p>停止:./sbin/stop-dfs.sh</p>
<ul>
<li>fs支持命令</li>
</ul>
<img src="images/image-20201025121543860.png" alt="image-20201025121543860" style="zoom:50%;">

<ul>
<li><p>首次使用HDFS，创建用户目录</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ./bin/hdfs dfs -mkdir -p /user/hadoop</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>该命令表示中HDFS中创建一个目录，“-p”表示如果是多级目录，则父目录和子目录一起创建，这里“/user/hadoop”就是一个多级目录，因此必须使用参数“-p”，否则会出错</p>
</blockquote>
</li>
<li><p>创建、删除都和linux命令差不多，需要注意的是地址，如果是”.”表示的是上面创建的目录，如果是“/”表示HDFS根目录</p>
</li>
</ul>
<h3 id="文件操作"><a href="#文件操作" class="headerlink" title="文件操作"></a>文件操作</h3><p>从本地文件系统向HDFS中上传文件，或者把HDFS中的文件下载到本地文件系统</p>
<ul>
<li><p>将本地文件上传到HDFS指定目录-put</p>
<img src="images/image-20201025123017558.png" alt="image-20201025123017558" style="zoom:50%;">
</li>
<li><p>将HDFS文件下载到本地-get</p>
<img src="images/image-20201025123241340.png" alt="image-20201025123241340" style="zoom:50%;">
</li>
<li><p>对于把HDFS目录下一个文件拷贝到另一个目录，操作与linux查不多，加入前缀hadoop -dfs即可</p>
</li>
</ul>
<h3 id="利用Web界面管理HDFS"><a href="#利用Web界面管理HDFS" class="headerlink" title="利用Web界面管理HDFS"></a>利用Web界面管理HDFS</h3><blockquote>
<p><a target="_blank" rel="noopener" href="http://localhost:9870/dfshealth.html#tab-overview">http://localhost:9870/dfshealth.html#tab-overview</a></p>
</blockquote>
<h2 id="HDFS常用JAVA-API"><a href="#HDFS常用JAVA-API" class="headerlink" title="HDFS常用JAVA API"></a>HDFS常用JAVA API</h2><p>Hadoop API文档</p>
<p><a target="_blank" rel="noopener" href="http://hadoop.apache.org/docs/stable/api/">http://hadoop.apache.org/docs/stable/api/</a></p>
<h3 id="应用程序的部署"><a href="#应用程序的部署" class="headerlink" title="应用程序的部署"></a>应用程序的部署</h3><p>把Java应用程序生成JAR包，部署到Hadoop平台上运行</p>
<p>export -&gt; java -&gt; runnable jar file</p>
<ul>
<li>launch configuration:生成jar包被部署启动时运行的主类</li>
<li>export destination:jar包输出目录</li>
</ul>
<p><img src="images/image-20201025184313219.png" alt="image-20201025184313219"></p>
<p><strong>执行：</strong></p>
<p>使用hadoop jar运行程序</p>
<h3 id="练习"><a href="#练习" class="headerlink" title="练习"></a>练习</h3><h4 id="写入文件"><a href="#写入文件" class="headerlink" title="写入文件"></a>写入文件</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;  </span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FSDataOutputStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">write</span> </span>&#123;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123; </span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">                Configuration conf = <span class="keyword">new</span> Configuration();  </span><br><span class="line">                conf.set(<span class="string">&quot;fs.defaultFS&quot;</span>,<span class="string">&quot;hdfs://localhost:9000&quot;</span>);</span><br><span class="line">                conf.set(<span class="string">&quot;fs.hdfs.impl&quot;</span>,<span class="string">&quot;org.apache.hadoop.hdfs.DistributedFileSystem&quot;</span>);</span><br><span class="line">                FileSystem fs = FileSystem.get(conf);</span><br><span class="line">                <span class="keyword">byte</span>[] buff = <span class="string">&quot;Hello world&quot;</span>.getBytes(); <span class="comment">// 要写入的内容</span></span><br><span class="line">                String filename = <span class="string">&quot;test&quot;</span>; <span class="comment">//要写入的文件名</span></span><br><span class="line">                FSDataOutputStream os = fs.create(<span class="keyword">new</span> Path(filename));</span><br><span class="line">                os.write(buff,<span class="number">0</span>,buff.length);</span><br><span class="line">                System.out.println(<span class="string">&quot;Create:&quot;</span>+ filename);</span><br><span class="line">                os.close();</span><br><span class="line">                fs.close();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;  </span><br><span class="line">        		System.out.println(<span class="string">&quot;failed&quot;</span>);</span><br><span class="line">                e.printStackTrace();  </span><br><span class="line">        &#125;  </span><br><span class="line">	&#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>result:</p>
<img src="images/image-20201025185627646.png" alt="image-20201025185627646" style="zoom:50%;">

<h4 id="判断文件是否存在"><a href="#判断文件是否存在" class="headerlink" title="判断文件是否存在"></a>判断文件是否存在</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">isexistthefile</span> </span>&#123;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">                String filename = <span class="string">&quot;test&quot;</span>;</span><br><span class="line"></span><br><span class="line">                Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">                conf.set(<span class="string">&quot;fs.defaultFS&quot;</span>,<span class="string">&quot;hdfs://localhost:9000&quot;</span>);</span><br><span class="line">                conf.set(<span class="string">&quot;fs.hdfs.impl&quot;</span>,<span class="string">&quot;org.apache.hadoop.hdfs.DistributedFileSystem&quot;</span>);</span><br><span class="line">                FileSystem fs = FileSystem.get(conf);</span><br><span class="line">                <span class="keyword">if</span>(fs.exists(<span class="keyword">new</span> Path(filename)))&#123;</span><br><span class="line">                        System.out.println(<span class="string">&quot;文件存在&quot;</span>);</span><br><span class="line">                &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                        System.out.println(<span class="string">&quot;文件不存在&quot;</span>);</span><br><span class="line">                &#125;</span><br><span class="line">                fs.close();</span><br><span class="line">	    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">	            e.printStackTrace();</span><br><span class="line">	    &#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h4 id="读取文件"><a href="#读取文件" class="headerlink" title="读取文件"></a>读取文件</h4><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.BufferedReader;</span><br><span class="line"><span class="keyword">import</span> java.io.InputStreamReader;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FSDataInputStream;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">read</span> </span>&#123;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">                Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">                conf.set(<span class="string">&quot;fs.defaultFS&quot;</span>,<span class="string">&quot;hdfs://localhost:9000&quot;</span>);</span><br><span class="line">                conf.set(<span class="string">&quot;fs.hdfs.impl&quot;</span>,<span class="string">&quot;org.apache.hadoop.hdfs.DistributedFileSystem&quot;</span>);</span><br><span class="line">                FileSystem fs = FileSystem.get(conf);</span><br><span class="line">                Path file = <span class="keyword">new</span> Path(<span class="string">&quot;test&quot;</span>); </span><br><span class="line">                FSDataInputStream getIt = fs.open(file);</span><br><span class="line">                BufferedReader d = <span class="keyword">new</span> BufferedReader(<span class="keyword">new</span> InputStreamReader(getIt));</span><br><span class="line">                String content = d.readLine(); <span class="comment">//读取文件一行</span></span><br><span class="line">                System.out.println(content);</span><br><span class="line">                d.close(); <span class="comment">//关闭文件</span></span><br><span class="line">                fs.close(); <span class="comment">//关闭hdfs</span></span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>result:</p>
<img src="images/image-20201025190008986.png" alt="image-20201025190008986" style="zoom:50%;">

<h1 id="分布式数据库HBase"><a href="#分布式数据库HBase" class="headerlink" title="分布式数据库HBase"></a>分布式数据库HBase</h1><h2 id="概述-2"><a href="#概述-2" class="headerlink" title="概述"></a>概述</h2><p>分布式存储系统支持PB级别数据，可以存储几千台服务器具有高可扩展性</p>
<p><strong>特点：</strong>：高可靠性、高性能、面向列、可伸缩</p>
<p>HBase分布式数据库可以用来存储非结构化和半结构化松散数据</p>
<p>HBase和BigTable的底层技术对应关系：</p>
<p><img src="images/image-20201025203821941.png" alt="image-20201025203821941"></p>
<blockquote>
<p>HBase架构在底层分布式文件系统HDFS之上，是BigTable 的开源实现</p>
</blockquote>
<p>HBase：虽然有了HDFS和MapReduce，但是Hadoop主要解决大规模数据离线批量处理，没有办法满足大数据实时处理需求。而随着数据规模爆炸式增长，传统关系型数据库扩展能力非常有限，另外很多数据的结构会变化，而传统关系型数据库模式很难变化。</p>
<h3 id="HBase和传统关系型数据库的联系和区别"><a href="#HBase和传统关系型数据库的联系和区别" class="headerlink" title="HBase和传统关系型数据库的联系和区别"></a>HBase和传统关系型数据库的联系和区别</h3><ul>
<li><p>数据类型</p>
<p>传统关系数据库用非常经典的关系数据模型</p>
<p>hbase数据模型简单，将数据存储为未经解释的字符串即字节数组，由应用程序开发人员来解释</p>
</li>
<li><p>数据操作</p>
<p>关系数据库多种操作而hbase操作简单，不对数据规范化</p>
</li>
<li><p>存储模式</p>
<p>hbase基于列存储</p>
</li>
<li><p>数据索引</p>
<p>关系数据库可以直接针对各个不同列构建复杂索引</p>
<p>hbase原始设计只支持对行键的简单索引</p>
</li>
<li><p>数据维护</p>
<p>关系数据库做一些更新操作时，旧值被覆盖</p>
<p>hbase不存在替换，旧版本会保存只会定期清理</p>
</li>
<li><p>可伸缩型</p>
<p>关系数据库很难水平扩展，只能变多核等方式纵向扩展，而hbase是借助分布式集群存储海量数据，水平可扩展性好</p>
</li>
</ul>
<h3 id="HBase访问接口"><a href="#HBase访问接口" class="headerlink" title="HBase访问接口"></a>HBase访问接口</h3><ul>
<li>提供了一个原生JAVA API</li>
<li>Shell</li>
<li>Thrift Gateway</li>
<li>REST Gateway</li>
<li>提供sql类型接口 </li>
<li>pig</li>
<li>数据仓库产品hive（hive sql）</li>
</ul>
<h2 id="数据模型"><a href="#数据模型" class="headerlink" title="数据模型"></a>数据模型</h2><p>HBase是一个稀疏的多维度的排序的映射表</p>
<img src="images/image-20201025205309295.png" alt="image-20201025205309295" style="zoom: 67%;">

<ul>
<li><p>每个值都是未经解释的字符串也就是Bytes数组,单元格具体存储数据</p>
</li>
<li><p>一行可以有一个行键和任意多个列族，以列族为单位存储，不同列族存在不同文件,列可以增加或删除</p>
</li>
<li><p>列族支持动态扩展，执行数据更新操作会保留旧版本，因为架构在HDFS上，没有办法直接修改数据，只能加入时间戳</p>
<blockquote>
<p>时间戳：数据更新，旧版本会保留，新版本通过时间戳来进行区分，因此一个单元格可能有非常多版本数据保存按时间戳</p>
</blockquote>
</li>
<li><p>HBase不考虑冗余，追求分析效率</p>
</li>
</ul>
<h3 id="数据坐标"><a href="#数据坐标" class="headerlink" title="数据坐标"></a>数据坐标</h3><p>传统关系数据库只要通过一个行一个列两个维度就可以确定一个唯一数据</p>
<p>而HBase(键值数据库)对数据的定位采用四维坐标:<strong><code>行键 列族 列限定符 时间戳</code></strong></p>
<h3 id="概念视图"><a href="#概念视图" class="headerlink" title="概念视图"></a>概念视图</h3><p><img src="images/image-20201025211904093.png" alt="image-20201025211904093"></p>
<p>可以看出hbase是一个稀疏表</p>
<p><strong>但是底层存储是按列族存储</strong></p>
<h3 id="物理视图"><a href="#物理视图" class="headerlink" title="物理视图"></a>物理视图</h3><p><img src="images/image-20201025212018349.png" alt="image-20201025212018349"></p>
<blockquote>
<p>按列族存储</p>
</blockquote>
<p>传统的面向行存储是对于传统的<strong>事务型</strong>操作,但是做<strong>分析</strong>时候,比如性别特征,针对一个列分析,此时列式存储占据优势,并且按列存储可以带来很高的数据压缩率.</p>
<h2 id="实现原理"><a href="#实现原理" class="headerlink" title="实现原理"></a>实现原理</h2><h3 id="功能组件"><a href="#功能组件" class="headerlink" title="功能组件"></a>功能组件</h3><h4 id="库函数"><a href="#库函数" class="headerlink" title="库函数"></a>库函数</h4><p>链接每个客户端</p>
<h4 id="master服务器"><a href="#master服务器" class="headerlink" title="master服务器"></a>master服务器</h4><p>管家</p>
<ul>
<li>对分区信息维护和管理</li>
<li>维护一个Region服务器列表,可以查看region服务器状态</li>
<li>负责对region分配</li>
<li>负载均衡</li>
</ul>
<h4 id="region服务器"><a href="#region服务器" class="headerlink" title="region服务器"></a>region服务器</h4><p>负责存储不同的region</p>
<p>客户端要访问数据也是在region对数据存取,不依赖master获取位置信息</p>
<h3 id="核心概念表与Region"><a href="#核心概念表与Region" class="headerlink" title="核心概念表与Region"></a>核心概念表与Region</h3><p>一个HBase表被划分为多个Region</p>
<p>一个region会分裂成多个新的region,分裂时不进行物理分割,修改指向信息即可,实现快速分裂.同一个region不可能分裂到不同的region服务器上</p>
<p><img src="images/image-20201025213240407.png" alt="image-20201025213240407"></p>
<blockquote>
<p>region的实际大小取决于单台服务器的有效处理能力,最佳配置为1GB-2G</p>
<p>每一个region服务器大概存储10-1000个region</p>
</blockquote>
<h3 id="Region定位"><a href="#Region定位" class="headerlink" title="Region定位"></a>Region定位</h3><blockquote>
<p>首先构建了一个元数据表,一列记录region id 另一列记录region服务器id</p>
<p>hbase最开始创建时有一个映射表,称为.META. 表,用于存储元数据</p>
</blockquote>
<p>HBase设计三层结构实现region寻址与定位</p>
<p><img src="images/image-20201025214125767.png" alt="image-20201025214125767"></p>
<blockquote>
<p>-ROOT-最多一个region不再分裂,记录元数据表元素信息存储在哪,.META.记录具体数据位置</p>
<p>实际存储时-ROOT-表地址记录在ZooKeeper中(写死),然后根据地址找到.META.,然后找到用户数据表</p>
<img src="images/image-20201025214326971.png" alt="image-20201025214326971" style="zoom:50%;">

<p>.META.所有内容存储在内存中,因此需要考虑大小</p>
</blockquote>
<p>另外,为了加速寻址,客户端会缓存位置信息,同时需要解决缓存失效问题,采用惰性解决方式,只有发现找不到时才再次三级寻址</p>
<h2 id="运行机制"><a href="#运行机制" class="headerlink" title="运行机制"></a>运行机制</h2><h3 id="系统架构"><a href="#系统架构" class="headerlink" title="系统架构"></a>系统架构</h3><p><img src="images/image-20201025215128209.png" alt="image-20201025215128209"></p>
<p>HBase并不直接和底层磁盘联系</p>
<blockquote>
<ul>
<li><p>客户端:访问HBase接口,会维护访问region的信息</p>
</li>
<li><p>zookeeper:实现协同管理服务,大量用于分布式计算,提供配置维护,域名服务,分布式同步服务,维护整个hbase集群</p>
</li>
<li><p>master(主服务器):负责hbase表的操作以及region管理,负责不同region服务器的负载均衡,负责调整分裂\合并后region的分布,负责重新分配故障\失效的region服务器</p>
</li>
<li><p>region服务器:负责用户数据存储和管理</p>
<img src="images/image-20201025221348611.png" alt="image-20201025221348611" style="zoom:50%;">

<p>每个region服务器有很多region,多个region共用一个Hlog文件,每个region按列族切分,每个列族会单独构成一个store,而每个store并不直接与底层连接,而是先写入memstore缓存中,缓存满后刷写到storefile(底层是通过hdfs存储(hfile格式))中</p>
</li>
</ul>
</blockquote>
<h3 id="Region服务器工作原理"><a href="#Region服务器工作原理" class="headerlink" title="Region服务器工作原理"></a>Region服务器工作原理</h3><h4 id="用户读写数据过程"><a href="#用户读写数据过程" class="headerlink" title="用户读写数据过程"></a>用户读写数据过程</h4><p>写:</p>
<p><img src="images/image-20201025222546421.png" alt="image-20201025222546421"></p>
<p>读:</p>
<p>先读缓存再读磁盘的storefile</p>
<p><img src="images/image-20201025222629090.png" alt="image-20201025222629090"></p>
<h4 id="缓存刷新"><a href="#缓存刷新" class="headerlink" title="缓存刷新"></a>缓存刷新</h4><p><img src="images/image-20201025225103257.png" alt="image-20201025225103257"></p>
<h3 id="Store工作原理"><a href="#Store工作原理" class="headerlink" title="Store工作原理"></a>Store工作原理</h3><h4 id="StoreFile合并"><a href="#StoreFile合并" class="headerlink" title="StoreFile合并"></a>StoreFile合并</h4><p>当磁盘中刷写生成的storefile达到一定阈值才会合并,但是合并到一定程度后会触发分裂,一个region分裂为两个region</p>
<p><img src="images/image-20201025225419043.png" alt="image-20201025225419043"></p>
<h3 id="HLog工作原理"><a href="#HLog工作原理" class="headerlink" title="HLog工作原理"></a>HLog工作原理</h3><p>HBase是构建一个集群管理数据,典型的分布式环境,底层使用的是非常廉价的低端机,故障是难免的,为了保证数据恢复需要采用日志方式,因此每次需要写入日志后再写入缓存</p>
<p><img src="images/image-20201025225721964.png" alt="image-20201025225721964"></p>
<h3 id="zookeper"><a href="#zookeper" class="headerlink" title="zookeper"></a>zookeper</h3><p>zookeeper监视整个集群.发现故障,监听region服务器,发现故障告诉master,master进行处理,通过日志进行恢复,对hlog拆解,把属于各个region的log分配出来,把发生故障的region分配给可用的region服务器通过log恢复.<strong>为了提高表的写操作性能</strong>所以才只分配一个hlog.所以拆解比较耗时,故障只是少部分情况.</p>
<h2 id="HBase应用方案"><a href="#HBase应用方案" class="headerlink" title="HBase应用方案"></a>HBase应用方案</h2><h3 id="性能优化方法"><a href="#性能优化方法" class="headerlink" title="性能优化方法"></a>性能优化方法</h3><ul>
<li><p>时间靠近数据存在一起:把时间戳包含在行键中,按升序排序,越后时间戳越大,因此需要反过来排序</p>
<p><img src="images/image-20201025232622511.png" alt="image-20201025232622511"></p>
</li>
<li><p>提升读写性能</p>
<p>根据需要决定是否放入缓存</p>
<p><img src="images/image-20201025232748493.png" alt="image-20201025232748493"></p>
</li>
<li><p>设置最大版本,保存最新版本的数据参数设为1,节省存储空间</p>
<p>HColumnDescriptor.setMaxVersionsMaxVersions</p>
</li>
<li><p>数据自动清理</p>
<p>TimeToLive,一旦超过生命周期成为过期数据自动被系统删除</p>
<p>setTimeToLive(2 * 24 * 60 * 60)</p>
</li>
</ul>
<h3 id="检测性能"><a href="#检测性能" class="headerlink" title="检测性能"></a>检测性能</h3><p>工具:</p>
<ul>
<li>Master-status:浏览器查询hbase运行状态</li>
<li>ganglia</li>
<li>opentsdb</li>
<li>ambari</li>
</ul>
<h3 id="构建sql引擎和HBase二级索引"><a href="#构建sql引擎和HBase二级索引" class="headerlink" title="构建sql引擎和HBase二级索引"></a>构建sql引擎和HBase二级索引</h3><h4 id="sql引擎"><a href="#sql引擎" class="headerlink" title="sql引擎"></a>sql引擎</h4><p>sql语句查询hbase相关数据</p>
<p>引擎:hive  phoneix</p>
<h4 id="HBase二级索引-辅助索引"><a href="#HBase二级索引-辅助索引" class="headerlink" title="HBase二级索引(辅助索引)"></a>HBase二级索引(辅助索引)</h4><blockquote>
<p>hbase默认只支持对行键进行索引.而实际应用要对不同列构建索引,因此可以采用coprocessor,如华为的hindex以及redis和solr</p>
</blockquote>
<p>coprocessor提供了两个实现endpoint和observer</p>
<ul>
<li>endpoint:相当于关系型数据库的存储过程</li>
<li>observer:相当于触发器</li>
</ul>
<blockquote>
<p>从而hbase可以有主表和索引表,(引擎构建在hbase之上,既没有对hbase进行任何改动,也不需要上层应用做任何妥协)但是缺点是耗时</p>
</blockquote>
<h2 id="HBase-数据库"><a href="#HBase-数据库" class="headerlink" title="HBase 数据库"></a>HBase 数据库</h2><p>启动:./bin/start-hbase.sh</p>
<p>shell:hbase shell  ./bin/hbase shell</p>
<p>停止:./bin/stop-hbase.sh</p>
<p>在添加数据时，HBase会自动为添加的数据添加一个时间戳，故在需要修改数据时，只需直接添加数据，HBase即会生成一个新的版本，从而完成“改”操作，旧的版本依旧保留，系统会定时回收垃圾数据，只留下最新的几个版本，保存的版本数可以在创建表的时候指定</p>
<h3 id="创建表"><a href="#创建表" class="headerlink" title="创建表"></a>创建表</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">create &#x27;student&#x27;,&#x27;Sname&#x27;,&#x27;Ssex&#x27;,&#x27;Sage&#x27;,&#x27;Sdept&#x27;,&#x27;course&#x27;</span><br></pre></td></tr></table></figure>


<h3 id="添加数据"><a href="#添加数据" class="headerlink" title="添加数据"></a>添加数据</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">put &#x27;student&#x27;,&#x27;95001&#x27;,&#x27;Sname&#x27;,&#x27;LiYing&#x27; # 95001行键</span><br><span class="line">put &#x27;student&#x27;,&#x27;95001&#x27;,&#x27;course:math&#x27;,&#x27;80&#x27; # course列族加入了math列</span><br></pre></td></tr></table></figure>


<h3 id="删除数据"><a href="#删除数据" class="headerlink" title="删除数据"></a>删除数据</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">delete &#x27;student&#x27;,&#x27;95001&#x27;,&#x27;Ssex&#x27;</span><br><span class="line">deleteall &#x27;student&#x27;,&#x27;95001&#x27;  </span><br></pre></td></tr></table></figure>


<h3 id="查看数据"><a href="#查看数据" class="headerlink" title="查看数据"></a>查看数据</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">get &#x27;student&#x27;,&#x27;95001&#x27;</span><br></pre></td></tr></table></figure>
<p><img src="images/image-20201026103328324.png" alt="image-20201026103328324"></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">scan &#x27;student&#x27;</span><br></pre></td></tr></table></figure>
<p><img src="images/image-20201026103408520.png" alt="image-20201026103408520"></p>
<h3 id="删除表"><a href="#删除表" class="headerlink" title="删除表"></a>删除表</h3><blockquote>
<p>disable 表</p>
<p>drop 表</p>
</blockquote>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">disable &#x27;student&#x27;  </span><br><span class="line">drop &#x27;student&#x27;</span><br></pre></td></tr></table></figure>
<h3 id="查询表历史数据"><a href="#查询表历史数据" class="headerlink" title="查询表历史数据"></a>查询表历史数据</h3><h4 id="查询历史版本"><a href="#查询历史版本" class="headerlink" title="查询历史版本"></a>查询历史版本</h4><ol>
<li><p>创建表的时候，指定保存的版本数（假设指定为5）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">create &#x27;teacher&#x27;,&#123;NAME=&gt;&#x27;username&#x27;,VERSIONS=&gt;5&#125;</span><br></pre></td></tr></table></figure>

</li>
<li><p>插入数据然后更新数据，使其产生历史版本数据</p>
</li>
<li><p>查询时，指定查询的历史版本数。默认会查询出最新的数据</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">get &#x27;teacher&#x27;,&#x27;91001&#x27;,&#123;COLUMN=&gt;&#x27;username&#x27;,VERSIONS=&gt;5&#125;</span><br></pre></td></tr></table></figure>
<p><img src="images/image-20201026103950179.png" alt="image-20201026103950179"></p>
<p><img src="images/image-20201026104008006.png" alt="image-20201026104008006"></p>
</li>
</ol>
<p>退出HBase数据库是退出对数据库表的操作: exit</p>
<h2 id="HBase-JAVA-API"><a href="#HBase-JAVA-API" class="headerlink" title="HBase JAVA API"></a>HBase JAVA API</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.util.Bytes;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ExampleForHBase</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> Configuration configuration;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> Connection connection;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> Admin admin;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span><span class="keyword">throws</span> IOException</span>&#123;</span><br><span class="line">        init();<span class="comment">//设置连接,配置</span></span><br><span class="line">        createTable(<span class="string">&quot;student&quot;</span>,<span class="keyword">new</span> String[]&#123;<span class="string">&quot;score&quot;</span>&#125;);</span><br><span class="line">        insertData(<span class="string">&quot;student&quot;</span>,<span class="string">&quot;zhangsan&quot;</span>,<span class="string">&quot;score&quot;</span>,<span class="string">&quot;English&quot;</span>,<span class="string">&quot;69&quot;</span>);</span><br><span class="line">        insertData(<span class="string">&quot;student&quot;</span>,<span class="string">&quot;zhangsan&quot;</span>,<span class="string">&quot;score&quot;</span>,<span class="string">&quot;Math&quot;</span>,<span class="string">&quot;86&quot;</span>);</span><br><span class="line">        insertData(<span class="string">&quot;student&quot;</span>,<span class="string">&quot;zhangsan&quot;</span>,<span class="string">&quot;score&quot;</span>,<span class="string">&quot;Computer&quot;</span>,<span class="string">&quot;77&quot;</span>);</span><br><span class="line">        getData(<span class="string">&quot;student&quot;</span>, <span class="string">&quot;zhangsan&quot;</span>, <span class="string">&quot;score&quot;</span>,<span class="string">&quot;English&quot;</span>);</span><br><span class="line">        close();</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">init</span><span class="params">()</span></span>&#123;</span><br><span class="line">        configuration  = HBaseConfiguration.create(); <span class="comment">// 配置对象</span></span><br><span class="line">        configuration.set(<span class="string">&quot;hbase.rootdir&quot;</span>,<span class="string">&quot;hdfs://localhost:9000/hbase&quot;</span>); <span class="comment">//设置参数</span></span><br><span class="line">        <span class="keyword">try</span>&#123;</span><br><span class="line">            connection = ConnectionFactory.createConnection(configuration); <span class="comment">//连接对象</span></span><br><span class="line">            admin = connection.getAdmin();</span><br><span class="line">        &#125;<span class="keyword">catch</span> (IOException e)&#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span></span>&#123; <span class="comment">//关闭数据连接</span></span><br><span class="line">        <span class="keyword">try</span>&#123;</span><br><span class="line">            <span class="keyword">if</span>(admin != <span class="keyword">null</span>)&#123;</span><br><span class="line">                admin.close();</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span>(<span class="keyword">null</span> != connection)&#123;</span><br><span class="line">                connection.close();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;<span class="keyword">catch</span> (IOException e)&#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">createTable</span><span class="params">(String myTableName,String[] colFamily)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        TableName tableName = TableName.valueOf(myTableName); <span class="comment">//生成tablename对象</span></span><br><span class="line">        <span class="keyword">if</span>(admin.tableExists(tableName))&#123;	<span class="comment">//判断是否存在表</span></span><br><span class="line">            System.out.println(<span class="string">&quot;talbe is exists!&quot;</span>);</span><br><span class="line">        &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">            TableDescriptorBuilder tableDescriptor = TableDescriptorBuilder.newBuilder(tableName);</span><br><span class="line">            <span class="keyword">for</span>(String str:colFamily)&#123; <span class="comment">//创建列族</span></span><br><span class="line">                ColumnFamilyDescriptor family = </span><br><span class="line">ColumnFamilyDescriptorBuilder.newBuilder(Bytes.toBytes(str)).build();</span><br><span class="line">                tableDescriptor.setColumnFamily(family);</span><br><span class="line">            &#125;</span><br><span class="line">            admin.createTable(tableDescriptor.build());</span><br><span class="line">        &#125; </span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">insertData</span><span class="params">(String tableName,String rowKey,String colFamily,String col,String val)</span> <span class="keyword">throws</span> IOException </span>&#123; </span><br><span class="line">        Table table = connection.getTable(TableName.valueOf(tableName));</span><br><span class="line">        Put put = <span class="keyword">new</span> Put(rowKey.getBytes()); <span class="comment">//单元格对象</span></span><br><span class="line">        put.addColumn(colFamily.getBytes(),col.getBytes(), val.getBytes()); <span class="comment">//确定列族等</span></span><br><span class="line">        table.put(put);</span><br><span class="line">        table.close(); </span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">getData</span><span class="params">(String tableName,String rowKey,String colFamily, String col)</span><span class="keyword">throws</span>  IOException</span>&#123; </span><br><span class="line">        Table table = connection.getTable(TableName.valueOf(tableName));</span><br><span class="line">        Get get = <span class="keyword">new</span> Get(rowKey.getBytes());</span><br><span class="line">        get.addColumn(colFamily.getBytes(),col.getBytes());</span><br><span class="line">        Result result = table.get(get);</span><br><span class="line">        System.out.println(<span class="keyword">new</span> String(result.getValue(colFamily.getBytes(),col==<span class="keyword">null</span>?<span class="keyword">null</span>:col.getBytes())));</span><br><span class="line">        table.close(); </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="NoSQL数据库"><a href="#NoSQL数据库" class="headerlink" title="NoSQL数据库"></a>NoSQL数据库</h1><h2 id="概述-3"><a href="#概述-3" class="headerlink" title="概述"></a>概述</h2><p>Not Only SQL</p>
<p>灵活的可扩展性：水平可扩展</p>
<p>灵活的数据模型：无模式，不那么严格</p>
<p>和云计算的紧密结合：水平可扩展，充分利用云计算基础设备。可以根据负载实时变化，动态伸缩</p>
<ul>
<li>传统的关系数据库性能上没有办法满足海量数据的管理需求；没有办法满足高并发需求，网页访问数据库，当多个用户访问性能很差，早期使用动态网页静态化技术，提前把动态网页保留成静态模式，外部访问静态网页而不需要访问数据库，但是应用需要实时生成数据，无法满足，这种实时生成的数据对数据库的负载非常高；无法满足高可扩展性和高可用性的需求，突发的负载高峰期无法应对。</li>
<li>MySQL集群方式的缺陷：复杂性，整个集群部署管理配置都非常复杂；延迟性，异步方式，主库压力大时会有较大延迟；扩容问题，整个集群压力过大时，需要增加新机器对整个数据集进行重新分区，非常复杂；动态迁移问题，数据集划分人工划分，没有办法有效预测集群未来负载分布情况，因此需要负载再均衡，迁移相关数据，需要总控节点，且需要人工实现。</li>
</ul>
<p>关系型数据库无法适应不同的业务场景，对于海量数据批量处理强调的是高吞吐量，因此有了hadoop、mongoDB、redis。如Web2.0通常不要求严格数据库事务，使用事务会有额外开销；不需要严格的读写实时性；不包含复杂的SQL查询</p>
<h3 id="与关系数据库比较"><a href="#与关系数据库比较" class="headerlink" title="与关系数据库比较"></a>与关系数据库比较</h3><ul>
<li>在数据库原理方面，关系数据库具有完备的关系代数理论作为基础；NoSQL数据库缺乏理论基础</li>
<li>数据规模方面：关系数据库很难实现横向扩展，纵向扩展有限，NoSQL水平可扩展性强</li>
<li>在数据库模式方面，关系数据库要定义严格的数据库模式，而且要严格遵守事先定义的数据库模式。NoSQL数据模型较灵活</li>
<li>在查询效率方面：关系数据库适当数据查询效率高，数据量级增大效率变低，NoSQL较关系型数据库性能低</li>
<li>事务一致性：关系数据库遵循ACID事务模型保证事务强一致性；NoSQL放松了该性能，而保证base要求</li>
<li>数据完整性：关系数据库具有保证完整性的完备机制；NoSQL不能实现完整性约束</li>
<li>在可扩展性，NoSQL水平可扩展性好</li>
<li>在可用性，关系数据库为了保证严格一致性可用性被削弱，NoSQL具有非常好可用性，短时间迅速返回结果</li>
<li>在标准化，关系数据库遵循SQL标准标准化完善，NoSQL数据库未形成通用的行业标准</li>
<li>技术支持方面，关系数据库很多为商业数据库可以有非常强大技术和服务支持；NoSQL数据库多属于开源产品，处于发展初步阶段</li>
<li>在可维护方面，关系数据库需要管理员维护，NoSQL数据库没有成熟的基础和实践操作规范，维护较为复杂</li>
</ul>
<p>关系数据库的优势就是完备的关系代数理论、严格的标准、事务一致性、可以借助索引机制实现非常高效的查询。劣势是可扩展性差、数据模型定义严格，无法较好满足新型Web2.0。应用于电信银行的关键业务系统。</p>
<p>NoSQL的优势可以支持超大规模数据存储，数据模型灵活。劣势是缺乏底层基础理论支撑，不支持事务一致性，影响在关键业务系统的应用。应用于互联网企业以及一些传统企业的非关键业务。</p>
<h3 id="四大类型、三大基石"><a href="#四大类型、三大基石" class="headerlink" title="四大类型、三大基石"></a>四大类型、三大基石</h3><h4 id="四大类型"><a href="#四大类型" class="headerlink" title="四大类型"></a>四大类型</h4><p>键值数据库、列族数据库、文档数据库（可看作键值，只不过文档是值）、图数据库（图结构）</p>
<ul>
<li><p>键值数据库如Redis Memcached  SimpleDB</p>
<ul>
<li><p>数据模型：键是一个字符串对象（因此也无法存储结构化信息、条件查询效率低）</p>
</li>
<li><p>应用：涉及频繁读写，拥有简单数据模型的应用，内容缓存，如会话、配置文件、参数、购物车等，存储配置和用户数据信息等移动应用</p>
</li>
<li><p>一般不支持回滚</p>
</li>
<li><p>成为理想的缓冲层解决方案</p>
</li>
</ul>
</li>
<li><p>列族数据库如BigTable、HBase、Cassandra</p>
<ul>
<li><p>数据模型：列族</p>
</li>
<li><p>应用：分布式数据存储与管理。数据在地理上分布于多个数据中心的应用程序；可以容忍副本中存在短期不一致情况的应用程序；拥有动态字段的应用程序</p>
</li>
<li><p>大都不支持事务一致性（因此需要ACID事务支持的就不能用）</p>
</li>
</ul>
</li>
<li><p>文档数据库，如MongoDB CouchDB</p>
<ul>
<li>value是文档的键值数据库，能够将自己的数据内容和类型进行自我描述</li>
<li>数据结构：JSON数据格式</li>
<li>并发性高，文档数据库可以完整包含在一个文档里，有较好的并发性。对数据更新时，只需锁定一个文档既可以把相关数据修改</li>
<li>应用：存储、索引并管理面向文档数据；或者类似的半结构化数据</li>
<li>文档数据库不支持文档间事务</li>
</ul>
</li>
<li><p>图数据库 如Neo4j</p>
<ul>
<li>数据模型：图结构</li>
<li>应用：处理具有高度相互关联关系的数据，较适合于社交网络、模式识别、依赖分析、推荐系统以及路径寻找等问题</li>
</ul>
</li>
</ul>
<h4 id="三大基石"><a href="#三大基石" class="headerlink" title="三大基石"></a>三大基石</h4><h5 id="CAP"><a href="#CAP" class="headerlink" title="CAP"></a>CAP</h5><ul>
<li><p>Consistency：一致性，任何一个操作总能读到之前完成的写操作的结果</p>
</li>
<li><p>Availablity：可用性，快速获取数据，可以在确定的时间内返回操作结果，保证每个请求不管成功或者失败都有响应</p>
</li>
<li><p>Partition tolerance：分区容忍性，当出现网络分区的情况时，分离的系统也能够正常运行</p>
</li>
</ul>
<p>一个分布式系统不能同时满足CAP只能牺牲一个</p>
<img src="images/image-20201105112739921.png" alt="image-20201105112739921" style="zoom:50%;">

<p><img src="images/image-20201105112815458.png" alt="image-20201105112815458"></p>
<h5 id="BASE-Basically-Avaible-Soft-state、Eventual-consistency"><a href="#BASE-Basically-Avaible-Soft-state、Eventual-consistency" class="headerlink" title="BASE-Basically Avaible Soft state、Eventual consistency"></a>BASE-Basically Avaible Soft state、Eventual consistency</h5><blockquote>
<p>ACID是关系数据库事务的四性质，NoSQL中BASE是对应关系</p>
</blockquote>
<ul>
<li><p>基本可用：允许分区失败出现，一个分布式系统的一部分发生问题变得不可用时其他部分仍然可以正常使用</p>
</li>
<li><p>软状态：状态可以有一段时间不同步，具有一定滞后性（硬状态：数据库状态必须一直保持数据库一致性，任意时刻数据必须正确）</p>
</li>
<li><p>最终一致性：一致性的类型包括强一致性和弱一致性，二者主要区别在于高并发的数据访问操作下，后续操作是否能够获得最新数据</p>
<p>根据更新数据后各进程访问到数据的时间和方式不同，可以分为：</p>
<ul>
<li>因果一致性：A通知B可以获得</li>
<li>读己之所写一致性</li>
<li>单调读一致性</li>
<li>会话一致性：会话存在就可以保证读己之所写一致性</li>
<li>单调写一致性：保证同一个进程写操作按顺序执行</li>
</ul>
</li>
</ul>
<h6 id="实现一致性"><a href="#实现一致性" class="headerlink" title="实现一致性"></a>实现一致性</h6><p>假设一个分布式系统，实现可靠性，N：对数据进行冗余存储份数，W：更新数据时保证写完成的节点数，R：读取数据时候需要读取的节点数</p>
<p>当W+R&gt;N，强一致性<img src="images/image-20201105120108268.png" alt="image-20201105120108268" style="zoom:50%;"></p>
<p>当W+R&lt;=N，弱一致性<img src="images/image-20201105120243549.png" alt="image-20201105120243549" style="zoom:50%;"></p>
<p>一般N&gt;=3</p>
<p>如果N=W R=1，任何一个写节点失效都会导致写失败，因此可用性降低，但是由于数据分布的N个节点是同步写入的因此可以保证强一致性</p>
<p>HDFS采用强一致性保证，N=W R=1</p>
<h3 id="NoSQL和NewSQL数据库区别"><a href="#NoSQL和NewSQL数据库区别" class="headerlink" title="NoSQL和NewSQL数据库区别"></a>NoSQL和NewSQL数据库区别</h3><img src="images/image-20201105120632265.png" alt="image-20201105120632265" style="zoom:50%;">

<p>而现在根据不同应用场景来使用</p>
<p>分析型应用：NewSQL；事务型应用：OldSQL；互联网应用：NoSQL</p>
<p>NewSQL是关系型数据库，同时具备 OldSQL和NoSQL优点</p>
<img src="images/image-20201105120827245.png" alt="image-20201105120827245" style="zoom:50%;">



<h3 id="文档数据库MongoDB为实例介绍NoSQL数据库编程实战"><a href="#文档数据库MongoDB为实例介绍NoSQL数据库编程实战" class="headerlink" title="文档数据库MongoDB为实例介绍NoSQL数据库编程实战"></a>文档数据库MongoDB为实例介绍NoSQL数据库编程实战</h3><img src="images/image-20201105121059323.png" alt="image-20201105121059323" style="zoom:50%;">

<h4 id="与关系数据库的比较"><a href="#与关系数据库的比较" class="headerlink" title="与关系数据库的比较"></a>与关系数据库的比较</h4><p><img src="images/image-20201105121116356.png" alt="image-20201105121116356"></p>
<p>比如博客评论，记录在一个文档即可</p>
<p>客户端是mongod服务器端是mongo</p>
<p>集合就是mongoDB文档组，类似于RDBMS的表格，集合存在于数据库中，没有固定结构，可以对集合查入不同格式和类型的数据</p>
<h4 id="数据库"><a href="#数据库" class="headerlink" title="数据库"></a>数据库</h4><img src="images/image-20201105121345717.png" alt="image-20201105121345717" style="zoom:50%;">

<h4 id="文档"><a href="#文档" class="headerlink" title="文档"></a>文档</h4><img src="images/image-20201105121357484.png" alt="image-20201105121357484" style="zoom:50%;">

<p>安装：</p>
<p><img src="images/image-20201105121637215.png" alt="image-20201105121637215"></p>
<p>使用：</p>
<p><img src="images/image-20201105121736509.png" alt="image-20201105121736509"></p>
<p>编程方式访问：</p>
<p><img src="images/image-20201105121843521.png" alt="image-20201105121843521"></p>
<p><img src="images/image-20201105121857478.png" alt="image-20201105121857478"></p>
<p><img src="images/image-20201105121920473.png" alt="image-20201105121920473"></p>
<p><img src="images/image-20201105121950080.png" alt="image-20201105121950080"></p>
<p><img src="images/image-20201105122004775.png" alt="image-20201105122004775"></p>
<h1 id="云数据库"><a href="#云数据库" class="headerlink" title="云数据库"></a>云数据库</h1><h2 id="概述-4"><a href="#概述-4" class="headerlink" title="概述"></a>概述</h2><p>云计算：通过网络以服务的方式为用户提供廉价的IT资源。按需服务，随时服务，通用性，高可靠性，极其廉价，超大规模，虚拟概念，虚拟化技术</p>
<p>云数据库：IaaS，PaaS，SaaS部署和虚拟化在云计算环境下的数据库，因此动态可扩展、高可用性、较低的使用代价、易用性、免维护、高性能、安全</p>
<p>个性化存储需求：大企业海量数据存储需求、中小企业低成本数据存储需求、企业动态变化的存储需求</p>
<p>云数据库和其他数据库关系，以数据模型角度通过网络以服务的方式提供数据库功能，并没有专属的数据模型</p>
<h2 id="UMP系统"><a href="#UMP系统" class="headerlink" title="UMP系统"></a>UMP系统</h2><h3 id="概述-5"><a href="#概述-5" class="headerlink" title="概述"></a>概述</h3><blockquote>
<p>mysql解决方案，低成本高性能开源数据库</p>
</blockquote>
<p>UMP在设计时实现以下原则：</p>
<ul>
<li>整个系统保持单一的对外访问入口，统一的入口，统一的资源池供调用</li>
<li>消除单点故障，保证服务的高可用性，存在管家Controller多个</li>
<li>具有良好的可伸缩，能够动态增加、减少计算资源</li>
<li>可以实现资源之间的相互隔离，因为云数据库属于多租户，而共用底层资源，可能会出现某个用户消耗资源过多的情况，因此UMP设置了安全限制</li>
</ul>
<h3 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h3><p>mysql集群负责具体数据库存储，controller服务器管理（多个，由zookeeper选出总管），对外提供服务窗口由proxy服务器，LVS实现集群负载均衡</p>
<img src="images/image-20201106144450532.png" alt="image-20201106144450532" style="zoom:50%;">

<blockquote>
<p>mnesia：分布式数据库管理系统，支持事务，支持透明的数据分片，数据库模式可以运行时动态配置</p>
<img src="images/image-20201106144633822.png" alt="image-20201106144633822" style="zoom:50%;">

<p>RabbitMQ：工业级消息队列产品，异步传输，保证可靠的消息传输</p>
<img src="images/image-20201106144756010.png" alt="image-20201106144756010" style="zoom:50%;">

<p>ZooKeeper：高效可靠的协调服务，在UMP系统中作为全局的配置服务器，可以设定相关监听，检测到后告诉其他，提供分布式锁（选出集群总管，多个管家中一个，其他后备），监控所有mysql实例状态</p>
<p>LVS Linux Virtual Server：实现集群内部负载均衡，采用IP负载均衡技术和基于内容请求分发技术，调度器是LVS集群系统唯一入口点，整个服务器集群结构对客户透明</p>
<p>Controller服务器：集群管理，运行了一组mnesia分布式数据库服务，为了避免单点故障，设置多个Controller服务器，而由ZooKeeper服务器确定总管，提供对外服务</p>
<p>Web控制台：提供界面</p>
<p>Proxy服务器：面向用户提供访问mysql数据库服务，通过用户名获得用户认证信息，进行资源配额限制，后台mysql实例地址</p>
<p>agent服务器：部署在允许mysql进程的机器上，用来管理每台物理机上的mysql实例。</p>
<p>日志分析服务器：对整个日志进行分析</p>
<p>信息统计服务器：系统运营数据如用户连接数、每秒查询数、mysql实例进程状态</p>
<p>愚公系统：数据迁移，允许不停机情况下动态扩容、缩容、迁移</p>
</blockquote>
<h3 id="功能"><a href="#功能" class="headerlink" title="功能"></a>功能</h3><h1 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h1><h2 id="概述-6"><a href="#概述-6" class="headerlink" title="概述"></a>概述</h2><h3 id="分布式并行编程"><a href="#分布式并行编程" class="headerlink" title="分布式并行编程"></a>分布式并行编程</h3><p>数据处理能力提升的两条路线：</p>
<ol>
<li>单核CPU到双核到四核到八核</li>
<li>分布式并行编程：借助一个集群通过多台机器同时并行处理大规模数据集</li>
</ol>
<img src="images/image-20201106151304653.png" alt="image-20201106151304653">

<h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p>Map + Reduce</p>
<h4 id="策略：分而治之"><a href="#策略：分而治之" class="headerlink" title="策略：分而治之"></a>策略：分而治之</h4><p>把非常庞大的数据集切分成非常多的独立小分片，为每一个分片单独启动一个map任务， 最终通过多个map任务，并行地在多个机器上去处理</p>
<p><strong>遵循</strong>计算向数据靠拢（构建集群选取机器做map机器进行数据处理分析，数据块在不同机器上，数据寻找离所在机器最近的map机器，通常是同台，数据不需迁移，计算就在数据节点上执行）而不是数据向计算靠拢（选择计算结点，把运行数据分析的程序放在计算节点上运行，然后把它涉及的数据从各个不同节点上拉过来，传输到计算发生地方）减少了数据传输开销</p>
<img src="images/image-20201106153907633.png" alt="image-20201106153907633" style="zoom:50%;">

<h4 id="架构-1"><a href="#架构-1" class="headerlink" title="架构"></a>架构</h4><p>Master/slave</p>
<img src="images/image-20201106154104934.png" alt="image-20201106154104934" style="zoom:50%;">

<h4 id="map"><a href="#map" class="headerlink" title="map"></a>map</h4><img src="images/image-20201106154208859.png" alt="image-20201106154208859">

<h4 id="reduce"><a href="#reduce" class="headerlink" title="reduce"></a>reduce</h4><p><img src="images/image-20201106154301887.png" alt="image-20201106154301887"></p>
<h2 id="体系结构-1"><a href="#体系结构-1" class="headerlink" title="体系结构"></a>体系结构</h2><h3 id="client"><a href="#client" class="headerlink" title="client"></a>client</h3><ul>
<li><p>通过client提交用户编写应用程序，将其交给jobtracker端</p>
</li>
<li><p>同时可以查看当前提交作业运行状态</p>
</li>
</ul>
<h3 id="jobtracker"><a href="#jobtracker" class="headerlink" title="jobtracker"></a>jobtracker</h3><ul>
<li>负责资源的监控和作业调度</li>
<li>监控底层的其他tasktracker以及当前运行的job健康状态</li>
<li>一旦探测到失败情况把任务转移到其他节点继续跟踪任务执行进度和资源使用量（涉及任务调度器）</li>
</ul>
<h3 id="tasktrackr-任务调度器"><a href="#tasktrackr-任务调度器" class="headerlink" title="tasktrackr-任务调度器"></a>tasktrackr-任务调度器</h3><ul>
<li>执行具体的相关任务一般接收jobtracker的命令</li>
<li>把一些自己的资源使用情况，以及任务的运行进度通过心跳的方式，即heartbeat发送给jobtracker</li>
</ul>
<h4 id="衡量资源状态-slot槽"><a href="#衡量资源状态-slot槽" class="headerlink" title="衡量资源状态-slot槽"></a>衡量资源状态-slot槽</h4><p>所有资源进行打包，然后等分为slot（map类型、reduce类型），两种类型的slot并不通用，map类型的由map任务执行，以slot为单位调度资源，也就是说只有有空闲的slot才能把相关task分配执行</p>
<h3 id="task-schedule"><a href="#task-schedule" class="headerlink" title="task schedule"></a>task schedule</h3><ul>
<li>map任务</li>
<li>reduce任务</li>
</ul>
<h2 id="工作流程"><a href="#工作流程" class="headerlink" title="工作流程"></a>工作流程</h2><p><img src="images/image-20201108105351128.png" alt="image-20201108105351128"></p>
<blockquote>
<p>应用程序执行大规模数据分析，大规模数据保存在分布式文件系统，如HDFS，因此需要分片，每个分片单独分配map任务，输入输出均为key value，map任务输出结果分配给不同的reduce，分区数量一般取决于reduce数量，分发过程为shuffle，最后给reduce，reduce处理后写入hdfs</p>
</blockquote>
<ul>
<li>不同map任务不进行通信，不同的reduce也不进行信息交换，用户也不能从一台机器向另一台机器发送消息</li>
</ul>
<h3 id="执行的各个阶段"><a href="#执行的各个阶段" class="headerlink" title="执行的各个阶段"></a>执行的各个阶段</h3><p><img src="images/image-20201108110244574.png" alt="image-20201108110244574"></p>
<blockquote>
<ul>
<li>InputFormat：首先执行从HDFS加载文件，对输入进行格式验证，同时对大数据集切分为split，<strong>逻辑上切分</strong></li>
<li>由recordreader记录阅读器具体根据分片长度信息，从hdfs各块中读出&lt;key, value&gt;，交给map</li>
<li>map函数输入后，根据用户的处理逻辑，处理后得到一堆key-value</li>
<li>对输出进行分区排序合并归并（shuffle，洗牌），然后再把相关键值对交给对应reduce任务</li>
<li>reduce处理用户撰写的处理逻辑，完成数据分析，以value输出</li>
<li>outputFomat对输出进行检查，然后交给hdfs</li>
</ul>
</blockquote>
<h4 id="split-分片"><a href="#split-分片" class="headerlink" title="split-分片"></a>split-分片</h4><p>inputformat将大的文件分成很多split</p>
<p>hdfs分块是物理块，分片是逻辑分片，由用户定义，每次产生分片，会为其分配map任务，因此map任务多会浪费管理资源，如果过少无法很好的利用并行，因此一般根据块大小定义，否则横跨block，如果不在一台机器，那么会有额外的数据开销</p>
<p><img src="images/image-20201108111555841.png" alt="image-20201108111555841"></p>
<h4 id="map、reduce分配"><a href="#map、reduce分配" class="headerlink" title="map、reduce分配"></a>map、reduce分配</h4><p>map数量一般按照分片数量，reduce任务的数量一般取决于集群中可用的reduce任务槽slot的数目（通常设置比reduce任务槽数目稍微小一些的reduce任务个数，可以预留一些系统资源处理可能发生的错误）</p>
<h2 id="shuffle过程原理"><a href="#shuffle过程原理" class="headerlink" title="shuffle过程原理"></a>shuffle过程原理</h2><p><img src="images/image-20201108111927974.png" alt="image-20201108111927974"></p>
<blockquote>
<p>数据从分布式文件系统输入，输入后分片处理，每个分片分配一个map执行处理逻辑，输出键值对。</p>
<p>键值对先进入缓存，缓存满后再溢写到磁盘（过程中进行分区排序合并），溢写发生多次，生成多个磁盘文件，多个磁盘文件进行统一归并，然后通知reduce取走。reduce执行归并得到键值对，交给reduce执行处理逻辑，然后给分布式文件系统</p>
</blockquote>
<h3 id="map端shuffle"><a href="#map端shuffle" class="headerlink" title="map端shuffle"></a>map端shuffle</h3><p><img src="images/image-20201108112436428.png" alt="image-20201108112436428"></p>
<h4 id="输入数据和执行map任务"><a href="#输入数据和执行map任务" class="headerlink" title="输入数据和执行map任务"></a>输入数据和执行map任务</h4><p>切分给map任务，每个map任务分配缓存</p>
<h4 id="写入缓存"><a href="#写入缓存" class="headerlink" title="写入缓存"></a>写入缓存</h4><h4 id="溢写"><a href="#溢写" class="headerlink" title="溢写"></a>溢写</h4><p>一般设置溢写比例（0.8）防止满了造成丢失</p>
<h5 id="分区"><a href="#分区" class="headerlink" title="分区"></a>分区</h5><p>不同的reduce不同的区，一般采用哈希函数</p>
<h5 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h5><p>默认操作，根据key</p>
<h5 id="合并（非必须）"><a href="#合并（非必须）" class="headerlink" title="合并（非必须）"></a>合并（非必须）</h5><p>减少溢写到磁盘的数据量，如A、A合并为（A，2）（由于合并有代价，可以设置阈值再进行合并）</p>
<p>注意保证不改变计算结果</p>
<h4 id="归并"><a href="#归并" class="headerlink" title="归并"></a>归并</h4><p>多次溢写，磁盘有多个溢写文件，那么系统对其进行归并，最后交给本地磁盘，大文件内键值对是分区且排序的</p>
<p><strong>jobtracker检测到写完数据到磁盘生成大文件，会通知reduce取出数据</strong></p>
<h3 id="reduce端shuffle"><a href="#reduce端shuffle" class="headerlink" title="reduce端shuffle"></a>reduce端shuffle</h3><p><img src="images/image-20201108113215979.png" alt="image-20201108113215979"></p>
<blockquote>
<p>reduce从map机器拉走数据，需要从多个map任务取走，那么既然是多个map机器取出，肯定还可以进行合并，因此进行归并再合并</p>
<p>归并：生成&lt;key, value-list&gt;，如果不进行合并就是key value-list</p>
<p>如果有合并则合并后写入磁盘，磁盘中有可能有若干个文件，那么还需要归并成大文件，多轮归并可能还是若干个，可以直接给reduce处理</p>
</blockquote>
<h2 id="应用程序执行过程"><a href="#应用程序执行过程" class="headerlink" title="应用程序执行过程"></a>应用程序执行过程</h2><p><img src="images/image-20201108113750732.png" alt="image-20201108113750732"></p>
<ol>
<li><p>程序部署</p>
<ul>
<li>master：1个，负责管家角色，jobtracker</li>
<li>worker：多个，有map也有reduce</li>
</ul>
</li>
<li><p>分配map\reduce任务，分配worker任务</p>
<p>对数据分片，分配map执行分片处理</p>
</li>
<li><p>从分布式文件系统读数据，得到键值对交给map，先写缓存</p>
</li>
<li><p>本地写数据到磁盘，得到大文件</p>
</li>
<li><p>从远端拉走数据到本地，远程读数据，执行reduce函数，得到键值对</p>
</li>
<li><p>写数据到输出文件，也就是分布式文件系统</p>
</li>
</ol>
<p>注意：中间过程并不写在分布式文件系统，只是个中间结果在本地磁盘即可</p>
<h2 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h2><ul>
<li>能够分而治之的任务才能由mapreduce来做，如果有相互依赖性则无法使用</li>
</ul>
<img src="images/image-20201108115035223.png" alt="image-20201108115035223" style="zoom:50%;">

<img src="images/image-20201108115044211.png" alt="image-20201108115044211" style="zoom:50%;">

<img src="images/image-20201108115335996.png" alt="image-20201108115335996" style="zoom:50%;">

<h3 id="实现自然连接"><a href="#实现自然连接" class="headerlink" title="实现自然连接"></a>实现自然连接</h3><img src="images/image-20201108115847808.png" alt="image-20201108115847808" style="zoom: 50%;">

<p>实现如下：</p>
<p><img src="images/image-20201108115917327.png" alt="image-20201108115917327"></p>
<ul>
<li><p>使用map生成键值对</p>
<p>b作为key，&lt;R, a&gt;可以根据元组找到来自哪个关系，因为只能进行R S连接</p>
<p><img src="images/image-20201108120043939.png" alt="image-20201108120043939"></p>
</li>
<li><p>交给reduce任务，相同key，来自不同关系进行连接</p>
<p><img src="images/image-20201108120113749.png" alt="image-20201108120113749"></p>
</li>
</ul>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p><img src="images/image-20201109062539800.png" alt="image-20201109062539800"></p>
<blockquote>
<p>注意:需要导入相应的jar包</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.Iterator;</span><br><span class="line"><span class="keyword">import</span> java.util.StringTokenizer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.GenericOptionsParser;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCount</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">WordCount</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line">     <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        String[] otherArgs = (<span class="keyword">new</span> GenericOptionsParser(conf, args)).getRemainingArgs();</span><br><span class="line">        <span class="keyword">if</span>(otherArgs.length &lt; <span class="number">2</span>) &#123;</span><br><span class="line">            System.err.println(<span class="string">&quot;Usage: wordcount &lt;in&gt; [&lt;in&gt;...] &lt;out&gt;&quot;</span>);</span><br><span class="line">            System.exit(<span class="number">2</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        Job job = Job.getInstance(conf, <span class="string">&quot;word count&quot;</span>); <span class="comment">// 设置环境参数</span></span><br><span class="line">        job.setJarByClass(WordCount.class);	<span class="comment">// 设置整个程序类名</span></span><br><span class="line">        job.setMapperClass(WordCount.TokenizerMapper.class);	<span class="comment">// 添加mapper类</span></span><br><span class="line">        job.setCombinerClass(WordCount.IntSumReducer.class);</span><br><span class="line">        job.setReducerClass(WordCount.IntSumReducer.class);	<span class="comment">// 添加reducer类</span></span><br><span class="line">        job.setOutputKeyClass(Text.class);	<span class="comment">// 设置输出类型</span></span><br><span class="line">        job.setOutputValueClass(IntWritable.class); </span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; otherArgs.length - <span class="number">1</span>; ++i) &#123;</span><br><span class="line">            FileInputFormat.addInputPath(job, <span class="keyword">new</span> Path(otherArgs[i]));	<span class="comment">// 设置输入文件</span></span><br><span class="line">        &#125;</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(otherArgs[otherArgs.length - <span class="number">1</span>]));	<span class="comment">//设置输出文件</span></span><br><span class="line">        System.exit(job.waitForCompletion(<span class="keyword">true</span>)?<span class="number">0</span>:<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">TokenizerMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">Object</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;		<span class="comment">// 继承自Mapper</span></span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> IntWritable one = <span class="keyword">new</span> IntWritable(<span class="number">1</span>); <span class="comment">//数据通过网络传输,因此需要保证可序列化</span></span><br><span class="line">        <span class="keyword">private</span> Text word = <span class="keyword">new</span> Text();</span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="title">TokenizerMapper</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 输入输出类型定义</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(Object key, Text value, Mapper&lt;Object, Text, Text, IntWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            <span class="comment">// 执行map处理逻辑</span></span><br><span class="line">            StringTokenizer itr = <span class="keyword">new</span> StringTokenizer(value.toString()); </span><br><span class="line">            <span class="keyword">while</span>(itr.hasMoreTokens()) &#123;</span><br><span class="line">                <span class="keyword">this</span>.word.set(itr.nextToken());</span><br><span class="line">                context.write(<span class="keyword">this</span>.word, one);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">IntSumReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">   		<span class="comment">// 继承自reducer 	</span></span><br><span class="line">    	<span class="comment">// 执行前经历shuffle,生成&lt;KEY, VALUE-LIST&gt;</span></span><br><span class="line">        <span class="keyword">private</span> IntWritable result = <span class="keyword">new</span> IntWritable();</span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="title">IntSumReducer</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        &#125;</span><br><span class="line">    	<span class="comment">// 输入iterable容器</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Reducer&lt;Text, IntWritable, Text, IntWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">            IntWritable val;</span><br><span class="line">            <span class="keyword">for</span>(Iterator i$ = values.iterator(); i$.hasNext(); sum += val.get()) &#123;</span><br><span class="line">                val = (IntWritable)i$.next();</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">this</span>.result.set(sum);	<span class="comment">// 需要设置为可序列化类型</span></span><br><span class="line">            context.write(key, <span class="keyword">this</span>.result);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>打包为jar包后执行输出结果如下:</p>
<p><img src="images/image-20201109120905340.png" alt="image-20201109120905340"></p>
<h1 id="Hadoop发展"><a href="#Hadoop发展" class="headerlink" title="Hadoop发展"></a>Hadoop发展</h1><ul>
<li>抽象层次低，需人工编码</li>
<li>表达能力有限</li>
<li>难以看到程序整体逻辑</li>
<li>开发者自己管理作业之间依赖关系</li>
<li>执行迭代操作效率低</li>
<li>资源浪费</li>
<li>实时性差</li>
</ul>
<img src="images/image-20201203160548636.png" alt="image-20201203160548636" style="zoom:50%;">

<blockquote>
<p>热备机制高可用性；多个命名空间实现资源隔离；yarn来承担资源管理</p>
</blockquote>
<h2 id="HDFS-HA、HDFS-Federation"><a href="#HDFS-HA、HDFS-Federation" class="headerlink" title="HDFS HA、HDFS Federation"></a>HDFS HA、HDFS Federation</h2><h3 id="HDFS-HA"><a href="#HDFS-HA" class="headerlink" title="HDFS HA"></a>HDFS HA</h3><p>解决单点故障问题，热备份</p>
<ul>
<li><p>架构</p>
<p>Zookeeper集群帮助保证只有一个节点处于活跃状态</p>
<p>共享存储系统保证待命节点与活跃节点保证数据信息同步，同步的是editlog，并不是映射表（映射关系是通过底层不断汇报获得）</p>
<img src="images/image-20201203161256058.png" alt="image-20201203161256058" style="zoom: 80%;">

</li>
</ul>
<h3 id="HDFS-Fedration"><a href="#HDFS-Fedration" class="headerlink" title="HDFS Fedration"></a>HDFS Fedration</h3><p>解决水平扩展问题、单个名称节点吞吐量（系统性能）、不同程序之间隔离性。</p>
<ul>
<li><p>架构</p>
<p>多个名称节点，相互独立构成联盟关系，提供向后兼容性、且所有名称节点共享底层数据存储池（块池只是逻辑概念）</p>
<img src="images/image-20201203161950514.png" alt="image-20201203161950514" style="zoom:80%;">

<blockquote>
<p>全局命名空间，可以通过客户端挂载方式，访问不同挂载点访问不同空间</p>
<img src="images/image-20201203162311798.png" alt="image-20201203162311798" style="zoom: 50%;">
</blockquote>
<ol>
<li>多个名称节点，可扩展</li>
<li>性能更高效，多个名称节点提供对外服务</li>
<li>良好的隔离性</li>
</ol>
</li>
<li><p>依然未解决单点故障问题，各个名称节点不是备份关系，需要热备份</p>
</li>
</ul>
<h2 id="Yarn"><a href="#Yarn" class="headerlink" title="Yarn"></a>Yarn</h2><img src="images/image-20201203162908015.png" alt="image-20201203162908015" style="zoom: 67%;">

<p>MapReduce成为了纯粹的计算框架，不再负责资源调度管理服务</p>
<h3 id="体系结构-2"><a href="#体系结构-2" class="headerlink" title="体系结构"></a>体系结构</h3><p><img src="images/image-20201203220218297.png" alt="image-20201203220218297"></p>
<p><img src="images/image-20201203163117695.png" alt="image-20201203163117695"></p>
<h4 id="ResourceManager"><a href="#ResourceManager" class="headerlink" title="ResourceManager"></a>ResourceManager</h4><p>负责处理客户端请求；启动/监控ApplicationMaster；监控NodeManager、资源分配与调度</p>
<blockquote>
<p><strong>全局资源管理器</strong>，负责整个系统的资源管理和分配，主要包括两个组件：调度器和应用程序管理器</p>
</blockquote>
<h5 id="调度器"><a href="#调度器" class="headerlink" title="调度器"></a>调度器</h5><p>接收来自ApplicationMaster应用程序资源请求，把集群中的资源以容器的形式分配给提出申请的应用程序，容器的选择通常考虑应用程序所要处理的数据位置，进行就近选择从而实现<strong>计算向数据靠拢</strong></p>
<ul>
<li><strong>容器Container</strong>：作为动态资源分配单位，每个容器中都封装了一定数量的CPU、内存、磁盘等资源，从而限定每个应用程序可以使用的资源量</li>
</ul>
<p>调度器被设计成一个可插拔的组件，不仅自身提供了许多种直接可用的调度器，也可以允许用户根据自己需求重新设计调度器</p>
<h5 id="应用程序管理器"><a href="#应用程序管理器" class="headerlink" title="应用程序管理器"></a>应用程序管理器</h5><p>负责系统中<strong>所有应用程序的管理工作</strong>（管ApplicationMaster），主要包括应用程序提交、与调度器协商资源以启动ApplicationMaster、监控ApplicationMaster运行状态并在失败时重新启动</p>
<p><strong>过渡：</strong>ResourceManager接收用户提交的作业，按照作业的上下文信息以及NodeManager收集来的容器状态信息，启动调度过程，<strong>为用户作业启动一个ApplicationMaster</strong></p>
<h4 id="ApplicationMaster"><a href="#ApplicationMaster" class="headerlink" title="ApplicationMaster"></a>ApplicationMaster</h4><p>为应用程序申请资源并分配给内部任务；任务调度、监控与容错</p>
<p>功能：</p>
<ul>
<li>当用户<strong>作业提交</strong>（用户程序是以作业形式提交）时，ApplicationMaster与ResourceManager协商获取资源，ResourceManager以容器的形式给ApplicationMaster分配资源</li>
<li>把获得的资源进一步分配给内部的各个任务（Map Reduce），实现资源的<strong>二次分配</strong></li>
<li>与NodeManager<strong>保持交互通信</strong>进行应用程序的启动、运行、监控和停止，<strong>监控申请到的资源的使用情况</strong>，对所有任务的执行进度和状态进行监控，<strong>并在任务发生失败时执行失败恢复</strong>（即重新申请资源重启任务）</li>
<li>定时向ResourceManager发送心跳信息，报告资源的使用情况和应用的进度信息</li>
<li>作业完成时ApplicationMaster向ResourceManager注销容器，执行周期完成</li>
</ul>
<h4 id="NodeManager"><a href="#NodeManager" class="headerlink" title="NodeManager"></a>NodeManager</h4><p>单个节点上的资源管理；处理来自ResourceManager命令；处理来自ApplicationMaster命令</p>
<blockquote>
<p>驻留在一个YARN集群中每个节点上的代理，主要负责：</p>
<ul>
<li>容器生命周期管理</li>
<li>监控每个容器的资源（CPU、内存）使用情况</li>
<li>跟踪节点健康状况</li>
<li>以心跳方式和ResourceManager保持通信</li>
<li>向ResourceManager汇报作业的资源使用情况和每个容器的运行状态</li>
<li>接收来自ApplicationMaster的启动/停止容器的各种请求</li>
</ul>
</blockquote>
<p><strong>注意</strong>：NodeManager主要负责管理抽象的容器，只处理与容器相关的事情，而不具体负责每个任务（Map任务或Reduce任务）自身状态的管理，这些管理工作由ApplicationMaster完成，ApplicationMaster会通过不断与NodeManager通信来掌握各个任务的执行状态</p>
<h3 id="工作流程-1"><a href="#工作流程-1" class="headerlink" title="工作流程"></a>工作流程</h3><img src="images/image-20201203220408893.png" alt="image-20201203220408893" style="zoom: 80%;">

<blockquote>
<ol>
<li>用户编写客户端应用程序向YARN提交应用程序（3个）</li>
<li>YARN中的ResourceManager负责接收和处理来自客户端的请求，为应用程序<strong>分配一个容器</strong>，<strong>在该容器中启动</strong>一个ApplicationMaster</li>
<li>ApplicationMaster被创建后会首先向ResourceManager注册（才能监控管理）</li>
<li>ApplicationMaster采用轮询的方式向ResourceManager申请资源</li>
<li>ResourceManager以容器的形式向提出申请的ApplicationMaster分配资源</li>
<li>在容器中启动任务（运行环境、脚本）容器中会二次分配资源</li>
<li>各个任务向ApplicationMaster汇报自己的状态和进度</li>
<li>应用程序运行完成后，ApplicationMaster向ResourceManager的应用程序管理器注销并关闭自己</li>
</ol>
</blockquote>
<h3 id="发展目标"><a href="#发展目标" class="headerlink" title="发展目标"></a>发展目标</h3><ul>
<li>运行MapReduce批处理</li>
<li>运行流计算Storm</li>
<li>运行基于内存计算的Spark</li>
</ul>
<blockquote>
<p><strong>YARN的目标是实现一个集群多个框架，即在一个集群上部署一个统一的资源调度管理框架YARN，在YARN上可以部署其他各种计算框架</strong></p>
<p>因为企业会同时存在各种不同的业务应用场景，需要采用不同的计算框架，如</p>
<img src="images/image-20201203221934285.png" alt="image-20201203221934285" style="zoom:50%;">

<p>不同的计算框架会争抢资源，因此为了避免不同类型应用之间互相干扰，需要把内部的服务器拆分成多个集群，分别安装运行不同的计算框架即“一个框架一个集群”，然而集群资源利用率低（一些会处于空闲状态）；数据无法共享（集群之间隔离）；维护代价高</p>
<blockquote>
<p>YARN为这些计算框架提供统一的资源调度管理服务，并且能够根据各种计算框架的负载需求，调整各自占用的资源，实现集群资源共享和资源弹性收缩</p>
<p>可以实现一个集群上不同应用负载混搭，有效提高了集群的利用率</p>
<p>不同计算框架可以共享底层存储，避免了数据集跨集群移动，如下</p>
<img src="images/image-20201203222546157.png" alt="image-20201203222546157" style="zoom:50%;">
</blockquote>
</blockquote>
<h2 id="其他组件"><a href="#其他组件" class="headerlink" title="其他组件"></a>其他组件</h2><h3 id="Pig"><a href="#Pig" class="headerlink" title="Pig"></a>Pig</h3><ul>
<li>Apache项目中一个开源项目</li>
<li>提供了类似SQL的Pig Latin语言（包含Filter、GroupBy，Join，OrderBy等操作，同时也支持用户自定义函数）</li>
<li>用户只需要撰写非常简单的Pig Latin语句即可完成各种复杂的数据分析任务</li>
<li>Pig会自动把用户编写的脚本转换成MapReduce作业在Hadoop集群上运行，而且具备对生成的MapReduce程序进行自动优化功能</li>
<li>因此用户在编写Pig程序的时候，不需要关心程序的运行效率，大大减少了用户编程时间</li>
</ul>
<p>提供过滤、分组、连接、排序操作</p>
<p><strong>过程</strong>：</p>
<ol>
<li>加载数据</li>
<li>表达转换数据</li>
<li>存储最终结果</li>
</ol>
<blockquote>
<ul>
<li>通过LOAD语句去文件系统读取数据</li>
<li>通过一系列“转换”语句对数据进行处理</li>
<li>通过STORE语句把处理结果输出到文件当中去或者用DUMP语句把处理结果输出到屏幕上</li>
</ul>
<p>一般先数据收集，到Pig数据加工，然后到Hive做数据仓库进行海量数据分析</p>
</blockquote>
<img src="images/image-20201203223354254.png" alt="image-20201203223354254" style="zoom:80%;">

<p>转换为一堆Map任务与Reduce任务，其中一些横跨两个任务</p>
<img src="images/image-20201203223625184.png" alt="image-20201203223625184" style="zoom:50%;">





<h3 id="Tez"><a href="#Tez" class="headerlink" title="Tez"></a>Tez</h3><ul>
<li><p>Apache开源的支持DAG作业的计算框架，直接缘与MapReduce框架</p>
</li>
<li><p>核心思想是将Map和Reduce两个操作进一步拆分，分解后可以任意灵活组合，进行细粒度划分，产生新操作，经过一些控制程序组装后形成一个大的DAG作业，通过DAG作业方式运行MapReduce对多余操作和衔接部分处理，提供程序运行的整体处理逻辑</p>
<img src="images/image-20201203224029805.png" alt="image-20201203224029805" style="zoom:50%;">

<img src="images/image-20201203224038514.png" alt="image-20201203224038514" style="zoom:50%;">

</li>
</ul>
<p><img src="images/image-20201203224410083.png" alt="image-20201203224410083"></p>
<blockquote>
<p>四次的MapReduce任务变为右</p>
<p>优势：</p>
<ul>
<li><p>去除连续两个作业之间写入HDFS操作</p>
</li>
<li><p>去除每个工作流中多余的Map阶段</p>
<img src="images/image-20201203224619674.png" alt="image-20201203224619674" style="zoom:50%;">


</li>
</ul>
</blockquote>
<h3 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h3><img src="images/image-20201203224848721.png" alt="image-20201203224848721" style="zoom:50%;">

<blockquote>
<p>Spark是用于大规模数据处理的快速、通用引擎</p>
<ul>
<li>采用内存计算，带来更高的迭代运算效率</li>
<li>基于DAG的任务调度执行机制，优于MapReduce的迭代执行机制</li>
</ul>
</blockquote>
<h3 id="Kafka"><a href="#Kafka" class="headerlink" title="Kafka"></a>Kafka</h3><p>一种高吞吐量的分布式发布订阅消息系统，用户通过Kafka系统可以发布大量消息，同时也能实时订阅消费消息</p>
<ul>
<li><p>可以同时满足在线实时处理和批量离线处理</p>
</li>
<li><p>作为数据交换的中枢</p>
<blockquote>
<p><img src="images/image-20201203225124017.png" alt="image-20201203225124017"></p>
<ul>
<li>解决大规模数据存储，采用NoSQL</li>
<li>流计算Storm</li>
<li>内存计算Spark</li>
</ul>
<p>不同类型的分布式系统可以统一介入到Kafka，实现和Hadoop各个组件之间的不同类型数据的实时高效交换</p>
</blockquote>
</li>
</ul>
<h1 id="数据仓库Hive"><a href="#数据仓库Hive" class="headerlink" title="数据仓库Hive"></a>数据仓库Hive</h1><h2 id="概述-7"><a href="#概述-7" class="headerlink" title="概述"></a>概述</h2><p>数据仓库：</p>
<blockquote>
<p>面向主题的、集成的、相对稳定的、反映历史变化的数据集合，用于支持管理决策</p>
<p>为了支持企业内部商业分析和决策</p>
</blockquote>
<img src="images/image-20201204185222584.png" alt="image-20201204185222584" style="zoom:80%;">

<ul>
<li><p>Hive</p>
<img src="images/image-20201204185839070.png" alt="image-20201204185839070" style="zoom:50%;">
</li>
<li><p>Hive架构在底层的Hadoop核心组件基础上</p>
<p>依赖HDFS存储数据、依赖分布式并行计算模型MapReduce处理数据、借鉴SQL语言设计了新的查询语言HiveQL来运行MapReduce任务实现数据仓库分析，类似SQL接口容易移植</p>
</li>
<li><p>Hive的特性：</p>
<ol>
<li>采用批处理方式处理海量数据<ul>
<li>Hive把HiveQL语句转换成MapReduce任务进行运行</li>
<li>数据仓库存储的是静态数据，对静态数据的分析适合采用批处理方式，不需要快速响应给出结构，而且数据本身也不会频繁变换</li>
</ul>
</li>
<li>Hive提供了一系列对数据进行提取、转换、加载(ETL)工具。可以存储、查询和分析存储在Hadoop中的大规模数据，能很好的满足数据仓库各种应用场景</li>
</ol>
</li>
<li><p>Hive在Hadoop生态系统</p>
<p>与Pig类似，但是Pig轻量级，适合实时交互性分析，主要用于ETL环节</p>
<p>Hive主要用于数据仓库海量数据的批处理分析</p>
<img src="images/image-20201204190514466.png" alt="image-20201204190514466" style="zoom:50%;">
</li>
<li><p>Hive与传统数据库差别</p>
<img src="images/image-20201204190753638.png" alt="image-20201204190753638" style="zoom:50%;">

<blockquote>
<p>Hive数据仓库保存静态数据，不允许更新；Hive需要海量数据批处理无法实时；Hive水平可扩展性很好</p>
</blockquote>
</li>
<li><p>Hive在企业大数据分析平台中的应用</p>
<img src="images/image-20201204190934371.png" alt="image-20201204190934371" style="zoom:33%;">
</li>
<li><p>Hive系统架构</p>
<img src="images/image-20201204191224514.png" alt="image-20201204191224514" style="zoom:50%;">

<blockquote>
<p>较重要的模块：</p>
<ul>
<li><p>对外接口模块</p>
<p>CLI:一种命令行工具</p>
<p>HWI:Hive Web Interface是Hive的Web接口</p>
<p>JDBC和ODBC:开放数据库连接接口</p>
<p>Thrift Server:基于Thrift架构开发的接口，允许外界通过该接口实现对Hive仓库的RPC调用</p>
</li>
<li><p>驱动模块</p>
<p>包含编译器、优化器、执行器</p>
<p>就是把HiveQL语句转换成一系列MapReduce作业</p>
</li>
<li><p>元数据存储模块Metastore</p>
<p>一个独立的关系型数据库</p>
<p>通过MySQL</p>
</li>
</ul>
</blockquote>
</li>
<li><p>Hive HA</p>
<p>高可用性Hive解决方案</p>
<p>在很多时候Hive表现出不稳定，因此提出Hive HA，即通过整个Hive集群，设置多个Hive实例，放入统一资源池，外界访问设置统一访问接口HAProxy</p>
<img src="images/image-20201204191700371.png" alt="image-20201204191700371" style="zoom:67%;">

<blockquote>
<ul>
<li>访问HA Proxy</li>
<li>一次询问Hive实例，执行逻辑可用性测试，不通过加入黑名单</li>
<li>每隔一定周期，HA Proxy重新对列入黑名单的实例进行统一处理</li>
</ul>
</blockquote>
</li>
</ul>
<h2 id="SQL转MapReduce作业原理"><a href="#SQL转MapReduce作业原理" class="headerlink" title="SQL转MapReduce作业原理"></a>SQL转MapReduce作业原理</h2><p>举例如连接操作：</p>
<ol>
<li>编写一个Map处理逻辑</li>
<li>Map处理逻辑输入关系数据库的表</li>
<li>每条记录通过Map转换</li>
</ol>
<p><strong>原理：</strong></p>
<p>当用户向Hive输入一段命令或查询时，Hive需要与Hadoop交互工作来完成该操作</p>
<ul>
<li>驱动模块接收该命令或查询编译器</li>
<li>对该命令或查询进行解析编译</li>
<li>由优化器对该命令或查询进行优化计算</li>
<li>该命令或查询通过执行器进行执行</li>
</ul>
<img src="images/image-20201204193536637.png" alt="image-20201204193536637" style="zoom: 80%;">

<blockquote>
<p>HiveQL输入后</p>
<ol>
<li>由Hive驱动模块中的编译器对用户输入的SQL语言进行词法和语法解析， 将SQL语句转化为抽象语法树形式</li>
<li>抽象语法树仍然复杂，因此转化为查询块</li>
<li>把查询块转换成逻辑查询计划，里面包含了许多逻辑操作符</li>
<li>重写逻辑查询计划，进行优化合并多余操作，减少MapReduce任务数量</li>
<li>把逻辑操作符转换成需要执行的具体MapReduce任务</li>
<li>对生成的MapReduce任务进行优化生成最终的MapReduce任务执行计划</li>
<li>由Hive驱动模块中的执行器对最终的MapReduce任务进行执行输出</li>
</ol>
<p>Hive本身不生成MapReduce程序</p>
<p>而是通过一个表示“JOB执行计划”的XML文件驱动执行内置的、原生的Mapper和Reducer模块</p>
<p>Hive通过和JobTracker通信来初始化MapReduce任务，不必直接部署在JobTracker所在的管理节点上执行</p>
<p>通常在大型集群上，有专门的网关机来部署Hive工具，与远程JobTracker通信来完成具体任务</p>
</blockquote>
<h2 id="Impala"><a href="#Impala" class="headerlink" title="Impala"></a>Impala</h2><ul>
<li><p>Impala的允许需要依赖于Hive的元数据，参照Dremel系统进行设计，响应高</p>
</li>
<li><p>不需要转化为MapReduce任务，采用了与商用并行关系数据库类似的分布式查询引擎，直接与HDFS和HBase进行交互查询，数据存储在HDFS和HBase中</p>
<img src="images/image-20201204194448198.png" alt="image-20201204194448198" style="zoom:67%;">
</li>
<li><p>和Hive采用相同的SQL语法ODBC驱动程序和用户接口</p>
</li>
</ul>
<img src="images/image-20201204195246149.png" alt="image-20201204195246149" style="zoom:50%;">

<h3 id="系统架构-1"><a href="#系统架构-1" class="headerlink" title="系统架构"></a>系统架构</h3><p><img src="images/image-20201204194538924.png" alt="image-20201204194538924"></p>
<blockquote>
<p>Impala组件并不是单独部署，和Hadoop其他组件一起部署，部署于同一个集群，因此可以用底层的HDFS和HBase</p>
</blockquote>
<p><strong>Impala的组件</strong>：</p>
<ul>
<li>Impalad：负责具体的相关查询任务</li>
<li>State Store：负责元数据管理和状态信息维护</li>
<li>CLI：用户访问接口</li>
</ul>
<h4 id="Impalad"><a href="#Impalad" class="headerlink" title="Impalad"></a>Impalad</h4><p>驻留在各个不同数据节点上的不同的相关进程</p>
<blockquote>
<p>包含Query Planner、Query Coordinator、Query Exec Engine三个模块</p>
</blockquote>
<ul>
<li>负责协调客户端提交的查询的执行</li>
<li>与HDFS的数据节点（HDFS DN）允许在同一节点</li>
<li>给其他Impalad分配任务以及收集其他Impalad的执行结果进行汇总（分布）</li>
<li>也会执行其他Impalad给其分配的任务，对本地HDFS和HBase里的部分数据进行操作</li>
<li></li>
</ul>
<h4 id="State-Store"><a href="#State-Store" class="headerlink" title="State Store"></a>State Store</h4><p>每个查询的提交都会创建一个StateStore进程，其来跟踪监控各个查询任务</p>
<ul>
<li>负责收集分布在集群中各个Impalad进程的资源信息用于查询调度</li>
</ul>
<h4 id="CLI"><a href="#CLI" class="headerlink" title="CLI"></a>CLI</h4><p>给用户提供查询使用的命令行工具</p>
<p>提供了Hue、JDBC、ODBC的使用接口</p>
<h3 id="查询执行过程"><a href="#查询执行过程" class="headerlink" title="查询执行过程"></a>查询执行过程</h3><img src="images/image-20201204195324284.png" alt="image-20201204195324284" style="zoom:50%;">

<blockquote>
<ul>
<li>当用户提交查询前，Impala先创建一个负责协调客户端提交的查询的Impalad进程，该进程会向Impala State Store提交注册订阅信息，State Store会创建一个statestored进程，其通过创建多个线程来处理Impalad的注册订阅信息</li>
<li>用户通过CLI客户端提交一个查询到impalad进程，Impalad的Query Planner对SQL语句解析，生成解析树，Planner把这个查询的解析树变成若干PlanFragment，发送到Query Coordinator</li>
<li>Coordinator通过从MySQL源数据库中获取元数据，从HDFS名称节点中获取数据地址，以得到存储这个查询相关数据的所有数据节点</li>
<li>Coordinator初始化相应impalad上的任务执行，即把查询任务分配给所有存储该查询相关数据的数据节点</li>
<li>Query Executor通过流式交换中间输出，并由Query Coordinator汇聚来自各个impalad的结果</li>
<li>Coordinator把汇总后的结果返回给CLI客户端</li>
</ul>
</blockquote>
<h3 id="与Hive对比"><a href="#与Hive对比" class="headerlink" title="与Hive对比"></a>与Hive对比</h3><p><strong>不同点：</strong></p>
<ul>
<li><p>Hive适合长时间的批处理查询分析，而Impala适合实时交互式查询，因为Hive构建在MapReduce上，需要转换，impala直接架构在底层存储之上</p>
<img src="images/image-20201204202728443.png" alt="image-20201204202728443" style="zoom:50%;">
</li>
<li><p>Hive依赖MapReduce计算框架，Impala把执行计划表现为一棵完整的执行计划树，直接分发执行计划到各个Impalad执行查询</p>
</li>
<li><p>Hive执行中，当内存放不下所有数据，会使用外存</p>
<p>而Impala在遇到内存不足，不会利用外存，因此处理查询其实受到限制</p>
</li>
</ul>
<p><strong>相同点：</strong></p>
<ul>
<li>使用系统的存储数据池都支持把数据存储于HDFS和HBase中</li>
<li>Hive与Impala使用相同的元数据</li>
<li>Hive与Impala中对SQL的解释处理相似，都是通过词法分析生成执行计划</li>
</ul>
<blockquote>
<p>Impala不是替换Hive工具，而是弥补不足，如实时性</p>
<p>实际上可以和Hive组合使用，如用Hive进行数据转换处理，然后再用Impala在Hive处理后的结果数据集上进行快速的数据分析</p>
</blockquote>
<h2 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h2><h3 id="基本数据类型"><a href="#基本数据类型" class="headerlink" title="基本数据类型"></a>基本数据类型</h3><p>Hive支持基本数据类型和复杂类型, 基本数据类型主要有数值类型(INT、FLOAT、DOUBLE ) 、布尔型和字符串, 复杂类型有三种:ARRAY、MAP 和 STRUCT。</p>
<h4 id="基本数据类型-1"><a href="#基本数据类型-1" class="headerlink" title="基本数据类型"></a>基本数据类型</h4><ul>
<li>TINYINT: 1个字节</li>
<li>SMALLINT: 2个字节</li>
<li>INT: 4个字节</li>
<li>BIGINT: 8个字节</li>
<li>BOOLEAN: TRUE/FALSE</li>
<li>FLOAT: 4个字节，单精度浮点型</li>
<li>DOUBLE: 8个字节，双精度浮点型STRING 字符串</li>
</ul>
<h4 id="复杂数据类型"><a href="#复杂数据类型" class="headerlink" title="复杂数据类型"></a>复杂数据类型</h4><ul>
<li>ARRAY: 有序字段</li>
<li>MAP: 无序字段</li>
<li>STRUCT: 一组命名的字段</li>
</ul>
<h3 id="操作命令"><a href="#操作命令" class="headerlink" title="操作命令"></a>操作命令</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> database if <span class="keyword">not</span> <span class="keyword">exists</span> hive;       #创建数据库</span><br><span class="line"><span class="keyword">show</span> databases;                           #查看Hive中包含数据库</span><br><span class="line"><span class="keyword">show</span> databases <span class="keyword">like</span> <span class="string">&#x27;h.*&#x27;</span>;                #查看Hive中以h开头数据库</span><br><span class="line"><span class="keyword">describe</span> databases;                       #查看hive数据库位置等信息</span><br><span class="line"><span class="keyword">alter</span> database hive <span class="keyword">set</span> dbproperties;     #为hive设置键值对属性</span><br><span class="line">use hive;                                 #切换到hive数据库下</span><br><span class="line"><span class="keyword">drop</span> database if <span class="keyword">exists</span> hive;             #删除不含表的数据库</span><br><span class="line"><span class="keyword">drop</span> database if <span class="keyword">exists</span> hive cascade;     #删除数据库和它中的表</span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">#创建内部表（管理表）</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> if <span class="keyword">not</span> <span class="keyword">exists</span> hive.usr(</span><br><span class="line">      name string comment <span class="string">&#x27;username&#x27;</span>,</span><br><span class="line">      pwd string comment <span class="string">&#x27;password&#x27;</span>,</span><br><span class="line">      address struct<span class="operator">&lt;</span>street:string,city:string,state:string,zip:<span class="type">int</span><span class="operator">&gt;</span> comment  <span class="string">&#x27;home address&#x27;</span>,</span><br><span class="line">      identify map<span class="operator">&lt;</span><span class="type">int</span>,tinyint<span class="operator">&gt;</span> comment <span class="string">&#x27;number,sex&#x27;</span>) </span><br><span class="line">      comment <span class="string">&#x27;description of the table&#x27;</span>  </span><br><span class="line">     tblproperties(<span class="string">&#x27;creator&#x27;</span><span class="operator">=</span><span class="string">&#x27;me&#x27;</span>,<span class="string">&#x27;time&#x27;</span><span class="operator">=</span><span class="string">&#x27;2016.1.1&#x27;</span>); </span><br><span class="line">#创建外部表</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> if <span class="keyword">not</span> <span class="keyword">exists</span> usr2(</span><br><span class="line">      name string,</span><br><span class="line">      pwd string,</span><br><span class="line">  address struct<span class="operator">&lt;</span>street:string,city:string,state:string,zip:<span class="type">int</span><span class="operator">&gt;</span>,</span><br><span class="line">      identify map<span class="operator">&lt;</span><span class="type">int</span>,tinyint<span class="operator">&gt;</span>) </span><br><span class="line">      <span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span></span><br><span class="line">     location <span class="string">&#x27;/usr/local/hive/warehouse/hive.db/usr&#x27;</span>; </span><br><span class="line">#创建分区表</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> if <span class="keyword">not</span> <span class="keyword">exists</span> usr3(</span><br><span class="line">      name string,</span><br><span class="line">      pwd string,</span><br><span class="line">      address struct<span class="operator">&lt;</span>street:string,city:string,state:string,zip:<span class="type">int</span><span class="operator">&gt;</span>,</span><br><span class="line">      identify map<span class="operator">&lt;</span><span class="type">int</span>,tinyint<span class="operator">&gt;</span>) </span><br><span class="line">      partitioned <span class="keyword">by</span>(city string,state string);    </span><br><span class="line">#复制usr表的表模式  </span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> if <span class="keyword">not</span> <span class="keyword">exists</span> hive.usr1 <span class="keyword">like</span> hive.usr;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">show</span> tables <span class="keyword">in</span> hive;  </span><br><span class="line"><span class="keyword">show</span> tables <span class="string">&#x27;u.*&#x27;</span>;        #查看hive中以u开头的表</span><br><span class="line"><span class="keyword">describe</span> hive.usr;        #查看usr表相关信息</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> usr rename <span class="keyword">to</span> custom;      #重命名表</span><br><span class="line"> </span><br><span class="line">#为表增加一个分区</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> usr3 <span class="keyword">add</span> if <span class="keyword">not</span> <span class="keyword">exists</span> <span class="keyword">partition</span>(city<span class="operator">=</span><span class="string">&#x27;beijing&#x27;</span>,state<span class="operator">=</span><span class="string">&#x27;China&#x27;</span>) location <span class="string">&#x27;/usr/local/hive/warehouse/usr3/China/beijing&#x27;</span>; </span><br><span class="line">#修改分区路径</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> usr3 <span class="keyword">partition</span>(city<span class="operator">=</span>&quot;beijing&quot;,state<span class="operator">=</span>&quot;China&quot;)</span><br><span class="line">     <span class="keyword">set</span> location <span class="string">&#x27;/usr/local/hive/warehouse/usr3/CH/beijing&#x27;</span>;</span><br><span class="line">#删除分区</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> usr3 <span class="keyword">drop</span> if <span class="keyword">exists</span>  <span class="keyword">partition</span>(city<span class="operator">=</span><span class="string">&#x27;beijing&#x27;</span>,state<span class="operator">=</span><span class="string">&#x27;China&#x27;</span>);</span><br><span class="line">#修改列信息</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> custom change <span class="keyword">column</span> name username string after pwd;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> custom <span class="keyword">add</span> columns(hobby string);                  #增加列</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> custom replace columns(uname string);              #删除替换列</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> custom <span class="keyword">set</span> tblproperties(<span class="string">&#x27;creator&#x27;</span><span class="operator">=</span><span class="string">&#x27;liming&#x27;</span>);      #修改表属性</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> usr3 <span class="keyword">partition</span>(city<span class="operator">=</span>”beijing”,state<span class="operator">=</span>”China”) <span class="keyword">set</span> fileformat sequencefile;    #修改存储属性           </span><br><span class="line">use hive;                                                   #切换到hive数据库下</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> if <span class="keyword">exists</span> usr1;                                  #删除表</span><br><span class="line"><span class="keyword">drop</span> database if <span class="keyword">exists</span> hive cascade;                       #删除数据库和它中的表</span><br></pre></td></tr></table></figure>
<p><a target="_blank" rel="noopener" href="http://dblab.xmu.edu.cn/blog/2440-2/">http://dblab.xmu.edu.cn/blog/2440-2/</a> 更多实操见此</p>
<h1 id="Spark-1"><a href="#Spark-1" class="headerlink" title="Spark"></a>Spark</h1><p>使用DAG执行引擎以支持循环数据流与内存计算；可交互式编程；提供完整强大的技术栈，包括SQL查询、流式计算、机器学习和图算法组件；运行模式多样，可运行于独立的集群模式中。</p>
<img src="images/image-20201207124634936.png" alt="image-20201207124634936" style="zoom:50%;">

<blockquote>
<p>相比于Hadoop MapReduce，Spark提供了内存计算，将中间结果存放到内存中，对于迭代运算效率更高</p>
<p><img src="images/image-20201207125035911.png" alt="image-20201207125035911" style="zoom:50%;"><img src="images/image-20201207125202326.png" alt="image-20201207125202326"></p>
</blockquote>
<img src="images/image-20201207125206002.png" alt="image-20201207125206002" style="zoom:50%;">

<h2 id="生态系统"><a href="#生态系统" class="headerlink" title="生态系统"></a>生态系统</h2><p>实际应用中大数据处理需要复杂的批量数据处理，通常时间跨度大、基于历史数据交互式查询、基于实时数据流的数据处理</p>
<blockquote>
<p>以上情形需要不同的软件，如MapReduce解决复杂的批量数据处理、Impala基于历史数据的交互式查询、Storm解决实时数据流处理。然而不同输入输出数据无法做到无缝连接，需要格式转换、维护成本高、难以统一资源协调和分配</p>
</blockquote>
<blockquote>
<p>Spark设计使一个软件栈满足不同应用场景</p>
<ul>
<li>既能够提供内存计算框架</li>
<li>也可以支持SQL即时查询、实时流式计算、机器学习和图计算等</li>
</ul>
<img src="images/image-20201207132254729.png" alt="image-20201207132254729" style="zoom:50%;">
</blockquote>
<img src="images/image-20201207132432340.png" alt="image-20201207132432340" style="zoom:50%;">

<p><strong>应用场景：</strong></p>
<p><img src="images/image-20201207132506555.png" alt="image-20201207132506555"></p>
<h2 id="运行架构"><a href="#运行架构" class="headerlink" title="运行架构"></a>运行架构</h2><h3 id="概述和架构设计"><a href="#概述和架构设计" class="headerlink" title="概述和架构设计"></a>概述和架构设计</h3><p>RDD：<strong>弹性分布式数据集</strong>，是分布式内存，提供高度受限的共享内存模型（只读），一种数据抽象</p>
<p>DAG：有向无环图，反映RDD之间依赖关系，RDD之间一次又一次操作生成了有向无环图</p>
<p>Executor：运行在工作节点的一个<strong>进程</strong>，负责运行Task</p>
<p>Task：运行在Executor上工作单元</p>
<p>Application：用户编写的Spark应用程序</p>
<p>Job：包含多个RDD及作用于相应RDD上各种操作</p>
<p>stage：Job基本调度单位，一个Job分为多组Task，每组Task被称为Stage，或者也被称为TaskSet，代表一组关联的、相互之间没有Shuffle依赖关系的任务组成的任务集</p>
<h4 id="运行架构-1"><a href="#运行架构-1" class="headerlink" title="运行架构"></a>运行架构</h4><img src="images/image-20201207133223594.png" alt="image-20201207133223594" style="zoom:67%;">

<blockquote>
<ul>
<li>集群资源管理器负责对整个应用程序的资源分配和调度，CPU内存带宽，可以自带、或者yarn或者mesos</li>
<li>Worker Node负责运行具体任务</li>
<li>Driver类似管家，负责应用控制，对任务阶段拆解</li>
</ul>
</blockquote>
<h5 id="优点："><a href="#优点：" class="headerlink" title="优点："></a>优点：</h5><ul>
<li><p><strong>Executor采用多线程来执行具体任务</strong>，减少任务的启动开销</p>
</li>
<li><p>Executor存在一个BlockManager存储模块，<strong>将内存和磁盘共同作为存储设备</strong>，有效减少IO开销</p>
</li>
</ul>
<h4 id="运行过程"><a href="#运行过程" class="headerlink" title="运行过程"></a>运行过程</h4><img src="images/image-20201207133438254.png" alt="image-20201207133438254" style="zoom: 67%;">

<blockquote>
<p>一个Application由一个Driver和若干个Job构成，一个Job由多个stage构成，一个stage由多个没有shuffle关系的Task组成</p>
<img src="images/image-20201207133726689.png" alt="image-20201207133726689" style="zoom:67%;">

<p>当执行一个Application，Driver会向集群管理器申请资源，启动Executor并向其发送应用程序代码和文件，然后在Executor上执行Task，运行结束后，执行结果返回给Driver，或者写到HDFS或其他数据库中</p>
</blockquote>
<h3 id="Spark运行基本流程"><a href="#Spark运行基本流程" class="headerlink" title="Spark运行基本流程"></a>Spark运行基本流程</h3><img src="images/image-20201207134833580.png" alt="image-20201207134833580" style="zoom: 67%;">

<blockquote>
<ul>
<li><p>为应用构建起基本的运行环境，即由Driver创建一个<strong>SparkContext（指挥官）</strong>进行<strong>资源的申请、任务的分配和监控，与资源管理器不断沟通</strong>。</p>
<img src="images/image-20201214111754722.png" alt="image-20201214111754722" style="zoom:50%;">
</li>
<li><p>资源管理器为Executor分配资源，并启动Executor进程，启动后不断汇报资源使用情况</p>
<img src="images/image-20201214111814311.png" alt="image-20201214111814311" style="zoom:50%;">
</li>
<li><p>SparkContext（Driver生成，管家，连接集群的通道）<strong>根据RDD的依赖构建DAG图（代码就是对RDD一次次操作）</strong>，DAG图提交给DAGScheduler解析成Stage，然后把一个个TaskSet提交给底层调度器TaskScheduler处理</p>
<img src="images/image-20201214111911415.png" alt="image-20201214111911415" style="zoom:50%;">
</li>
<li><p>Executor向SparkContext申请Task，TaskScheduler将Task发放给Executor运行并提供应用程序代码</p>
</li>
<li><p>Task在Executor上运行把执行结果反馈给TaskScheduler，然后反馈给DAGScheduler，运行完毕后写入数据并释放所有资源</p>
<img src="images/image-20201214111954742.png" alt="image-20201214111954742" style="zoom:50%;">

<p>每个阶段若干个任务，然后把各个阶段提交给Task Scheduler负责分发任务，由Worker Node主动申请运行，根据申请情况，把任务扔到对应node上，让进程派线程执行（保证计算向数据靠拢）</p>
<img src="images/image-20201214112143169.png" alt="image-20201214112143169" style="zoom:50%;">

<p>任务运行后，得到结果反馈</p>
<img src="images/image-20201214112301347.png" alt="image-20201214112301347" style="zoom:50%;">

</li>
</ul>
</blockquote>
<img src="images/image-20201207163031364.png" alt="image-20201207163031364" style="zoom: 50%;">

<blockquote>
<p>数据本地性：执行计算时候，让计算和数据靠拢，让Task靠近数据所在运行</p>
<p>推测执行：当资源调光，需要判断是否迁移，根据迁移代价和等待代价</p>
</blockquote>
<h3 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a>RDD</h3><h4 id="概述-8"><a href="#概述-8" class="headerlink" title="概述"></a>概述</h4><p>迭代计算需要不同计算阶段重用中间结果，而且MapReduce的磁盘读写开销大</p>
<blockquote>
<p>而RDD提供了一种抽象的数据结构；不必担心底层数据的分布特性，只需要具体的应用逻辑表达为一系列转换处理；不同RDD之间的转换操作形<strong>成依赖，可以实现管道化</strong>，避免中间数据存储</p>
</blockquote>
<ul>
<li><p>RDD就是一个分布式对象集合，本质上是一个<strong>只读</strong>的分区记录集合，每个RDD可分成多个分区，每个分区就是一个数据集片段，并且一个RDD的不同分区可以被保存到集群中不同的节点上，<strong>从而可以在集群中的不同节点进行并行计算</strong></p>
<blockquote>
<p>就是因为是只读，因此RDD提供的是一种<strong>高度受限的共享内存模型</strong>，即RDD是只读的记录分区的集合，不能直接修改，<em>只能基于稳定的物理存储中的数据集创建RDD，或者通过在其他RDD上执行确定的转换操作如map、join、group by而创建得到新的RDD</em></p>
</blockquote>
</li>
<li><p>RDD提供了一组丰富的操作支持常见的数据运算，分为动作（action）和转换(transformation)，提供的转换接口非常简单，是粗粒度的数据转换操作，不是针对某个数据项的细粒度修改，因此不适合爬虫</p>
<blockquote>
<p>转换类型的操作：<strong>只记录</strong>转换的轨迹，而不进行真正的计算。只进行粗粒度的转换，即一次只能针对RDD全集进行转换</p>
<p>动作类型的操作：才进行从头到尾的计算</p>
</blockquote>
</li>
<li><p>表面上RDD功能受限，实际上可以高效表达许多框架的编程模型</p>
</li>
</ul>
<p><strong>典型执行过程：</strong></p>
<ul>
<li><p>RDD<strong>读入</strong>外部数据源进行创建</p>
</li>
<li><p>RDD经过系列转换操作每次都会产生不同的RDD供给下一个转换操作使用</p>
</li>
<li><p>最后一个RDD经过动作操作进行转换并输出到<strong>外部数据源</strong></p>
<img src="images/image-20201207164121220.png" alt="image-20201207164121220" style="zoom: 25%;">

<p><strong>惰性机制</strong>，只有最后动作执行才生成结果，<strong>这一系列处理称为一个Lineage血缘关系，即DAG拓扑排序结果</strong></p>
</li>
</ul>
<blockquote>
<p>惰性调用、管道化、避免同步等待、不需要保存中间结果、每次操作变得简单</p>
</blockquote>
<img src="images/image-20201214113544071.png" alt="image-20201214113544071" style="zoom:50%;">



<h4 id="特性"><a href="#特性" class="headerlink" title="特性"></a>特性</h4><p>高效计算：</p>
<ul>
<li><p>高效的容错机制</p>
<blockquote>
<p>由于现有的容错机制：数据复制或者记录日志（事务机制），<strong>数据密集型应用代价非常高</strong></p>
<p>RDD的血缘关系、重新计算丢失分区、<strong>无需回滚系统</strong>、重算过程在不同节点之间<strong>并行</strong>、只记录粗粒度操作。天然容错机制</p>
</blockquote>
</li>
<li><p><strong>中间结果持久化到内存</strong>，数据在内存中的多个RDD操作之间进行传递， 避免了不必要的读写磁盘开销</p>
<img src="images/image-20201214114556088.png" alt="image-20201214114556088" style="zoom:50%;">
</li>
<li><p>存放的数据可以是Java对象，避免了不必要的对象序列化和反序列化</p>
</li>
</ul>
<h4 id="依赖关系和运行过程"><a href="#依赖关系和运行过程" class="headerlink" title="依赖关系和运行过程"></a>依赖关系和运行过程</h4><p><strong>依赖关系</strong>：窄依赖与宽依赖</p>
<blockquote>
<p>RDD之间的依赖关系是划分stage的依据</p>
</blockquote>
<img src="images/image-20201207165019890.png" alt="image-20201207165019890" style="zoom:50%;">

<blockquote>
<p>窄依赖：表现为一个父RDD的分区对应于一个子RDD的分区，或多个父RDD的分区对应于一个子RDD的分区</p>
<p>宽依赖：表现为存在一个父RDD的一个分区对应于一个子RDD的多个分区，只要发生shuffle（大量的网络之间的数据分发）则是宽依赖</p>
</blockquote>
<p>Spark通过分析各个RDD的依赖关系生成DAG；再通过分析各个RDD的分区之间依赖关系划分stage，<strong>具体方法</strong>如下</p>
<ul>
<li>在DAG中进行<strong>反向解析</strong>，遇到宽依赖断开</li>
<li>遇到窄依赖把当前的RDD加入到stage中</li>
<li>将窄依赖尽量划分在同一个stage中，可以实现流水线计算，从而使得数据可以直接在内存中进行变换，避免了磁盘IO开销</li>
</ul>
<blockquote>
<p>由于：<strong>窄依赖可以进行流水线优化，宽依赖不能进行流水线优化</strong></p>
<p>优化原理：fork/join机制，fork/join是并行执行任务的框架。为了并行执行，fork到不同机器一起执行，再将结果进行join，每次转换都进行一次fork/join。因此对于这种的转换进行优化，如例：</p>
<p>取消不必要的路障，能够流水线尽量流水线，窄依赖就不需要等待</p>
<img src="images/image-20201214120411122.png" alt="image-20201214120411122" style="zoom: 33%;">

<img src="images/image-20201214120422975.png" alt="image-20201214120422975" style="zoom:33%;">

<p>上为窄依赖，下为宽依赖，进行了一次shuffle</p>
<img src="images/image-20201214120648262.png" alt="image-20201214120648262" style="zoom:33%;">
</blockquote>
<p><strong>划分的stage分为</strong></p>
<ul>
<li>ShuffleMapStage：不是最终的stage，所以输出一定经过shuffle过程，并作为后续stage的输入。以shuffle为输出边界，其输入边界可以是从外部获取数据，也可以是另一个shuffleMapStage的输出其输出可以是另一个stage的开始。在一个job可以没有该stage</li>
<li>ResultStage：最终的stage，无输出，直接产生结果或存储。输入边界可以是从外部获取数据，也可以是另一个shufflemapstage输出。一个job必定有该stage，至少含一个。</li>
</ul>
<img src="images/image-20201207183149765.png" alt="image-20201207183149765" style="zoom:50%;">

<h2 id="SparkSQL"><a href="#SparkSQL" class="headerlink" title="SparkSQL"></a>SparkSQL</h2><p>hive on spark：shark</p>
<blockquote>
<p>shark沿用hive中各个组件，完成SQL解析，只有在最后物理计算时候生成Spark程序</p>
<p><strong>不足：</strong></p>
<ul>
<li>执行计划完全依赖于hive，不方便添加新的优化策略</li>
<li>spark线程级别并行，而MapReduce是进程并行，因此兼容上存在线程安全问题，需要用另外一套独立维护的打了补丁的Hive源码分支</li>
</ul>
<p>由此有了SparkSQL</p>
<img src="images/image-20201207184212479.png" alt="image-20201207184212479" style="zoom: 50%;">

<img src="images/image-20201207184332530.png" alt="image-20201207184332530" style="zoom:50%;">
</blockquote>
<p>Spark SQL增加SchemaRDD让用户在Spark SQL中执行SQL语句</p>
<h2 id="部署和应用"><a href="#部署和应用" class="headerlink" title="部署和应用"></a>部署和应用</h2><p><strong>三种部署：</strong></p>
<ul>
<li>standalone：slot为资源分配单位，自带集群资源管理器</li>
<li>spark on mesos</li>
<li>spark on yarn</li>
</ul>
<p><strong>应用</strong></p>
<ul>
<li>采用hadoop+storm部署方式</li>
</ul>
<img src="images/image-20201207191723260.png" alt="image-20201207191723260" style="zoom:50%;">

<blockquote>
<p>实时通过storm做流式处理，hadoop批量处理</p>
<p>但繁琐，采用spark</p>
</blockquote>
<ul>
<li>spark</li>
</ul>
<blockquote>
<p>用Streaming做实时数据处理分析，批量计算用spark，交互式用sparkSQL，实现一站式服务</p>
<img src="images/image-20201207191835131.png" alt="image-20201207191835131" style="zoom:50%;">
</blockquote>
<ul>
<li><p>还可用hadoop+spark，因为一些hadoop的功能spark无法取代。</p>
<p>可以实现资源按需伸缩；不同负载应用混搭，集群利用率高；共享底层存储，避免数据跨集群迁移</p>
<img src="images/image-20201207192021460.png" alt="image-20201207192021460" style="zoom:50%;">

</li>
</ul>
<h2 id="shell"><a href="#shell" class="headerlink" title="shell"></a>shell</h2><h3 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd /usr/local/spark</span><br><span class="line">bin/spark-shell</span><br></pre></td></tr></table></figure>
<img src="images/image-20201210103422953.png" alt="image-20201210103422953" style="zoom:67%;">

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./bin/spark-shell --master &lt;master-url&gt;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>master-url：</p>
<ul>
<li><p>local：一个worker线程运行spark</p>
</li>
<li><p>local|*|：使用逻辑CPU个数数量的线程运行spark。如2个物理CPU，每个CPU有2给Core则为4，<strong>也是默认进入模式</strong></p>
</li>
<li><p>spark:HOST:PORT：独立集群模式，如spark://localhost:7077</p>
</li>
<li><p>yarn的两种模式：</p>
<p>yarn-client：指挥部在客户端，不可关。调试程序时可用</p>
<p>yarn-cluster：在集群中放指挥部。进入生产环境时可用</p>
</li>
<li><p>mesos:HOST:PORT：mesos集群模式</p>
</li>
</ul>
<img src="images/image-20201214122846707.png" alt="image-20201214122846707" style="zoom:50%;">

<img src="images/image-20201214122857312.png" alt="image-20201214122857312" style="zoom:50%;">


</blockquote>
<h3 id="加载text文件"><a href="#加载text文件" class="headerlink" title="加载text文件"></a>加载text文件</h3><p>spark创建sc，可以加载本地文件和HDFS文件创建RDD</p>
<blockquote>
<p>加载HDFS文件和本地文件都是使用textFile，区别是添加<strong>前缀</strong>(hdfs://和file:///)进行标识</p>
</blockquote>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> textFile = sc.textFile(<span class="string">&quot;file:///usr/local/spark/README.md&quot;</span>)</span><br></pre></td></tr></table></figure>


<h3 id="简单RDD操作"><a href="#简单RDD操作" class="headerlink" title="简单RDD操作"></a>简单RDD操作</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//获取RDD文件textFile的第一行内容</span></span><br><span class="line">textFile.first()</span><br><span class="line"><span class="comment">//获取RDD文件textFile所有项的计数</span></span><br><span class="line">textFile.count()</span><br><span class="line"><span class="comment">//抽取含有“Spark”的行，返回一个新的RDD</span></span><br><span class="line"><span class="keyword">val</span> lineWithSpark = textFile.filter(line =&gt; line.contains(<span class="string">&quot;Spark&quot;</span>))</span><br><span class="line"><span class="comment">//统计新的RDD的行数</span></span><br><span class="line">lineWithSpark.count()</span><br></pre></td></tr></table></figure>
<p>可以通过组合RDD操作进行组合，可以实现简易MapReduce操作</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//找出文本中每行的最多单词数</span></span><br><span class="line">textFile.map(line =&gt; line.split(<span class="string">&quot; &quot;</span>).size).reduce((a, b) =&gt; <span class="keyword">if</span> (a &gt; b) a <span class="keyword">else</span> b)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>在Spark程序中必须创建一个SparkContext对象，该对象是Spark程序的入口，负责创建RDD、启动任务等</p>
<p>在启动Spark Shell后，该对象会自动创建，可以通过变量sc进行访问</p>
<p><img src="images/image-20201210103829452.png" alt="image-20201210103829452"></p>
</blockquote>
<h4 id="action"><a href="#action" class="headerlink" title="action"></a>action</h4><img src="images/image-20201210055519214.png" alt="image-20201210055519214" style="zoom:67%;">

<h4 id="transformation"><a href="#transformation" class="headerlink" title="transformation"></a>transformation</h4><img src="images/image-20201210055541577.png" alt="image-20201210055541577" style="zoom:67%;">

<h3 id="退出"><a href="#退出" class="headerlink" title="退出"></a>退出</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">:quit</span><br></pre></td></tr></table></figure>


<h2 id="独立应用程序编程"><a href="#独立应用程序编程" class="headerlink" title="独立应用程序编程"></a>独立应用程序编程</h2><h3 id="使用sbt对Scala独立应用程序进行编译打包"><a href="#使用sbt对Scala独立应用程序进行编译打包" class="headerlink" title="使用sbt对Scala独立应用程序进行编译打包"></a>使用sbt对Scala独立应用程序进行编译打包</h3><ol>
<li><p>Scala应用程序</p>
<p> 应用程序根目录 </p>
<p><img src="images/image-20201210060328542.png" alt="image-20201210060328542"></p>
<blockquote>
<p>scala文件下建代码文件</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/* SimpleApp.scala */</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span>._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SimpleApp</span> </span>&#123;</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">            <span class="keyword">val</span> logFile = <span class="string">&quot;file:///usr/local/spark/README.md&quot;</span> <span class="comment">// Should be some file on your system</span></span><br><span class="line">            <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;Simple Application&quot;</span>)</span><br><span class="line">            <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)	<span class="comment">//指挥官</span></span><br><span class="line">            <span class="keyword">val</span> logData = sc.textFile(logFile, <span class="number">2</span>).cache()	<span class="comment">//底层文本文件加载进来后生成RDD 在内存当中进行缓存</span></span><br><span class="line">            <span class="keyword">val</span> numAs = logData.filter(line =&gt; line.contains(<span class="string">&quot;a&quot;</span>)).count()	<span class="comment">//logData已经是个RDD lambda表达式</span></span><br><span class="line">            <span class="keyword">val</span> numBs = logData.filter(line =&gt; line.contains(<span class="string">&quot;b&quot;</span>)).count()</span><br><span class="line">            println(<span class="string">&quot;Lines with a: %s, Lines with b: %s&quot;</span>.format(numAs, numBs))</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<p>不同于 Spark shell，独立应用程序需要通过 <code>val sc = new SparkContext(conf)</code> 初始化 SparkContext，SparkContext 的参数 SparkConf 包含了应用程序的信息。 </p>
</blockquote>
</li>
<li><p>该程序依赖 Spark API，因此需要通过 sbt 进行编译打包。 在~/sparkapp这个目录中新建文件simple.sbt（.sbt即可），命令如下： </p>
<p><img src="images/image-20201210060518843.png" alt="image-20201210060518843"></p>
<p>内容，给出相关依赖说明：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">name :&#x3D; &quot;Simple Project&quot;</span><br><span class="line">version :&#x3D; &quot;1.0&quot;</span><br><span class="line">scalaVersion :&#x3D; &quot;2.11.12&quot;</span><br><span class="line">libraryDependencies +&#x3D; &quot;org.apache.spark&quot; %% &quot;spark-core&quot; % &quot;2.4.0&quot;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>%%：不指定版本</p>
<p>%：需要指定版本</p>
</blockquote>
</li>
<li><p>sbt打包scala程序</p>
<p>查看当前目录结构</p>
<img src="images/image-20201210105652926.png" alt="image-20201210105652926" style="zoom:50%;">

<p>打包：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">/usr/local/sbt/sbt package</span><br></pre></td></tr></table></figure>

</li>
<li><p>生成jar包位置在target，通过 spark-submit 运行程序</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">/usr/local/spark/bin/spark-submit --class &quot;SimpleApp&quot; ~/sparkapp/target/scala-2.11/simple-project_2.11-1.0.jar</span><br><span class="line"><span class="meta">#</span><span class="bash"> 上面命令执行后会输出太多信息，可以不使用上面命令，而使用下面命令查看想要的结果</span></span><br><span class="line">/usr/local/spark/bin/spark-submit --class &quot;SimpleApp&quot; ~/sparkapp/target/scala-2.11/simple-project_2.11-1.0.jar 2&gt;&amp;1 | grep &quot;Lines with a:&quot;</span><br></pre></td></tr></table></figure>


</li>
</ol>
<h3 id="使用Maven对Java独立应用程序进行编译打包"><a href="#使用Maven对Java独立应用程序进行编译打包" class="headerlink" title="使用Maven对Java独立应用程序进行编译打包"></a>使用Maven对Java独立应用程序进行编译打包</h3><ol>
<li><p>java应用程序</p>
<p>程序根目录</p>
<p><img src="images/image-20201210064136385.png" alt="image-20201210064136385"></p>
<p>main/java 下建立一个名为 SimpleApp.java 的文件，内容：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/*** SimpleApp.java ***/</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SimpleApp</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        String logFile = <span class="string">&quot;file:///usr/local/spark/README.md&quot;</span>; <span class="comment">// Should be some file on your system</span></span><br><span class="line">        SparkConf conf=<span class="keyword">new</span> SparkConf().setMaster(<span class="string">&quot;local&quot;</span>).setAppName(<span class="string">&quot;SimpleApp&quot;</span>);</span><br><span class="line">        JavaSparkContext sc=<span class="keyword">new</span> JavaSparkContext(conf);</span><br><span class="line">        JavaRDD&lt;String&gt; logData = sc.textFile(logFile).cache(); </span><br><span class="line">        <span class="keyword">long</span> numAs = logData.filter(<span class="keyword">new</span> Function&lt;String, Boolean&gt;() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> Boolean <span class="title">call</span><span class="params">(String s)</span> </span>&#123; <span class="keyword">return</span> s.contains(<span class="string">&quot;a&quot;</span>); &#125;</span><br><span class="line">        &#125;).count(); </span><br><span class="line">        <span class="keyword">long</span> numBs = logData.filter(<span class="keyword">new</span> Function&lt;String, Boolean&gt;() &#123;</span><br><span class="line">            <span class="function"><span class="keyword">public</span> Boolean <span class="title">call</span><span class="params">(String s)</span> </span>&#123; <span class="keyword">return</span> s.contains(<span class="string">&quot;b&quot;</span>); &#125;</span><br><span class="line">        &#125;).count(); </span><br><span class="line">        System.out.println(<span class="string">&quot;Lines with a: &quot;</span> + numAs + <span class="string">&quot;, lines with b: &quot;</span> + numBs);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</li>
<li><p>该程序依赖Spark Java API,因此需要通过Maven进行编译打包。在./sparkapp2目录中新建文件pom.xml，命令如下： </p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">project</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>cn.edu.xmu<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>simple-project<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>Simple Project<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">packaging</span>&gt;</span>jar<span class="tag">&lt;/<span class="name">packaging</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">repositories</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">repository</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">id</span>&gt;</span>jboss<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">name</span>&gt;</span>JBoss Repository<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">url</span>&gt;</span>http://repository.jboss.com/maven2/<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">repository</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">repositories</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span> <span class="comment">&lt;!-- Spark dependency --&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-core_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">project</span>&gt;</span>  </span><br></pre></td></tr></table></figure>

</li>
<li><p>使用Maven打Java程序</p>
<p><img src="images/image-20201210110856328.png" alt="image-20201210110856328"></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd ~/sparkapp2</span><br><span class="line">/usr/local/maven/bin/mvn package</span><br></pre></td></tr></table></figure>

</li>
<li><p>通过spark-submit 运行程序</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">/usr/local/spark/bin/spark-submit --class &quot;SimpleApp&quot; ~/sparkapp2/target/simple-project-1.0.jar</span><br><span class="line"><span class="meta">#</span><span class="bash"> 上面命令执行后会输出太多信息，可以不使用上面命令，而使用下面命令查看想要的结果</span></span><br><span class="line">/usr/local/spark/bin/spark-submit --class &quot;SimpleApp&quot; ~/sparkapp2/target/simple-project-1.0.jar 2&gt;&amp;1 | grep &quot;Lines with a&quot;</span><br></pre></td></tr></table></figure>
<h3 id="使用Maven对Scala独立应用程序进行编译打包"><a href="#使用Maven对Scala独立应用程序进行编译打包" class="headerlink" title="使用Maven对Scala独立应用程序进行编译打包"></a>使用Maven对Scala独立应用程序进行编译打包</h3></li>
<li><p>Scala应用程序</p>
<p><img src="images/image-20201210064404674.png" alt="image-20201210064404674"></p>
<p>main/scala 下建立一个名为 SimpleApp.scala 的文件，内容：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/* SimpleApp.scala */</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span>._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SimpleApp</span> </span>&#123;</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">            <span class="keyword">val</span> logFile = <span class="string">&quot;file:///usr/local/spark/README.md&quot;</span> <span class="comment">// Should be some file on your system</span></span><br><span class="line">            <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;Simple Application&quot;</span>)</span><br><span class="line">            <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">            <span class="keyword">val</span> logData = sc.textFile(logFile, <span class="number">2</span>).cache()</span><br><span class="line">            <span class="keyword">val</span> numAs = logData.filter(line =&gt; line.contains(<span class="string">&quot;a&quot;</span>)).count()</span><br><span class="line">            <span class="keyword">val</span> numBs = logData.filter(line =&gt; line.contains(<span class="string">&quot;b&quot;</span>)).count()</span><br><span class="line">            println(<span class="string">&quot;Lines with a: %s, Lines with b: %s&quot;</span>.format(numAs, numBs))</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>不同于 Spark shell，独立应用程序需要通过 <code>val sc = new SparkContext(conf)</code> 初始化 SparkContext，SparkContext 的参数 SparkConf 包含了应用程序的信息。 </p>
</blockquote>
</li>
<li><p>使用Maven进行编译打包</p>
<p><img src="images/image-20201210133606699.png" alt="image-20201210133606699"></p>
<p>该程序依赖Spark Java API,因此需要通过Maven进行编译打包。在./sparkapp3目录中新建文件pom.xml，如下：</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">project</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>cn.edu.xmu<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>simple-project<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>Simple Project<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">packaging</span>&gt;</span>jar<span class="tag">&lt;/<span class="name">packaging</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">repositories</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">repository</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">id</span>&gt;</span>jboss<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">name</span>&gt;</span>JBoss Repository<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">url</span>&gt;</span>http://repository.jboss.com/maven2/<span class="tag">&lt;/<span class="name">url</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">repository</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">repositories</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span> <span class="comment">&lt;!-- Spark dependency --&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-core_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">sourceDirectory</span>&gt;</span>src/main/scala<span class="tag">&lt;/<span class="name">sourceDirectory</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.scala-tools<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-scala-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">              <span class="tag">&lt;<span class="name">goal</span>&gt;</span>compile<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">scalaVersion</span>&gt;</span>2.11.12<span class="tag">&lt;/<span class="name">scalaVersion</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">args</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">arg</span>&gt;</span>-target:jvm-1.8<span class="tag">&lt;/<span class="name">arg</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;/<span class="name">args</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">project</span>&gt;</span></span><br></pre></td></tr></table></figure>

</li>
<li><p>打包：</p>
<p><img src="images/image-20201210064613262.png" alt="image-20201210064613262"></p>
</li>
<li><p>通过spark-submit运行程序</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">/usr/local/spark/bin/spark-submit --class &quot;SimpleApp&quot; ~/sparkapp3/target/simple-project-1.0.jar</span><br><span class="line"><span class="meta">#</span><span class="bash">上面命令执行后会输出太多信息，可以不使用上面命令，而使用下面命令查看想要的结果</span></span><br><span class="line">/usr/local/spark/bin/spark-submit --class &quot;SimpleApp&quot; ~/sparkapp3/target/simple-project-1.0.jar 2&gt;&amp;1 | grep &quot;Lines with a:&quot;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>其他参数：</p>
<img src="images/image-20201214124640721.png" alt="image-20201214124640721" style="zoom:50%;">
</blockquote>
</li>
</ol>
<p>集群环境搭建：</p>
<p><a target="_blank" rel="noopener" href="https://www.icourse163.org/learn/XMU-1205811805?tid=1460184441#/learn/content?type=detail&amp;id=1236291022&amp;sm=1">https://www.icourse163.org/learn/XMU-1205811805?tid=1460184441#/learn/content?type=detail&amp;id=1236291022&amp;sm=1</a></p>
<h2 id="RDD编程"><a href="#RDD编程" class="headerlink" title="RDD编程"></a>RDD编程</h2><p>Spark的数据抽象：RDD</p>
<h3 id="创建："><a href="#创建：" class="headerlink" title="创建："></a>创建：</h3><ul>
<li><p>从文件系统中加载数据</p>
<p><strong>调用sparkcontext提供的.textFile()方法</strong>，读取数据生成内存中的RDD</p>
<blockquote>
<p>支持的数据类型有本地文件系统（file 三斜杠）、分布式文件系统HDFS（hdfs://）、Amazon S3等</p>
</blockquote>
<img src="images/image-20201214131115717.png" alt="image-20201214131115717" style="zoom:50%;">

<img src="images/image-20201214131151513.png" alt="image-20201214131151513" style="zoom:50%;">

<blockquote>
<p>文本文件中每一行为RDD中一个元素</p>
</blockquote>
<img src="images/image-20201214131300745.png" alt="image-20201214131300745" style="zoom:50%;">

<blockquote>
<p>下两行等价的条件是用户名为hadoop</p>
</blockquote>
</li>
<li><p>通过并行集合（数组）</p>
<p><strong>调用SparkContext对象提供的Parallelize方法</strong></p>
<img src="images/image-20201214131433233.png" alt="image-20201214131433233" style="zoom:50%;">

<img src="images/image-20201214131515827.png" alt="image-20201214131515827" style="zoom:50%;">

<img src="images/image-20201214131455601.png" alt="image-20201214131455601" style="zoom:50%;">

</li>
</ul>
<h3 id="操作："><a href="#操作：" class="headerlink" title="操作："></a>操作：</h3><h4 id="transformation-1"><a href="#transformation-1" class="headerlink" title="transformation"></a>transformation</h4><ol>
<li><p>filter</p>
<img src="images/image-20201214144412031.png" alt="image-20201214144412031" style="zoom: 67%;">


</li>
<li><p>map</p>
<img src="images/image-20201214144548579.png" alt="image-20201214144548579" style="zoom:67%;">
</li>
<li><p>flatMap</p>
<img src="images/image-20201214144907279.png" alt="image-20201214144907279" style="zoom:67%;">
</li>
<li><p>groupByKey()</p>
<img src="images/image-20201214145031235.png" alt="image-20201214145031235" style="zoom:67%;">
</li>
<li><p>reduceByKey(func)</p>
<img src="images/image-20201214145126930.png" alt="image-20201214145126930" style="zoom:67%;">



</li>
</ol>
<h4 id="action-1"><a href="#action-1" class="headerlink" title="action"></a>action</h4><ol>
<li>count</li>
<li>collect</li>
<li>first</li>
<li>take</li>
<li>reduce</li>
<li>foreach</li>
</ol>
<blockquote>
<p>转换操作具备延后性，直到遇到动作操作才从头到尾开始计算</p>
</blockquote>
<h3 id="持久化"><a href="#持久化" class="headerlink" title="持久化"></a>持久化</h3><p>允许用户把<strong>反复使用数据集</strong>持久化到内存，</p>
<img src="images/image-20201214150110609.png" alt="image-20201214150110609" style="zoom:67%;">

<blockquote>
<p>一个Spark应用程序提交上来，生成多个job，每个job包含多个stage，每个stage包含多个任务</p>
<p>每个action储发，产生一个job</p>
</blockquote>
<blockquote>
<p><strong>如何持久化</strong>？使用persist()对一个RDD标记为持久化，也就是说并不是马上计算生成RDD并把他持久化，而是到动作类型操作执行时才真正持久化</p>
</blockquote>
<ul>
<li><strong>.persist(MEMORY_ONLY)**：只在内存，不够替换旧（等价于</strong>.cache()**）</li>
<li>**.persist(MEMORY_AND_DISK)**：如果内存不足，存放底层磁盘</li>
<li>**.unpersist()**：手动把持久化的RDD从缓存中移除</li>
</ul>
<h3 id="分区-1"><a href="#分区-1" class="headerlink" title="分区"></a>分区</h3><h4 id="分区作用和原则"><a href="#分区作用和原则" class="headerlink" title="分区作用和原则"></a>分区作用和原则</h4><p><strong>作用</strong>：</p>
<ul>
<li>增加并行度，把庞大RDD分区，散布在不同机器节点，可以分布式并行计算</li>
<li>减小通信开销，</li>
</ul>
<p><strong>原则</strong>：</p>
<ul>
<li><p>分区个数尽量等于集群中CPU核心数目。这样才可进行真正并行</p>
<blockquote>
<p>对于spark多个模式，可以设置具体参数值设置</p>
<p>spark.default.parallelism</p>
<p>本地：local</p>
<p>mesos：默认分区数目为8</p>
<p>standalone、yarn：集群中所有CPU核心数目总和与默认值取最大</p>
</blockquote>
</li>
</ul>
<h4 id="分区设置方法"><a href="#分区设置方法" class="headerlink" title="分区设置方法"></a>分区设置方法</h4><p>语法格式</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">sc.textFile(path, partitionNum)</span><br></pre></td></tr></table></figure>
<ul>
<li><p>使用reparitition方法重新设置分区个数</p>
<img src="images/image-20201214151611237.png" alt="image-20201214151611237" style="zoom:67%;">

</li>
</ul>
<blockquote>
<p>分区有：</p>
<ul>
<li><p>自定义分区</p>
<ol>
<li>定义Partitioner类</li>
<li>继承org.apache.spark.Partitioner</li>
<li>覆盖方法：<ul>
<li>numPartitions：Int 返回创建出来的分区个数</li>
<li>getPatition(key:Any):Int 返回给定键的分区编号</li>
<li>equals()：java判断相等性的标准方法</li>
</ul>
</li>
</ol>
<img src="images/image-20201214152156542.png" alt="image-20201214152156542" style="zoom:67%;">

<p>使用<strong>单例对象</strong>，程序的入口函数，首先对data进行转换变为键值对(_,1)<strong>因为partitioner自定义分区类只针对键值对</strong></p>
<img src="images/image-20201214155146812.png" alt="image-20201214155146812" style="zoom:67%;">


</li>
<li><p>HashPartitioner</p>
</li>
<li><p>RangePartitioner</p>
</li>
</ul>
</blockquote>
<h3 id="键值对RDD"><a href="#键值对RDD" class="headerlink" title="键值对RDD"></a>键值对RDD</h3><h4 id="创建：-1"><a href="#创建：-1" class="headerlink" title="创建："></a>创建：</h4><ul>
<li>从文件加载、flatMap</li>
</ul>
<h4 id="常见转换操作"><a href="#常见转换操作" class="headerlink" title="常见转换操作"></a>常见转换操作</h4><ul>
<li><p>reduceByKey(func)</p>
<p>使用func函数合并具有相同键的值（进行汇总求和）</p>
</li>
<li><p>groupByKey()</p>
<p>把具有相同键的值分组</p>
</li>
<li><p>keys</p>
<p>把Pair RDD中的key返回形成一个新的RDD</p>
</li>
<li><p>values</p>
<p>Pair RDD中的value返回形成一个新的RDD</p>
</li>
<li><p>sortByKey()</p>
<p>返回一个根据键排序的RDD（默认升序）</p>
</li>
<li><p>sortBy([_ . _ 1,_ . _2],[true,false])</p>
</li>
<li><p>mapValues(func)</p>
<p>对键值对RDD中的每个value都应用一个函数，可以不变</p>
</li>
<li><p>join</p>
<p>把<strong>几个RDD</strong>当中元素key相同的连接</p>
</li>
<li><p>combineByKey</p>
</li>
</ul>
<h3 id="数据读写-1"><a href="#数据读写-1" class="headerlink" title="数据读写"></a>数据读写</h3><h4 id="文件系统数据读写"><a href="#文件系统数据读写" class="headerlink" title="文件系统数据读写"></a>文件系统数据读写</h4><h5 id="本地文件系统"><a href="#本地文件系统" class="headerlink" title="本地文件系统"></a>本地文件系统</h5><p><img src="images/image-20201215113048065.png" alt="image-20201215113048065"></p>
<p>惰性机制：即使输入错误的语句，也不会马上报错，除非到动作操作</p>
<blockquote>
<p>写入文件时只能输入指定目录，即时.txt也无法指定文件，读取时可以输入目录，系统读取所有文件生成RDD</p>
<p>结果会返回part-0000、0001，0000说明未分区</p>
</blockquote>
<h5 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h5><ul>
<li>读入</li>
</ul>
<p><img src="images/image-20201215113224989.png" alt="image-20201215113224989"></p>
<p>与下等价</p>
<p><img src="images/image-20201215113306333.png" alt="image-20201215113306333"></p>
<ul>
<li>写出</li>
</ul>
<img src="images/image-20201215113323379.png" alt="image-20201215113323379" style="zoom:67%;">

<blockquote>
<p>其为HDFS目录</p>
</blockquote>
<h4 id="json文件数据读写"><a href="#json文件数据读写" class="headerlink" title="json文件数据读写"></a>json文件数据读写</h4><ul>
<li>读入</li>
</ul>
<p><img src="images/image-20201215113448419.png" alt="image-20201215113448419"></p>
<ul>
<li>解析</li>
</ul>
<p>使用：scala.util.parsing.json.JSON自带库</p>
<blockquote>
<p><strong>JSON.parseFull(jsonString:String)函数解析</strong></p>
<ul>
<li>成功返回Some(map:Map[String,Any])</li>
<li>失败返回None</li>
</ul>
</blockquote>
<p><img src="images/image-20201215113731201.png" alt="image-20201215113731201"></p>
<blockquote>
<p>依次遍历进行解析</p>
</blockquote>
<h4 id="读写HBase数据"><a href="#读写HBase数据" class="headerlink" title="读写HBase数据"></a>读写HBase数据</h4><p>bug:</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_43473753/article/details/106263313">https://blog.csdn.net/qq_43473753/article/details/106263313</a></p>
<h2 id="Spark-SQL"><a href="#Spark-SQL" class="headerlink" title="Spark SQL"></a>Spark SQL</h2><p>只采用了Shark中的将SQL转换成抽象语法树，其他由Spark SQL自己来做</p>
<ul>
<li><p>Spark SQL的数据抽象：<strong>DataFrame</strong></p>
<blockquote>
<p>认为是带有Schema信息的RDD</p>
</blockquote>
</li>
<li><p>Spark SQL支持关系查询、能够处理结构化、半结构化和非结构化数据、支持复杂分析算法</p>
<p>DataFrame就是一些关系型表格</p>
<img src="images/image-20201222223548584.png" alt="image-20201222223548584" style="zoom:50%;">

</li>
</ul>
<h3 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h3><img src="images/image-20201222223732807.png" alt="image-20201222223732807" style="zoom: 80%;">

<p>Spark SQL是对DataFrame进行一次次转换</p>
<img src="images/image-20201222223837046.png" alt="image-20201222223837046" style="zoom:50%;">

<h4 id="创建DataFrame"><a href="#创建DataFrame" class="headerlink" title="创建DataFrame"></a>创建DataFrame</h4><ol>
<li><p>Spark 2.0后使用SparkSession生成，因此需要构建SparkSession对象</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().getOrCreate()</span><br></pre></td></tr></table></figure>
<blockquote>
<p>交互式环境已经默认建好该对象，即spark</p>
</blockquote>
</li>
<li><p>创建DataFrame</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._	<span class="comment">//导入隐式转换的包</span></span><br><span class="line"><span class="comment">//不同格式的文件均可读入，如json、csb、parquet，生成DataFrame</span></span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>json</li>
</ul>
<img src="images/image-20201222224627685.png" alt="image-20201222224627685" style="zoom:50%;">

<ul>
<li>parquet</li>
</ul>
<img src="images/image-20201222224645546.png" alt="image-20201222224645546" style="zoom:50%;">

<ul>
<li>csv</li>
</ul>
<img src="images/image-20201222224653564.png" alt="image-20201222224653564" style="zoom:50%;">
</blockquote>
</li>
</ol>
<h4 id="显示"><a href="#显示" class="headerlink" title="显示"></a>显示</h4><p>df.show</p>
<h4 id="保存DataFrame"><a href="#保存DataFrame" class="headerlink" title="保存DataFrame"></a>保存DataFrame</h4><p>使用spark.write保存DataFrame</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">df.write.json(people.json)</span><br><span class="line">df.write.parquet(...)</span><br><span class="line">df.write.csv(...)</span><br><span class="line"><span class="comment">//format(&quot;..&quot;)可以转换格式</span></span><br></pre></td></tr></table></figure>
<h4 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h4><ul>
<li><p>打印模式信息：df.printSchema()</p>
</li>
<li><p>选择多列：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">df.select(df(<span class="string">&quot;name&quot;</span>),df(<span class="string">&quot;age&quot;</span>)+<span class="number">1</span>).show()</span><br></pre></td></tr></table></figure>

</li>
<li><p>条件过滤</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">df.filter(df(<span class="string">&quot;age&quot;</span>) &gt; <span class="number">20</span>).show()</span><br></pre></td></tr></table></figure>

</li>
<li><p>分组聚合</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">df.groupBy(<span class="string">&quot;age&quot;</span>).count().show()</span><br></pre></td></tr></table></figure>

</li>
<li><p>排序</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">df.sort(df(<span class="string">&quot;age&quot;</span>).desc).show()</span><br></pre></td></tr></table></figure>

</li>
<li><p>多列排序</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">df.sort(df(<span class="string">&quot;age&quot;</span>).desc, df(<span class="string">&quot;name&quot;</span>).asc).show()</span><br></pre></td></tr></table></figure>
<p>先对age降序对name升序</p>
</li>
<li><p>队列进行重命名</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">df.select(df(<span class="string">&quot;name&quot;</span>).as(<span class="string">&quot;username&quot;</span>), df(<span class="string">&quot;age&quot;</span>)).show()</span><br></pre></td></tr></table></figure>


</li>
</ul>
<h3 id="从RDD转换得到DataFrame"><a href="#从RDD转换得到DataFrame" class="headerlink" title="从RDD转换得到DataFrame"></a>从RDD转换得到DataFrame</h3><h4 id="利用反射机制推断RDD模式"><a href="#利用反射机制推断RDD模式" class="headerlink" title="利用反射机制推断RDD模式"></a>利用反射机制推断RDD模式</h4><p>文本到DataFrame，文本文件先转化为RDD再转化为DataFrame</p>
<p>在利用反射机制推断RDD模式时，需要首先定义一个case class(样例类)，因为，只有case class才能被Spark隐式地转换为DataFrame</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">import</span> org.apache.spark.sql.catalyst.encoders.<span class="type">ExpressionEncoder</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.catalyst.encoders.<span class="type">ExpressionEncoder</span></span><br><span class="line"> </span><br><span class="line">scala&gt; <span class="keyword">import</span> org.apache.spark.sql.<span class="type">Encoder</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">Encoder</span></span><br><span class="line"> </span><br><span class="line">scala&gt; <span class="keyword">import</span> spark.implicits._  <span class="comment">//导入包，支持把一个RDD隐式转换为一个DataFrame</span></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"> </span><br><span class="line">scala&gt; <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Long</span></span>)  <span class="comment">//定义一个case class</span></span></span><br><span class="line"><span class="class"><span class="title">defined</span> <span class="title">class</span> <span class="title">Person</span></span></span><br><span class="line"><span class="class"> </span></span><br><span class="line"><span class="class"><span class="title">scala&gt;</span> <span class="title">val</span> <span class="title">peopleDF</span> </span>= spark.sparkContext.textFile(<span class="string">&quot;file:///usr/local/spark/examples/src/main/resources/people.txt&quot;</span>).map(_.split(<span class="string">&quot;,&quot;</span>)).map(attributes =&gt; <span class="type">Person</span>(attributes(<span class="number">0</span>), attributes(<span class="number">1</span>).trim.toInt)).toDF()	<span class="comment">//文本文件加载进来生成一个RDD，经过map操作用逗号拆分然后生成对象</span></span><br><span class="line"><span class="comment">//case class是非常特殊自动定义伴生对象，通过定义伴生对象定义一个方法不需要new</span></span><br><span class="line"><span class="comment">//.toDF转为DataFrame，已经在内存中，是一个关系表</span></span><br><span class="line">peopleDF: org.apache.spark.sql.<span class="type">DataFrame</span> = [name: string, age: bigint]</span><br><span class="line"></span><br><span class="line">scala&gt; peopleDF.createOrReplaceTempView(<span class="string">&quot;people&quot;</span>)  <span class="comment">//必须注册为临时表才能供下面的查询使用</span></span><br><span class="line"> </span><br><span class="line">scala&gt; <span class="keyword">val</span> personsRDD = spark.sql(<span class="string">&quot;select name,age from people where age &gt; 20&quot;</span>)</span><br><span class="line"><span class="comment">//最终生成一个DataFrame，下面是系统执行返回信息</span></span><br><span class="line">personsRDD: org.apache.spark.sql.<span class="type">DataFrame</span> = [name: string, age: bigint]</span><br><span class="line">scala&gt; personsRDD.map(t =&gt; <span class="string">&quot;Name:&quot;</span>+t(<span class="number">0</span>)+<span class="string">&quot;,&quot;</span>+<span class="string">&quot;Age:&quot;</span>+t(<span class="number">1</span>)).show()  <span class="comment">//DataFrame中的每个元素都是一行记录，包含name和age两个字段，分别用t(0)和t(1)来获取值</span></span><br><span class="line"> </span><br><span class="line">+------------------+</span><br><span class="line">|             value|</span><br><span class="line">+------------------+</span><br><span class="line">|<span class="type">Name</span>:<span class="type">Michael</span>,<span class="type">Age</span>:<span class="number">29</span>|</span><br><span class="line">|   <span class="type">Name</span>:<span class="type">Andy</span>,<span class="type">Age</span>:<span class="number">30</span>|</span><br><span class="line">+------------------+</span><br></pre></td></tr></table></figure>


<h4 id="编程方式定义RDD模式"><a href="#编程方式定义RDD模式" class="headerlink" title="编程方式定义RDD模式"></a>编程方式定义RDD模式</h4><p> 前面一直各个字段（name，age），所以当无法提前定义case class时，就需要采用编程方式定义RDD模式</p>
<ol>
<li>制作表头，即模式信息</li>
<li>制作表中记录</li>
<li>把表头和表中记录拼装在一起</li>
</ol>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">scala&gt; <span class="keyword">import</span> org.apache.spark.sql.types._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types._</span><br><span class="line"> </span><br><span class="line">scala&gt; <span class="keyword">import</span> org.apache.spark.sql.<span class="type">Row</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">Row</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">//生成 RDD</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> peopleRDD = spark.sparkContext.textFile(<span class="string">&quot;file:///usr/local/spark/examples/src/main/resources/people.txt&quot;</span>)</span><br><span class="line">peopleRDD: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = file:<span class="comment">///usr/local/spark/examples/src/main/resources/people.txt MapPartitionsRDD[1] at textFile at &lt;console&gt;:26</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">//定义一个模式字符串</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> schemaString = <span class="string">&quot;name age&quot;</span></span><br><span class="line">schemaString: <span class="type">String</span> = name age</span><br><span class="line"> </span><br><span class="line"><span class="comment">//根据模式字符串生成模式</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> fields = schemaString.split(<span class="string">&quot; &quot;</span>).map(fieldName =&gt; <span class="type">StructField</span>(fieldName, <span class="type">StringType</span>, nullable = <span class="literal">true</span>))</span><br><span class="line">fields: <span class="type">Array</span>[org.apache.spark.sql.types.<span class="type">StructField</span>] = <span class="type">Array</span>(<span class="type">StructField</span>(name,<span class="type">StringType</span>,<span class="literal">true</span>), <span class="type">StructField</span>(age,<span class="type">StringType</span>,<span class="literal">true</span>))</span><br><span class="line"> </span><br><span class="line">scala&gt; <span class="keyword">val</span> schema = <span class="type">StructType</span>(fields)</span><br><span class="line">schema: org.apache.spark.sql.types.<span class="type">StructType</span> = <span class="type">StructType</span>(<span class="type">StructField</span>(name,<span class="type">StringType</span>,<span class="literal">true</span>), <span class="type">StructField</span>(age,<span class="type">StringType</span>,<span class="literal">true</span>))</span><br><span class="line"><span class="comment">//从上面信息可以看出，schema描述了模式信息，模式中包含name和age两个字段</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">//对peopleRDD 这个RDD中的每一行元素都进行解析val peopleDF = spark.read.format(&quot;json&quot;).load(&quot;examples/src/main/resources/people.json&quot;)</span></span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">scala&gt; <span class="keyword">val</span> rowRDD = peopleRDD.map(_.split(<span class="string">&quot;,&quot;</span>)).map(attributes =&gt; <span class="type">Row</span>(attributes(<span class="number">0</span>), attributes(<span class="number">1</span>).trim))</span><br><span class="line">rowRDD: org.apache.spark.rdd.<span class="type">RDD</span>[org.apache.spark.sql.<span class="type">Row</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">3</span>] at map at &lt;console&gt;:<span class="number">29</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">//表头和表中记录的拼装</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> peopleDF = spark.createDataFrame(rowRDD, schema)</span><br><span class="line">peopleDF: org.apache.spark.sql.<span class="type">DataFrame</span> = [name: string, age: string]</span><br><span class="line"> </span><br><span class="line"><span class="comment">//必须注册为临时表才能供下面查询使用</span></span><br><span class="line">scala&gt; peopleDF.createOrReplaceTempView(<span class="string">&quot;people&quot;</span>)</span><br><span class="line"> </span><br><span class="line">scala&gt; <span class="keyword">val</span> results = spark.sql(<span class="string">&quot;SELECT name,age FROM people&quot;</span>)</span><br><span class="line">results: org.apache.spark.sql.<span class="type">DataFrame</span> = [name: string, age: string]</span><br><span class="line"> </span><br><span class="line">scala&gt; results.map(attributes =&gt; <span class="string">&quot;name: &quot;</span> + attributes(<span class="number">0</span>)+<span class="string">&quot;,&quot;</span>+<span class="string">&quot;age:&quot;</span>+attributes(<span class="number">1</span>)).show()</span><br><span class="line">+--------------------+</span><br><span class="line">|               value|</span><br><span class="line">+--------------------+</span><br><span class="line">|name: <span class="type">Michael</span>,age:<span class="number">29</span>|</span><br><span class="line">|   name: <span class="type">Andy</span>,age:<span class="number">30</span>|</span><br><span class="line">| name: <span class="type">Justin</span>,age:<span class="number">19</span>|</span><br><span class="line">+--------------------+</span><br></pre></td></tr></table></figure>
<p>法2：</p>
<img src="images/image-20210101212457730.png" alt="image-20210101212457730" style="zoom:67%;">



<h3 id="使用Spark-SQL-读写数据库"><a href="#使用Spark-SQL-读写数据库" class="headerlink" title="使用Spark SQL 读写数据库"></a>使用Spark SQL 读写数据库</h3><ul>
<li>启动mysql服务</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">service mysql start</span><br><span class="line">mysql -u -root -p</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">create database spark;</span><br><span class="line">use spark;</span><br><span class="line">crate table stu(id int(4), name char(20), gender char(4), age int(4));</span><br><span class="line">insert into stu values(1, &#39;Xueian&#39;, &#39;F&#39;, 23);</span><br><span class="line">select * from stu;</span><br></pre></td></tr></table></figure>
<ul>
<li>jdbc连接,驱动放于spark安装目录jars下</li>
<li>书写语句连接数据库</li>
</ul>
<blockquote>
<p>驱动跟着在命令行</p>
<img src="../../BigData/images/image-20210106184617038.png" alt="image-20210106184617038" style="zoom:67%;">
</blockquote>
<ul>
<li><p>通过jdbc连接mysql数据库</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> jdbcDF = spark.read.format(<span class="string">&quot;jdbc&quot;</span>).</span><br><span class="line">|option(<span class="string">&quot;url&quot;</span>,<span class="string">&quot;jdbc:mysql://localhost:3306/spark&quot;</span>).</span><br><span class="line">|option(<span class="string">&quot;driver&quot;</span>,<span class="string">&quot;com.mysql.jdbc.Driver&quot;</span>).</span><br><span class="line">|option(<span class="string">&quot;dbtable&quot;</span>,<span class="string">&quot;stu&quot;</span>).</span><br><span class="line">|option(<span class="string">&quot;user&quot;</span>,<span class="string">&quot;root&quot;</span>).</span><br><span class="line">|option(<span class="string">&quot;password&quot;</span>,<span class="string">&quot;hadoop&quot;</span>).</span><br><span class="line">|load()</span><br></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">jdbcDF.show() <span class="comment">// 显示</span></span><br></pre></td></tr></table></figure></li>
<li><p>新增两条记录</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.<span class="type">Properties</span>	</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types_	<span class="comment">// 表示模式</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">Row</span>	<span class="comment">//封装行对象</span></span><br><span class="line"><span class="comment">// 学生信息</span></span><br><span class="line"><span class="keyword">val</span> studentRDD = spark.sparkContext.parallelize(<span class="type">Array</span>(<span class="string">&quot;3 Rongcheng M 26&quot;</span>,<span class="string">&quot;4 Guanhua M 27&quot;</span>)).map(_.split(<span class="string">&quot; &quot;</span>))</span><br></pre></td></tr></table></figure>
<p>表头与表中记录拼装</p>
<ul>
<li><p>获取表头</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 设置模式信息</span></span><br><span class="line"><span class="keyword">val</span> schema = <span class="type">StructType</span>(<span class="type">List</span>(<span class="type">StructField</span>(<span class="string">&quot;id&quot;</span>,<span class="type">IntegerType</span>,<span class="literal">true</span>),<span class="type">StructField</span>(<span class="string">&quot;name&quot;</span>,<span class="type">StringType</span>,<span class="literal">true</span>),<span class="type">StructField</span>(<span class="string">&quot;gender&quot;</span>,<span class="type">StringType</span>,<span class="literal">true</span>),<span class="type">StructField</span>(<span class="string">&quot;age&quot;</span>,<span class="type">IntegerType</span>,<span class="literal">true</span>)))</span><br><span class="line"><span class="comment">// 封装至List列表对象中，构成集合，把四个对象传入StructType类，生成模式信息得到表头schema</span></span><br></pre></td></tr></table></figure>

</li>
<li><p>表中记录</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 生成RDD对其进行拆分</span></span><br><span class="line"><span class="keyword">val</span> studentRDD = spark.sparkContext.parallelize(<span class="type">Array</span>(<span class="string">&quot;3 Rongcheng M 26&quot;</span>,<span class="string">&quot;4 Guanhua M 27&quot;</span>)).map(_.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<img src="../../BigData/images/image-20210106190051816.png" alt="image-20210106190051816" style="zoom:67%;">

<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 创建Row对象，每个Row对象都是rowRDD中的一行</span></span><br><span class="line"><span class="comment">// 依次遍历转类型，作为Row构造器处理</span></span><br><span class="line"><span class="keyword">val</span> rowRDD = studentRDD.map(p=&gt;<span class="type">Row</span>(p(<span class="number">0</span>).toInt,p(<span class="number">1</span>).trim,p(<span class="number">2</span>).trim,p(<span class="number">3</span>).toInt))</span><br></pre></td></tr></table></figure></li>
<li><p>拼接</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 建立起Row对象和模式之间的对应关系，也就是把数据和模式对应起来</span></span><br><span class="line"><span class="keyword">val</span> studentDF = spark.createDataFrame(rowRDD,schema)</span><br></pre></td></tr></table></figure></li>
<li><p>追加至关系数据库</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 创建一个prop变量保存JDBC连接参数</span></span><br><span class="line"><span class="keyword">val</span> prop = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">prop.put(<span class="string">&quot;user&quot;</span>,<span class="string">&quot;root&quot;</span>) <span class="comment">// 用户名</span></span><br><span class="line">prop.put(<span class="string">&quot;password&quot;</span>,<span class="string">&quot;hadoop&quot;</span>) <span class="comment">//密码</span></span><br><span class="line">prop.put(<span class="string">&quot;driver&quot;</span>,<span class="string">&quot;com.mysql.jdba.Driver&quot;</span>) <span class="comment">//驱动程序</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//连接数据库 采用append模式，追加记录到数据库spark的stu表</span></span><br><span class="line"><span class="comment">// jdbc全称路径，表名以及封装的其他信息</span></span><br><span class="line">studentDF.write.mode(<span class="string">&quot;append&quot;</span>).jdbc(<span class="string">&quot;jdbc:mysql://localhost:3306/spark&quot;</span>,<span class="string">&quot;spark.student&quot;</span>,prop)</span><br></pre></td></tr></table></figure>


</li>
</ul>
</li>
</ul>
<h2 id="Spark-Streaming"><a href="#Spark-Streaming" class="headerlink" title="Spark Streaming"></a>Spark Streaming</h2><h3 id="概述-9"><a href="#概述-9" class="headerlink" title="概述"></a>概述</h3><p>分两种数据：静态数据和流数据</p>
<p>流数据是动态数据，比如每时每刻摄像头采集的数据，对数据进行实时分析</p>
<p>流数据：大量快速时变的到达</p>
<ul>
<li>数据快速持续到达，潜在大小也许无穷无尽</li>
<li>数据来源众多，格式复杂</li>
<li>数据量大</li>
<li>注重数据的整体价值</li>
<li>数据顺序颠倒，或不完整</li>
</ul>
<p>对两种典型数据有两种计算，分别是批量计算和实时计算</p>
<p>对于批量计算处理大规模静态数据，无法满足秒级响应，因此MapReduce不适合实时</p>
<p><strong>实时计算：</strong>实时获取不同数据源的海量数据、经过实时分析处理、获得有价值的信息</p>
<h4 id="流数据基本理念："><a href="#流数据基本理念：" class="headerlink" title="流数据基本理念："></a>流数据基本理念：</h4><p>数据的价值会随着时间流逝而降低</p>
<p>因此需要高性能（每秒几十万数据）、海量式（TB、PB级数据规模）、低延迟（实时性，达到秒级、毫秒级响应）、分布式（大数据基本架构，平滑扩展）可扩展、易用性（快速开发和部署）、高可靠的引擎</p>
<p>典型的三类流计算框架</p>
<ol>
<li>IBM…</li>
<li>Twitter Storm，Yahoo S4</li>
<li>百度DStream 银河流计算平台</li>
</ol>
<h4 id="传统的数据处理流程："><a href="#传统的数据处理流程：" class="headerlink" title="传统的数据处理流程："></a>传统的数据处理流程：</h4><ul>
<li>存储的数据是旧的，不具备时效性</li>
<li>需要用户主动发出查询来获取结果</li>
</ul>
<h4 id="流数据处理流程"><a href="#流数据处理流程" class="headerlink" title="流数据处理流程"></a>流数据处理流程</h4><img src="images/image-20210114155342020.png" alt="image-20210114155342020" style="zoom:67%;">

<h5 id="数据实时采集"><a href="#数据实时采集" class="headerlink" title="数据实时采集"></a>数据实时采集</h5><p>如日志数据</p>
<ul>
<li>facebook Scribe</li>
<li>LinkerIn Kafka</li>
<li>taobao Time Tunnel</li>
<li>Apache Flume</li>
</ul>
<h5 id="数据实时计算"><a href="#数据实时计算" class="headerlink" title="数据实时计算"></a>数据实时计算</h5><img src="images/image-20210114155545489.png" alt="image-20210114155545489" style="zoom:50%;">

<h5 id="实时查询服务"><a href="#实时查询服务" class="headerlink" title="实时查询服务"></a>实时查询服务</h5><p>用户实时查询栈是存储</p>
<p>一般采用<strong>订阅</strong>的方式，主动推送</p>
<h3 id="Spark-Streaming-1"><a href="#Spark-Streaming-1" class="headerlink" title="Spark Streaming"></a>Spark Streaming</h3><img src="images/image-20210114163331112.png" alt="image-20210114163331112" style="zoom:50%;">

<p>把实时的输入数据流切分，每段交给spark核心引擎处理，流处理转为批处理，由于线程级别并行，响应级别高，因此变相的流计算</p>
<img src="images/image-20210114163413372.png" alt="image-20210114163413372" style="zoom:67%;">

<h4 id="数据抽象：DStream"><a href="#数据抽象：DStream" class="headerlink" title="数据抽象：DStream"></a>数据抽象：DStream</h4><blockquote>
<p>DStream本质上是一堆RDD</p>
<img src="images/image-20210114163541042.png" alt="image-20210114163541042" style="zoom:50%;">
</blockquote>
<h4 id="与Storm区别"><a href="#与Storm区别" class="headerlink" title="与Storm区别"></a>与Storm区别</h4><ul>
<li><p>因此Spark Streaming本质上不是流计算，无法实现毫秒级响应</p>
</li>
<li><p>RDD有更高的容错性</p>
</li>
<li><p>既需要实时数据，有需要历史数据一起进行分析，可以用Spark Streaming</p>
</li>
</ul>
<img src="images/image-20210114163824232.png" alt="image-20210114163824232" style="zoom:67%;">

<h4 id="Spark架构"><a href="#Spark架构" class="headerlink" title="Spark架构"></a>Spark架构</h4><img src="images/image-20210114164007427.png" alt="image-20210114164007427" style="zoom:67%;">



<h3 id="DStream操作"><a href="#DStream操作" class="headerlink" title="DStream操作"></a>DStream操作</h3><img src="images/image-20210114164344142.png" alt="image-20210114164344142" style="zoom:67%;">

<blockquote>
<p>集群管理器：Cluster Managger</p>
<p>管家节点：Driver</p>
<p>Exceutor进程派生出多个Task线程（<strong>线程级并发</strong>）</p>
<p><strong>Receiver驻留Task，处理流数据，负责一个Input DStream</strong>，挂接Inpu DStream</p>
</blockquote>
<h4 id="Input-DStream类型："><a href="#Input-DStream类型：" class="headerlink" title="Input DStream类型："></a>Input DStream类型：</h4><ul>
<li>套接字流</li>
<li>文件流</li>
<li>从Kafka中读取的输入流</li>
</ul>
<h4 id="Spark-Streaming-程序"><a href="#Spark-Streaming-程序" class="headerlink" title="Spark Streaming 程序"></a>Spark Streaming 程序</h4><ol>
<li><p>通过创建输入DStream <strong>定义输入源</strong></p>
<blockquote>
<p>数据源头：</p>
<ul>
<li>对文件进行监控：文件流</li>
<li>通过Kafka抛数据：Kafka数据流</li>
<li>创建一个RDD队列：RDD队列流</li>
</ul>
</blockquote>
</li>
<li><p>对DStream应用<strong>转换操作和输出操作</strong>，来<strong>定义流计算</strong></p>
</li>
<li><p>streamingContext.start()开始程序，接收数据和处理流程</p>
<p>streamingContext.awaitTermination()，合适时结束</p>
<p>streamingContext.stop()，手动结束流计算进程</p>
</li>
</ol>
<h4 id="streaming-Context"><a href="#streaming-Context" class="headerlink" title="streaming Context"></a>streaming Context</h4><ul>
<li>创建对象：</li>
</ul>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.streaming._</span><br><span class="line"><span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sc.<span class="type">Seconds</span>(<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming._</span><br><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;TestDStream&quot;</span>).setMaster(<span class="string">&quot;local[2]&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf.seconds(<span class="number">1</span>))</span><br></pre></td></tr></table></figure>


<h3 id="基本输入源"><a href="#基本输入源" class="headerlink" title="基本输入源"></a>基本输入源</h3><h4 id="文件流"><a href="#文件流" class="headerlink" title="文件流"></a>文件流</h4><p>对某目录进行监控，发现文件变化，捕捉，如<strong>日志捕捉</strong></p>
<ul>
<li><p>shell创建文件流</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.streaming._</span><br><span class="line"><span class="comment">// 分段周期20秒</span></span><br><span class="line"><span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sc.<span class="type">Seconds</span>(<span class="number">20</span>))</span><br><span class="line"><span class="comment">// 文件流input stream</span></span><br><span class="line"><span class="keyword">val</span> lines = ssc.textFileStream(<span class="string">&quot;file:///usr/local/spark/mycode/streaming/logfile&quot;</span>)</span><br><span class="line"><span class="comment">// 流计算过程</span></span><br><span class="line"><span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line"><span class="keyword">val</span> wordCounts = words.map(x=&gt;(x,<span class="number">1</span>)).reduceByKey(_+_)</span><br><span class="line">wordCounts.print()</span><br><span class="line"><span class="comment">// 启动流计算</span></span><br><span class="line">ssc.start()</span><br><span class="line"><span class="comment">// 遇错停止，否则不断工作</span></span><br><span class="line">ssc.awaitTermination()</span><br></pre></td></tr></table></figure>

</li>
<li><p>独立应用程序编写创建文件流</p>
<img src="images/image-20210114171950800.png" alt="image-20210114171950800" style="zoom:67%;">

<img src="images/image-20210114172011516.png" alt="image-20210114172011516" style="zoom:50%;">

<img src="images/image-20210114172028144.png" alt="image-20210114172028144" style="zoom:50%;">

<p>注意：注意一些依赖库的变化</p>
</li>
</ul>
<h4 id="套接字流"><a href="#套接字流" class="headerlink" title="套接字流"></a>套接字流</h4><img src="images/image-20210114172324134.png" alt="image-20210114172324134" style="zoom: 80%;">



<h5 id="使用NC产生数据"><a href="#使用NC产生数据" class="headerlink" title="使用NC产生数据"></a><strong>使用NC产生数据</strong></h5><p>独立程序编写：</p>
<p><strong>构建TCP客户端</strong>向服务端发送请求，请求数据，服务端会发送数据到Spark Streaming组件，然后进行词频统计</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.storage.<span class="type">StorageLevel</span></span><br><span class="line"><span class="comment">// 单例对象</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">NetworkWordCount</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args:<span class="type">Array</span>[<span class="type">String</span>])&#123;</span><br><span class="line">        <span class="keyword">if</span>(args.length&lt;<span class="number">2</span>)&#123;</span><br><span class="line">            <span class="comment">// 服务器主机的地址和端口号</span></span><br><span class="line">            <span class="type">System</span>.err.println(<span class="string">&quot;Usage: NetworkWordCount&lt;hostname&gt;&lt;port&gt;&quot;</span>)</span><br><span class="line">            <span class="type">System</span>.exit(<span class="number">1</span>)</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 设置log4j日志级别显示</span></span><br><span class="line">        <span class="comment">// 定义在org.apache.spark.examples.streaming中的StreamingExample.scala代码文件，单例对象，不需要实例化，直接静态方法</span></span><br><span class="line">        <span class="type">StreamingExamples</span>.setStreamingLogLevels()</span><br><span class="line">        <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;NetworkWordCount&quot;</span>).setMaster(<span class="string">&quot;local[2]&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf.<span class="type">Seconds</span>(<span class="number">1</span>))</span><br><span class="line">        <span class="comment">// 定义输入数据流，表套接字数据流，args[0]为服务器主机名，args[1]为端口号，后为数据的保存方式，此处为内存+磁盘</span></span><br><span class="line">        <span class="keyword">val</span> lines = ssc.socketTextStream(args(<span class="number">0</span>),args(<span class="number">1</span>).toInt,<span class="type">StorageLevel</span>.<span class="type">MEMORY_AND_DISK_SER</span>)</span><br><span class="line">        <span class="comment">// 处理逻辑</span></span><br><span class="line">        <span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">        <span class="keyword">val</span> wordCounts = words.map(x=&gt;(x,<span class="number">1</span>)).reduceByKey(_+_)</span><br><span class="line">        wordCounts.print()</span><br><span class="line">        ssc.start()</span><br><span class="line">        ssc.awaitTermination()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>设置log级别的代码：</p>
<p><img src="images/image-20210114174143273.png" alt="image-20210114174143273"></p>
<p><strong>启动服务端</strong></p>
<p>linux本地可以运行netcat：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">nc -lk &lt;port&gt;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>l 启动监听 k 连续监听</p>
</blockquote>
<p>此时可以在服务端输入数据，由客户端处理</p>
<h5 id="使用Socket编程实现自定义数据源"><a href="#使用Socket编程实现自定义数据源" class="headerlink" title="使用Socket编程实现自定义数据源"></a><strong>使用Socket编程实现自定义数据源</strong></h5><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.&#123;<span class="type">PrintWriter</span>&#125;</span><br><span class="line"><span class="keyword">import</span> java.net.<span class="type">ServerSocket</span></span><br><span class="line"><span class="keyword">import</span> scala.io.<span class="type">Source</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">DataSourceSocket</span></span>&#123;</span><br><span class="line">    <span class="comment">// 返回0-length随机数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">index</span></span>(length: <span class="type">Int</span>)=&#123;</span><br><span class="line">        <span class="keyword">val</span> rdm = <span class="keyword">new</span> java.uril.<span class="type">Random</span></span><br><span class="line">        rdm.nextInt(length)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args:<span class="type">Array</span>[<span class="type">String</span>])&#123;</span><br><span class="line">        <span class="comment">// 需要文件名从文件中抓取数据传送给客户端、监听端口号、数据传输间隔时间</span></span><br><span class="line">        <span class="keyword">if</span>(args.length!=<span class="number">3</span>)&#123;</span><br><span class="line">            <span class="type">System</span>.err.println(<span class="string">&quot;Usage:&lt;filename&gt;&lt;port&gt;&lt;millisecond&gt;&quot;</span>)</span><br><span class="line">            <span class="type">System</span>.exit(<span class="number">1</span>)</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">val</span> fileName = args(<span class="number">0</span>)</span><br><span class="line">        <span class="comment">// 读取文件形成列表</span></span><br><span class="line">        <span class="keyword">val</span> lines = <span class="type">Source</span>.fromFile(fileName).getLines.toList</span><br><span class="line">        <span class="comment">// 取随机行就是随机数</span></span><br><span class="line">        <span class="keyword">val</span> rowCount = lines.length</span><br><span class="line">        <span class="comment">// TCP服务端构建ServerSocket监听端口号 : args(1).toInt绑定</span></span><br><span class="line">        <span class="keyword">val</span> listener = <span class="keyword">new</span> <span class="type">ServerSocket</span>(args(<span class="number">1</span>).toInt)</span><br><span class="line">        <span class="keyword">while</span>(<span class="literal">true</span>)&#123;</span><br><span class="line">            <span class="comment">// 进入阻塞状态，等待连接唤醒</span></span><br><span class="line">            <span class="keyword">val</span> socket = listener.accept()</span><br><span class="line">            <span class="comment">// 激活后进入新线程即一旦客户端发起连接，派生线程</span></span><br><span class="line">            <span class="keyword">new</span> <span class="type">Thread</span>() &#123;</span><br><span class="line">                <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span> </span>= &#123;</span><br><span class="line">                    <span class="comment">// socket.getInetAddress客户端地址</span></span><br><span class="line">                    println(<span class="string">&quot;Got client connected from:&quot;</span>+socket.getInetAddress)</span><br><span class="line">                    <span class="comment">// 生成输出流</span></span><br><span class="line">                    <span class="keyword">val</span> out = <span class="keyword">new</span> <span class="type">PrintWriter</span>(socket.getOutputStream(),<span class="literal">true</span>)</span><br><span class="line">                    <span class="comment">// 建立连接不断发送数据</span></span><br><span class="line">                    <span class="keyword">while</span>(<span class="literal">true</span>)&#123;</span><br><span class="line">                        <span class="comment">// 每睡几毫秒，就发一次</span></span><br><span class="line">                        <span class="type">Thread</span>.sleep(args(<span class="number">2</span>).toLong)</span><br><span class="line">                        <span class="comment">// 随即行</span></span><br><span class="line">                        <span class="keyword">val</span> content = lines(index(rowCount))</span><br><span class="line">                        println(content)</span><br><span class="line">                        out.write(content+&#x27;\n&#x27;)</span><br><span class="line">                        <span class="comment">// 发出</span></span><br><span class="line">                        out.flush()</span><br><span class="line">                    &#125;</span><br><span class="line">                    socket.close()</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;.start()</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>while(true)中内容：</p>
<img src="images/image-20210114180406318.png" alt="image-20210114180406318" style="zoom:67%;">



<h4 id="RDD队列流"><a href="#RDD队列流" class="headerlink" title="RDD队列流"></a>RDD队列流</h4><p>输入数据是一个RDD队列</p>
<blockquote>
<p>以下每隔一秒发送RDD队列到Spark Streaming</p>
<p>而Spark Streaming每隔1秒对数据进行处理</p>
</blockquote>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.<span class="type">StreamingContext</span>._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>,<span class="type">StreamingContext</span>&#125;</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">QueueStream</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>])&#123;</span><br><span class="line">        <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;TestRDDQueue&quot;</span>).setMaster(<span class="string">&quot;local[2]&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf.<span class="type">Seconds</span>(<span class="number">1</span>))</span><br><span class="line">        <span class="comment">// 生成RDD队列，该对象每个类型是RDD，RDD中是Int类</span></span><br><span class="line">        <span class="keyword">val</span> rddQueue = <span class="keyword">new</span> scala.collection.mutable.<span class="type">SynchronizedQueue</span>[<span class="type">RDD</span>[<span class="type">Int</span>]]()</span><br><span class="line">        <span class="comment">// 创建输入流（队列流类型）</span></span><br><span class="line">        <span class="keyword">val</span> queueStream = ssc.queueStream(rddQueue)</span><br><span class="line">        <span class="comment">// 处理部分</span></span><br><span class="line">        <span class="comment">// 对队列流数据进行map操作</span></span><br><span class="line">        <span class="keyword">val</span> mappedStream = queueStream.map(r=&gt;(r%<span class="number">10</span>,<span class="number">1</span>))</span><br><span class="line">        <span class="comment">// 词频统计</span></span><br><span class="line">        <span class="keyword">val</span> reducesStream = mappedStream.reduceByKey(_+_)</span><br><span class="line">        <span class="comment">// 统计结果打印</span></span><br><span class="line">        reduceStream.print()</span><br><span class="line">        ssc.start()</span><br><span class="line">        <span class="comment">// for循环向队列输送数据</span></span><br><span class="line">        <span class="keyword">for</span>(i&lt;- <span class="number">1</span> to <span class="number">10</span>)&#123;</span><br><span class="line">            <span class="comment">// 1到100 分两个区</span></span><br><span class="line">            rddQueue += ssc.sparkContext.makeRDD(<span class="number">1</span> to <span class="number">100</span>, <span class="number">2</span>)</span><br><span class="line">            <span class="type">Thread</span>.sleep(<span class="number">1000</span>)</span><br><span class="line">        &#125;</span><br><span class="line">        ssc.stop()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<h3 id="高级数据源"><a href="#高级数据源" class="headerlink" title="高级数据源"></a>高级数据源</h3><blockquote>
<p>使用Apache Kafka作为Spark Streaming数据源</p>
</blockquote>
<h4 id="kafka"><a href="#kafka" class="headerlink" title="kafka"></a>kafka</h4><h5 id="概述-10"><a href="#概述-10" class="headerlink" title="概述"></a>概述</h5><blockquote>
<p>高吞吐量的分布式发布订阅消息系统</p>
<p>订阅消息、分发消息</p>
<p><strong>作为消息分发系统，起到信息传递中枢作用</strong></p>
</blockquote>
<h6 id="Broker"><a href="#Broker" class="headerlink" title="Broker"></a>Broker</h6><p><strong>服务器</strong>，kafka集群有多个服务器即Borker</p>
<h6 id="Topic"><a href="#Topic" class="headerlink" title="Topic"></a>Topic</h6><p><strong>主题</strong>，消息扔给Topic，订阅也是Topic，因此有分区概念</p>
<h6 id="Partition"><a href="#Partition" class="headerlink" title="Partition"></a>Partition</h6><img src="images/image-20210114213524708.png" alt="image-20210114213524708" style="zoom:67%;">

<h6 id="Producer"><a href="#Producer" class="headerlink" title="Producer"></a>Producer</h6><p><strong>生产者</strong>，负责把生产出来的消息发送给kafka Broker</p>
<h6 id="consumer"><a href="#consumer" class="headerlink" title="consumer"></a>consumer</h6><p><strong>消费者</strong>，向Kafka broker读取消息的客户端</p>
<p>如Spark Streaming</p>
<h6 id="Consumer-Group"><a href="#Consumer-Group" class="headerlink" title="Consumer Group"></a>Consumer Group</h6><p>每个Consumer只属于某个Consumer Group，若不指定则属于默认的组</p>
<h6 id="架构图"><a href="#架构图" class="headerlink" title="架构图"></a>架构图</h6><img src="images/image-20210114213819810.png" alt="image-20210114213819810" style="zoom:80%;">

<h5 id="kafka启动"><a href="#kafka启动" class="headerlink" title="kafka启动"></a>kafka启动</h5><p>需要借助zookeeper服务，因此需要启动(需要跟配置文件参数)</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 进入kafka所在的目录</span></span><br><span class="line">cd /usr/local/kafka</span><br><span class="line">bin/zookeeper-server-start.sh config/zookeeper.properties</span><br></pre></td></tr></table></figure>
<blockquote>
<p>关闭窗口会终止服务</p>
</blockquote>
<p>启动Kafka服务，同上</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd /usr/local/kafka</span><br><span class="line">bin/kafka-server-start.sh config/server.properties</span><br></pre></td></tr></table></figure>
<p><a target="_blank" rel="noopener" href="http://dblab.xmu.edu.cn/blog/1096-2/">http://dblab.xmu.edu.cn/blog/1096-2/</a></p>
<p>导入jar包：</p>
<p><a target="_blank" rel="noopener" href="http://dblab.xmu.edu.cn/blog/1358-2/">http://dblab.xmu.edu.cn/blog/1358-2/</a></p>
<h4 id="生产者程序"><a href="#生产者程序" class="headerlink" title="生产者程序"></a>生产者程序</h4><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> org.apache.spark.examples.streaming</span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">HashMap</span></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.&#123;<span class="type">KafkaProducer</span>, <span class="type">ProducerConfig</span>, <span class="type">ProducerRecord</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka._</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">KafkaWordProducer</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">      <span class="comment">// 需要参数 Broker地址， topic主题，每秒几个消息，每条消息几个单词</span></span><br><span class="line">      <span class="keyword">if</span> (args.length &lt; <span class="number">4</span>) &#123;</span><br><span class="line">          <span class="type">System</span>.err.println(<span class="string">&quot;Usage: KafkaWordCountProducer &lt;metadataBrokerList&gt; &lt;topic&gt; &quot;</span> +</span><br><span class="line">            <span class="string">&quot;&lt;messagesPerSec&gt; &lt;wordsPerMessage&gt;&quot;</span>)</span><br><span class="line">          <span class="type">System</span>.exit(<span class="number">1</span>)</span><br><span class="line">        &#125;</span><br><span class="line">      <span class="keyword">val</span> <span class="type">Array</span>(brokers, topic, messagesPerSec, wordsPerMessage) = args</span><br><span class="line">      <span class="comment">// Zookeeper connection properties 哈希映射</span></span><br><span class="line">      <span class="keyword">val</span> props = <span class="keyword">new</span> <span class="type">HashMap</span>[<span class="type">String</span>, <span class="type">Object</span>]()</span><br><span class="line">      <span class="comment">// 需要提供Broker地址</span></span><br><span class="line">      props.put(<span class="type">ProducerConfig</span>.<span class="type">BOOTSTRAP_SERVERS_CONFIG</span>, brokers)</span><br><span class="line">      props.put(<span class="type">ProducerConfig</span>.<span class="type">VALUE_SERIALIZER_CLASS_CONFIG</span>,</span><br><span class="line">      <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>)</span><br><span class="line">      props.put(<span class="type">ProducerConfig</span>.<span class="type">KEY_SERIALIZER_CLASS_CONFIG</span>,</span><br><span class="line">      <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>)</span><br><span class="line">      <span class="keyword">val</span> producer = <span class="keyword">new</span> <span class="type">KafkaProducer</span>[<span class="type">String</span>, <span class="type">String</span>](props)</span><br><span class="line">      <span class="comment">// 不断发送消息</span></span><br><span class="line">      <span class="keyword">while</span>(<span class="literal">true</span>) &#123;</span><br><span class="line">          <span class="comment">// 每秒几个消息</span></span><br><span class="line">          (<span class="number">1</span> to messagesPerSec.toInt).foreach &#123; messageNum =&gt;</span><br><span class="line">              <span class="comment">// 每条消息包含wordsPerMessage个单词</span></span><br><span class="line">              <span class="keyword">val</span> str = (<span class="number">1</span> to wordsPerMessage.toInt).map(x =&gt; scala.util.<span class="type">Random</span>.nextInt(<span class="number">10</span>).toString).mkString(<span class="string">&quot; &quot;</span>)</span><br><span class="line">              print(str)</span><br><span class="line">              println()</span><br><span class="line">              <span class="comment">// Kafka只接受ProducerRecord对象，因此封装，往topic传，key，value</span></span><br><span class="line">              <span class="keyword">val</span> message = <span class="keyword">new</span> <span class="type">ProducerRecord</span>[<span class="type">String</span>, <span class="type">String</span>](topic, <span class="literal">null</span>, str)</span><br><span class="line">              producer.send(message)</span><br><span class="line">          &#125;</span><br><span class="line">          <span class="type">Thread</span>.sleep(<span class="number">1000</span>)</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<h4 id="消费者程序"><a href="#消费者程序" class="headerlink" title="消费者程序"></a>消费者程序</h4><p>相关数据扔到kafka后，编写spark streaming获取数据进行词频统计</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> org.apache.spark.examples.streaming</span><br><span class="line"><span class="keyword">import</span> org.apache.spark._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.<span class="type">StreamingContext</span>._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka.<span class="type">KafkaUtils</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">KafkaWordCount</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args:<span class="type">Array</span>[<span class="type">String</span>])&#123;</span><br><span class="line">        <span class="type">StreamingExamples</span>.setStreamingLogLevels()</span><br><span class="line">        <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;KafkaWordCount&quot;</span>).setMaster(<span class="string">&quot;local[2]&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sc,<span class="type">Seconds</span>(<span class="number">10</span>))</span><br><span class="line">        <span class="comment">// 检查点，防止数据丢失</span></span><br><span class="line">        ssc.checkpoint(<span class="string">&quot;file:///usr/local/spark/mycode/kafka/checkpoint&quot;</span>) <span class="comment">//设置检查点，如果存放在HDFS上面，则写成类似ssc.checkpoint(&quot;/user/hadoop/checkpoint&quot;)这种形式，但是，要启动hadoop</span></span><br><span class="line">        <span class="keyword">val</span> zkQuorum = <span class="string">&quot;localhost:2181&quot;</span> <span class="comment">//Zookeeper服务器地址</span></span><br><span class="line">        <span class="keyword">val</span> group = <span class="string">&quot;1&quot;</span>  <span class="comment">//topic所在的group，可以设置为自己想要的名称，比如不用1，而是val group = &quot;test-consumer-group&quot; </span></span><br><span class="line">        <span class="keyword">val</span> topics = <span class="string">&quot;wordsender&quot;</span>  <span class="comment">//topics的名称          </span></span><br><span class="line">        <span class="keyword">val</span> numThreads = <span class="number">1</span>  <span class="comment">//每个topic的分区数</span></span><br><span class="line">        <span class="keyword">val</span> topicMap =topics.split(<span class="string">&quot;,&quot;</span>).map((_,numThreads.toInt)).toMap <span class="comment">//需要map形式提供给createStream</span></span><br><span class="line">        <span class="keyword">val</span> lineMap = <span class="type">KafkaUtils</span>.createStream(ssc,zkQuorum,group,topicMap)	<span class="comment">//数据源创建完毕</span></span><br><span class="line">        <span class="comment">// 数据源创建后 接下来编写转换</span></span><br><span class="line">        <span class="keyword">val</span> lines = lineMap.map(_._2)</span><br><span class="line">        <span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">        <span class="keyword">val</span> pair = words.map(x =&gt; (x,<span class="number">1</span>))</span><br><span class="line">        <span class="comment">// 转换操作</span></span><br><span class="line">        <span class="keyword">val</span> wordCounts = pair.reduceByKeyAndWindow(_ + _,_ - _,<span class="type">Minutes</span>(<span class="number">2</span>),<span class="type">Seconds</span>(<span class="number">10</span>),<span class="number">2</span>) </span><br><span class="line">        wordCounts.print</span><br><span class="line">        ssc.start</span><br><span class="line">        ssc.awaitTermination</span><br><span class="line">    &#125;</span><br><span class="line">&#125;        </span><br></pre></td></tr></table></figure>


<h5 id="编写日志格式设置程序"><a href="#编写日志格式设置程序" class="headerlink" title="编写日志格式设置程序"></a>编写日志格式设置程序</h5><p>设置log4j</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> org.apache.spark.examples.streaming</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.internal.<span class="type">Logging</span></span><br><span class="line"><span class="keyword">import</span> org.apache.log4j.&#123;<span class="type">Level</span>, <span class="type">Logger</span>&#125;</span><br><span class="line"><span class="comment">/** Utility functions for Spark Streaming examples. */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamingExamples</span> <span class="keyword">extends</span> <span class="title">Logging</span> </span>&#123;</span><br><span class="line">  <span class="comment">/** Set reasonable logging levels for streaming if the user has not configured log4j. */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">setStreamingLogLevels</span></span>() &#123;</span><br><span class="line">    <span class="keyword">val</span> log4jInitialized = <span class="type">Logger</span>.getRootLogger.getAllAppenders.hasMoreElements</span><br><span class="line">    <span class="keyword">if</span> (!log4jInitialized) &#123;</span><br><span class="line">      <span class="comment">// We first log something to initialize Spark&#x27;s default logging, then we override the</span></span><br><span class="line">      <span class="comment">// logging level.</span></span><br><span class="line">      logInfo(<span class="string">&quot;Setting log level to [WARN] for streaming example.&quot;</span> +</span><br><span class="line">        <span class="string">&quot; To override add a custom log4j.properties to the classpath.&quot;</span>)</span><br><span class="line">      <span class="type">Logger</span>.getRootLogger.setLevel(<span class="type">Level</span>.<span class="type">WARN</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><a target="_blank" rel="noopener" href="http://dblab.xmu.edu.cn/blog/1358-2/">http://dblab.xmu.edu.cn/blog/1358-2/</a></p>
<h3 id="转换操作"><a href="#转换操作" class="headerlink" title="转换操作"></a>转换操作</h3><h4 id="DStream无状态转换"><a href="#DStream无状态转换" class="headerlink" title="DStream无状态转换"></a>DStream无状态转换</h4><ul>
<li><p>map(func):一对一转换</p>
</li>
<li><p>flapMap(func):1个映射多个</p>
</li>
<li><p>filter(func):过滤</p>
</li>
<li><p>repartition(numPartitions):改变DStream用的分区数目</p>
</li>
<li><p>reduce(func):聚合运算</p>
</li>
<li><p>count():统计</p>
</li>
<li><p>union(otherStream):合并两个DStream</p>
</li>
<li><p>countByValue():用在元素类型为K的DStream上统计</p>
</li>
<li><p>reduceByKey(func,[numTasks]):key相同进行聚合</p>
</li>
<li><p>join(otherStream,[numTasks])</p>
</li>
<li><p>cogroup(otherStream,[numTasks]):(K,V)、(K,W)-&gt;(K,SEQ(V),SEQ(W))</p>
</li>
<li><p>transform(func):对整个源DStream应用RDD-to-RDD函数转换得到新的DStream</p>
</li>
</ul>
<blockquote>
<p>无状态?</p>
<p>只针对当前批次，新的到达，前面的丢失</p>
</blockquote>
<h4 id="DStream有状态转换"><a href="#DStream有状态转换" class="headerlink" title="DStream有状态转换"></a>DStream有状态转换</h4><h5 id="滑动窗口转换操作"><a href="#滑动窗口转换操作" class="headerlink" title="滑动窗口转换操作"></a>滑动窗口转换操作</h5><img src="images/image-20210114222708185.png" alt="image-20210114222708185" style="zoom:67%;">

<blockquote>
<p><strong>参数</strong>：</p>
<p>滑动窗口大小、滑动窗口时间间隔大小</p>
</blockquote>
<p><strong>方法</strong></p>
<ul>
<li><p>winndow(windowLength,slideInterval)</p>
</li>
<li><p>countByWindow(windowLength,slideInterval):返回流中元素的一个滑动窗口数</p>
</li>
<li><p>reduceByWindow(func,windowLength,slideInterval):返回一个单元素流，即聚合得到的值对应的流</p>
</li>
<li><p>reduceByKeyAndWindow(func,windowLength,slideInterval,[numTasks]):对当前窗口进行reduceByKey计算（numTasks几个线程并行进行）</p>
<blockquote>
<p>额外的invfunc：逆函数，如下第二个参数，逆向操作的原因就是提高整个程序的运算效率，在滑动时部分淘汰部分还在，为了防止重新计算，可以用逆向操作</p>
<img src="images/image-20210114223258049.png" alt="image-20210114223258049" style="zoom:50%;">

<p><img src="images/image-20210114223139842.png" alt="image-20210114223139842"></p>
</blockquote>
</li>
</ul>
<h5 id="updateStateByKey操作"><a href="#updateStateByKey操作" class="headerlink" title="updateStateByKey操作"></a>updateStateByKey操作</h5><blockquote>
<p>在跨批次之间维护状态</p>
</blockquote>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> org.apache.spark.examples.streaming</span><br><span class="line"><span class="keyword">import</span> org.apache.spark._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.storage.<span class="type">StorageLevel</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">NetworkWordCountStateful</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    <span class="comment">//定义状态更新函数 values传入sequence序列 state维护当前历史状态的词频，若首次出现是None</span></span><br><span class="line">    <span class="keyword">val</span> updateFunc = (values: <span class="type">Seq</span>[<span class="type">Int</span>], state: <span class="type">Option</span>[<span class="type">Int</span>]) =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> currentCount = values.foldLeft(<span class="number">0</span>)(_ + _)	<span class="comment">//值的序列进行以0为初始总值相加</span></span><br><span class="line">      <span class="keyword">val</span> previousCount = state.getOrElse(<span class="number">0</span>)	<span class="comment">//获取历史结果</span></span><br><span class="line">      <span class="type">Some</span>(currentCount + previousCount)	<span class="comment">//累加返回给Some</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">StreamingExamples</span>.setStreamingLogLevels()  <span class="comment">//设置log4j日志级别</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[2]&quot;</span>).setAppName(<span class="string">&quot;NetworkWordCountStateful&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line">    sc.checkpoint(<span class="string">&quot;file:///usr/local/spark/mycode/streaming/stateful/&quot;</span>)    <span class="comment">//设置检查点，检查点具有容错机制</span></span><br><span class="line">    <span class="keyword">val</span> lines = sc.socketTextStream(<span class="string">&quot;localhost&quot;</span>, <span class="number">9999</span>)	<span class="comment">//套接字数据源</span></span><br><span class="line">    <span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">&quot; &quot;</span>))	<span class="comment">//转换语句</span></span><br><span class="line">    <span class="keyword">val</span> wordDstream = words.map(x =&gt; (x, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">val</span> stateDstream = wordDstream.updateStateByKey[<span class="type">Int</span>](updateFunc)	<span class="comment">//跨批次状态维护</span></span><br><span class="line">    stateDstream.print()</span><br><span class="line">    sc.start()</span><br><span class="line">    sc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>注：还要设置日志格式</p>
<h3 id="输出操作"><a href="#输出操作" class="headerlink" title="输出操作"></a>输出操作</h3><h4 id="输出到文本文件"><a href="#输出到文本文件" class="headerlink" title="输出到文本文件"></a>输出到文本文件</h4><p>新增下：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">stateDstream.savaAsTextFiles(<span class="string">&quot;file:///usr/local/spark/mycode/streaming/dstreamoutput/output.txt&quot;</span>)</span><br><span class="line"><span class="comment">//...</span></span><br><span class="line">sc.start()</span><br><span class="line">sc.awaitTermination()</span><br></pre></td></tr></table></figure>
<p>生成多个文件，每10秒一个</p>
<h4 id="输出到mysql"><a href="#输出到mysql" class="headerlink" title="输出到mysql"></a>输出到mysql</h4><p>新增下：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//下面是新增的语句，把DStream保存到MySQL数据库中  stateDstream本身是RDD集合</span></span><br><span class="line">stateDstream.foreachRDD(rdd =&gt; &#123;</span><br><span class="line">    <span class="comment">//内部函数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">func</span></span>(records: <span class="type">Iterator</span>[(<span class="type">String</span>,<span class="type">Int</span>)]) &#123;</span><br><span class="line">        <span class="keyword">var</span> conn: <span class="type">Connection</span> = <span class="literal">null</span></span><br><span class="line">        <span class="comment">// 动态sql</span></span><br><span class="line">        <span class="keyword">var</span> stmt: <span class="type">PreparedStatement</span> = <span class="literal">null</span></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">val</span> url = <span class="string">&quot;jdbc:mysql://localhost:3306/spark&quot;</span></span><br><span class="line">            <span class="keyword">val</span> user = <span class="string">&quot;root&quot;</span></span><br><span class="line">            <span class="keyword">val</span> password = <span class="string">&quot;hadoop&quot;</span></span><br><span class="line">            <span class="comment">// 驱动程序</span></span><br><span class="line">            conn = <span class="type">DriverManager</span>.getConnection(url, user, password)</span><br><span class="line">            records.foreach(p =&gt; &#123;</span><br><span class="line">                <span class="comment">// 动态sql语句</span></span><br><span class="line">                <span class="keyword">val</span> sql = <span class="string">&quot;insert into wordcount(word,count) values (?,?)&quot;</span></span><br><span class="line">                stmt = conn.prepareStatement(sql);</span><br><span class="line">                stmt.setString(<span class="number">1</span>, p._1.trim)</span><br><span class="line">                stmt.setInt(<span class="number">2</span>,p._2.toInt)</span><br><span class="line">                stmt.executeUpdate()</span><br><span class="line">            &#125;)</span><br><span class="line">        &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">            <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt; e.printStackTrace()</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">            <span class="keyword">if</span> (stmt != <span class="literal">null</span>) &#123;</span><br><span class="line">                stmt.close()</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (conn != <span class="literal">null</span>) &#123;</span><br><span class="line">                conn.close()</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 重分区以后的RDD遍历每一个分区</span></span><br><span class="line">    <span class="keyword">val</span> repartitionedRDD = rdd.repartition(<span class="number">3</span>)</span><br><span class="line">    <span class="comment">// 把partition里每条记录用func函数写到底层的mysql数据库中，会传递给func Iterator类型数据</span></span><br><span class="line">    repartitionedRDD.foreachPartition(func)</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<p>mysql中：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql&gt; use spark</span><br><span class="line">mysql&gt; create table wordcount (word char(20), count int(4));</span><br><span class="line">mysql&gt; select * from wordcount</span><br><span class="line">&#x2F;&#x2F;这个时候wordcount表是空的，没有任何记录</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><a target="_blank" rel="noopener" href="http://dblab.xmu.edu.cn/blog/1391-2/">http://dblab.xmu.edu.cn/blog/1391-2/</a></p>
<h3 id="Structured-Streaming"><a href="#Structured-Streaming" class="headerlink" title="Structured Streaming"></a>Structured Streaming</h3><blockquote>
<p>spark2.0之前,将RDD抽象</p>
<p>Structured Streaming用<strong>DataFrame抽象</strong></p>
<p>还引入<strong>持续流式处理模型,可以将流处理延迟降低至毫秒级别</strong></p>
</blockquote>
<ul>
<li><p>structured streaming就是将不断输入的流式数据,变为内存中一个没有边界的数据库表</p>
</li>
<li><p>DataFrame这种数据抽象是关系表,没到达一个数据在里面增加一条记录,每增加一个记录,更新查询结果</p>
</li>
</ul>
<p><img src="images/image-20210114232148964.png" alt="image-20210114232148964"></p>
<blockquote>
<p>建模为结构化数据表</p>
<p>重新抽象了流式计算,基于DataFrame的数据抽象</p>
<p>易于实现数据的exactly-once(at-least once是所有数据至少处理一次,那是之前,而exactly-once保证数据只被处理一次),保证数据至少处理一次,而且只处理一次</p>
</blockquote>
<h2 id="Spark-MLlib"><a href="#Spark-MLlib" class="headerlink" title="Spark MLlib"></a>Spark MLlib</h2><h3 id="概述-11"><a href="#概述-11" class="headerlink" title="概述"></a>概述</h3><p>mapreduce基于磁盘架构进行设计，多次反复迭代会带来磁盘IO开销</p>
<p>而spark是基于内存的计算框架，实现管道化的处理，在内存中完成数据交接，适合大量迭代计算，虽然shuffle要进磁盘但也大大减少磁盘IO开销</p>
<blockquote>
<p>MLlib就是基于海量数据的机器学习库，<strong>分布式实现</strong></p>
<p>提供流水线，调整机器学习工作流</p>
<p>提供持久性，保存算法、加载算法、模型、管道这些的保存</p>
<ul>
<li><p>spark.mllib是基于RDD的数据抽象</p>
</li>
<li><p>新版本spark.ml是基于DataFrame的数据抽象，可以使Spark SQL融合进来</p>
</li>
</ul>
</blockquote>
<img src="images/image-20210106224943530.png" alt="image-20210106224943530" style="zoom: 50%;">

<img src="images/image-20210106224952149.png" alt="image-20210106224952149" style="zoom:50%;">



<p><img src="images/image-20210106225226139.png" alt="image-20210106225226139"></p>
<h3 id="机器学习流水线"><a href="#机器学习流水线" class="headerlink" title="机器学习流水线"></a>机器学习流水线</h3><h4 id="pipeline概述"><a href="#pipeline概述" class="headerlink" title="pipeline概述"></a>pipeline概述</h4><h5 id="DataFrame-1"><a href="#DataFrame-1" class="headerlink" title="DataFrame"></a>DataFrame</h5><p>采用DataFrame结构化数据作为抽象</p>
<p>数据加载封装在DataFrame中，调用接口进行相关转换</p>
<h5 id="Transformer转换器"><a href="#Transformer转换器" class="headerlink" title="Transformer转换器"></a>Transformer转换器</h5><p>把一个DataFrame转换为另一个DataFrame</p>
<p>即通过算法对数据进行训练，得到<strong>模型（该模型即转换器）</strong>，可以对测试数据集（封装在DataFrame）进行打标签操作</p>
<h6 id="转换器实现的方法为Transform"><a href="#转换器实现的方法为Transform" class="headerlink" title="转换器实现的方法为Transform()"></a><strong>转换器实现的方法为Transform()</strong></h6><blockquote>
<p>提供DataFrame，接收后转为另一个DataFrame</p>
</blockquote>
<h5 id="Estimator评估器"><a href="#Estimator评估器" class="headerlink" title="Estimator评估器"></a>Estimator评估器</h5><p>学习算法或在训练数据上的训练方法的概念抽象</p>
<blockquote>
<p>用评估器对DataFrame数据进行操作，会<strong>转换得到一个转换器</strong>，实际上是一个算法，进行训练得到模型</p>
</blockquote>
<h6 id="评估器实现的方法为fit"><a href="#评估器实现的方法为fit" class="headerlink" title="评估器实现的方法为fit()"></a><strong>评估器实现的方法为fit()</strong></h6><blockquote>
<p>传递DataFrame，自动训练得到转换器</p>
</blockquote>
<h5 id="PipeLine"><a href="#PipeLine" class="headerlink" title="PipeLine"></a>PipeLine</h5><p>转换器-评估器反复组合，流水线是完成机器学习基本工作</p>
<h6 id="构建Pipeline流水线："><a href="#构建Pipeline流水线：" class="headerlink" title="构建Pipeline流水线："></a><strong>构建Pipeline流水线：</strong></h6><ol>
<li><p>定义Pipeline中各个流水线阶段PipelineStage（包括转换器、评估器）</p>
</li>
<li><p>按照处理逻辑转换器和评估器有序地组织起来构建成PipeLine</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 生成机器学习流水线，传递各个阶段名称</span></span><br><span class="line"><span class="keyword">val</span> pipeline = <span class="keyword">new</span> <span class="type">Pipeline</span>().setStages(<span class="type">Array</span>(stage1,stage2,stage3,...))</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意得到的流水线大多时候为评估器，因此需要fit</p>
</blockquote>
</li>
<li><p>训练流水线，得到PipeLine Model。用PipeLine Model预测数据打标签</p>
<img src="images/image-20210106230721415.png" alt="image-20210106230721415" style="zoom:67%;">

<blockquote>
<p>Tokenizer是一个转换器，进行分词处理，是模型了</p>
<p>HashingTF也是一个转换器，将单词转换为特征向量，模型</p>
<p>Logistic Regression是一个评估器，还是一个算法，训练的</p>
<p><strong>流水线本身是一个评估器</strong></p>
<img src="images/image-20210106231012167.png" alt="image-20210106231012167" style="zoom:67%;">
</blockquote>
</li>
</ol>
<h4 id="构建"><a href="#构建" class="headerlink" title="构建"></a>构建</h4><p>以逻辑斯蒂回归为例</p>
<h5 id="任务描述"><a href="#任务描述" class="headerlink" title="任务描述"></a>任务描述</h5><p>含Spark为有，否则无，对数据打标签</p>
<h5 id="过程"><a href="#过程" class="headerlink" title="过程"></a>过程</h5><ol>
<li><p>构建SparkSession对象（同Spark SQL）</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().</span><br><span class="line">master(<span class="string">&quot;local&quot;</span>).</span><br><span class="line">appName(<span class="string">&quot;my App Name&quot;</span>).</span><br><span class="line">gerOrCreate()</span><br></pre></td></tr></table></figure>

</li>
<li><p>引入要包含的包并构建训练数据集</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.ml.feature._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.ml.classification.<span class="type">LogisticRegression</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.ml.&#123;<span class="type">Pipeline</span>,<span class="type">PipelineModel</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.ml.linalg.<span class="type">Vector</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">Row</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 容器类型sequence toDF上字段名称</span></span><br><span class="line"><span class="keyword">val</span> training = spark.createDataFrame(<span class="type">Seq</span>(</span><br><span class="line">|(<span class="number">0</span>L,<span class="string">&quot;a b c d e spark&quot;</span>,<span class="number">1.0</span>),</span><br><span class="line">|(<span class="number">1</span>L,<span class="string">&quot;b d&quot;</span>,<span class="number">0.0</span>),</span><br><span class="line">|(<span class="number">2</span>L,<span class="string">&quot;spark f g h&quot;</span>,<span class="number">1.0</span>)</span><br><span class="line">|(<span class="number">3</span>L,<span class="string">&quot;hadoop mapreduce&quot;</span>,<span class="number">0.0</span>)</span><br><span class="line">|)).toDF(<span class="string">&quot;id&quot;</span>,<span class="string">&quot;text&quot;</span>,<span class="string">&quot;label&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

</li>
<li><p>定义Pipeline中各个流水线阶段PipelineStage</p>
<img src="images/image-20210106232104604.png" alt="image-20210106232104604" style="zoom:50%;">

<ol>
<li><p>分词</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 设定输入列text进行单词拆分，每个做分词</span></span><br><span class="line"><span class="comment">// 切分后输出到words列，以数组形式保存</span></span><br><span class="line"><span class="keyword">val</span> tokenizer = <span class="keyword">new</span> <span class="type">Tokenizer</span>().</span><br><span class="line">|setInputCol(<span class="string">&quot;text&quot;</span>).</span><br><span class="line">|setOutputCol(<span class="string">&quot;words&quot;</span>)</span><br></pre></td></tr></table></figure>

</li>
<li><p>单词转换为特征向量</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// setNumFeatures特征向量维度</span></span><br><span class="line"><span class="comment">// setInputCol输入列</span></span><br><span class="line"><span class="comment">// OutputCol输出的列</span></span><br><span class="line"><span class="keyword">val</span> hashingTF = <span class="keyword">new</span> <span class="type">HashingTF</span>().</span><br><span class="line">|setNumFeatures(<span class="number">1000</span>).</span><br><span class="line">|setInputCol(tokenizer.getOutputCol).</span><br><span class="line">|setOutputCol(<span class="string">&quot;features&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

</li>
<li><p>逻辑斯蒂回归算法</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 评估器对象</span></span><br><span class="line"><span class="comment">// 不断迭代最大10</span></span><br><span class="line"><span class="keyword">val</span> lr = <span class="keyword">new</span> <span class="type">LogisticRegression</span>().</span><br><span class="line">|setMaxIter(<span class="number">10</span>).</span><br><span class="line">|serRegParam(<span class="number">0.01</span>)</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li><p>按照处理逻辑有序地组织PipelineStages，创建Pipeline</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 数组封装</span></span><br><span class="line"><span class="keyword">val</span> pipeline = <span class="keyword">new</span> <span class="type">Pipeline</span>().</span><br><span class="line">|setStages(<span class="type">Array</span>(tokenizer, hashingTF, lr))</span><br></pre></td></tr></table></figure>

</li>
<li><p>还包含一个评估器，评估器还没训练，需要fit</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// training是构建的训练数据DataFrame</span></span><br><span class="line"><span class="keyword">val</span> model = pipeline.fit(training)</span><br></pre></td></tr></table></figure>
<p>训练结束得到PipelineModel</p>
</li>
</ol>
<h5 id="测试数据"><a href="#测试数据" class="headerlink" title="测试数据"></a>测试数据</h5><ol>
<li><p>测试数据封装(无标签)</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> test = spark.createDataFrame(<span class="type">Seq</span>(</span><br><span class="line">|(<span class="number">4</span>L,<span class="string">&quot;spark i j k&quot;</span>),</span><br><span class="line">|(<span class="number">5</span>L,<span class="string">&quot;l m n&quot;</span>),</span><br><span class="line">|(<span class="number">6</span>L,<span class="string">&quot;spark a&quot;</span>)</span><br><span class="line">|(<span class="number">7</span>L,<span class="string">&quot;apache hadoop&quot;</span>)</span><br><span class="line">|)).toDF(<span class="string">&quot;id&quot;</span>,<span class="string">&quot;text&quot;</span>)</span><br></pre></td></tr></table></figure>

</li>
<li><p>测试数据按顺序通过拟合的流水线，生成预测结果</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// foreach按指定格式打印</span></span><br><span class="line"><span class="comment">// 遍历到的DataFrame为当前Row对象，case会把id提取到ID字段里面，text提取赋值给text变量，prob概率向量提取...</span></span><br><span class="line"><span class="comment">// println中为s插值，凡是变量则加$，打印时把变量值替换掉把值打印</span></span><br><span class="line">model.transform(test).</span><br><span class="line">|select(<span class="string">&quot;id&quot;</span>,<span class="string">&quot;text&quot;</span>,<span class="string">&quot;probability&quot;</span>,<span class="string">&quot;prediction&quot;</span>).</span><br><span class="line">|collect().</span><br><span class="line">|foreach &#123; <span class="keyword">case</span> <span class="type">Row</span>(id:<span class="type">Long</span>,text:<span class="type">String</span>,prob:<span class="type">Vector</span>,prediction:<span class="type">Double</span>)=&gt;</span><br><span class="line">|println(<span class="string">s&quot;(<span class="subst">$id</span>,<span class="subst">$text</span>)--&gt;prob=<span class="subst">$prob</span>,prediction=&amp;prediction&quot;</span>)</span><br><span class="line">|&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>如：</p>
<img src="images/image-20210106233918354.png" alt="image-20210106233918354" style="zoom:67%;">



</li>
</ol>
<h3 id="特征抽取、转化和选择"><a href="#特征抽取、转化和选择" class="headerlink" title="特征抽取、转化和选择"></a>特征抽取、转化和选择</h3><h4 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h4><p>词频-逆向文件频率</p>
<ul>
<li>IDF</li>
</ul>
<p>$$<br>IDF(t,D) = log\frac{|D|+1}{DF(t,D)+1}<br>$$</p>
<ul>
<li>度量值表示：</li>
</ul>
<p>$$<br>TFIDF(t,d,D)=TF(t,d)\cdot IDF(t,D)<br>$$</p>
<h5 id="TF"><a href="#TF" class="headerlink" title="TF"></a>TF</h5><blockquote>
<p>使用HashingTF转换器，可以让词条集合转化为固定长度特征向量</p>
</blockquote>
<h5 id="IDF"><a href="#IDF" class="headerlink" title="IDF"></a>IDF</h5><blockquote>
<p>使用评估器，给予数据集训练得到IDFModel</p>
</blockquote>
<p>用TF的特征向量作为输入传到IDFModel</p>
<h5 id="过程-1"><a href="#过程-1" class="headerlink" title="过程"></a>过程</h5><blockquote>
<p>输入数据-&gt;分解器Tokenizer-&gt;得到词袋-&gt;HashingTF得到特征向量-&gt;IDF调整得到的特征向量</p>
</blockquote>
<ol>
<li><p>导入TF-IDF所需要的包</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.ml.feature.&#123;<span class="type">HashingTF</span>,<span class="type">IDF</span>,<span class="type">Tokenizer</span>&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>开启RDD隐式转换</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._</span><br></pre></td></tr></table></figure></li>
<li><p>创建一个简单DataFrame，每个句子代表一个文档</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> sentenceData = spark.createDataFrame(<span class="type">Seq</span>(</span><br><span class="line">|(<span class="number">0</span>,<span class="string">&quot;I heard about Spark and I love Spark&quot;</span>),</span><br><span class="line">|(<span class="number">0</span>,<span class="string">&quot;I wish Java could use case classes&quot;</span>),</span><br><span class="line">|(<span class="number">1</span>,<span class="string">&quot;Logistic regression models are neat&quot;</span>)</span><br><span class="line">|)).toDF(<span class="string">&quot;label&quot;</span>,<span class="string">&quot;sentence&quot;</span>)</span><br></pre></td></tr></table></figure>

</li>
<li><p>得到文档集合后，即可用tokenizer对句子进行分词</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> tokenizer = <span class="keyword">new</span> <span class="type">Tokenizer</span>().setInputCol(<span class="string">&quot;sentence&quot;</span>).setOutputCol(<span class="string">&quot;words&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> wordsData = tokenizer.transform(sentenceData)</span><br><span class="line">wordsData.show(<span class="literal">false</span>)</span><br></pre></td></tr></table></figure>

</li>
<li><p>使用HashingTF的transform()方法把句子哈希成特征向量</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> hashingTF = <span class="keyword">new</span> <span class="type">HashingTF</span>().</span><br><span class="line">|setInputCol(<span class="string">&quot;words&quot;</span>).setOutputCol(<span class="string">&quot;rawFeatures&quot;</span>).setNumFeatures(<span class="number">2000</span>)</span><br><span class="line"><span class="keyword">val</span> featurizedData = hashingTF.transform(wordsData)</span><br><span class="line">featurizedData.select(<span class="string">&quot;rawFeatures&quot;</span>).show(<span class="literal">false</span>)</span><br></pre></td></tr></table></figure>
<p><img src="images/image-20210107101021538.png" alt="image-20210107101021538"></p>
<blockquote>
<p>2000为哈希桶数目，后数组表示分词后每个词在哈希桶映射到的编号，后为词频</p>
</blockquote>
</li>
<li><p>调权重：使用IDF来对单纯的词频特征向量进行修正(评估器)</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> idf = <span class="keyword">new</span> <span class="type">IDF</span>().setInputCol(<span class="string">&quot;rawFeatures&quot;</span>).setOutputCol(<span class="string">&quot;features&quot;</span>)</span><br><span class="line"><span class="comment">// 训练</span></span><br><span class="line"><span class="keyword">val</span> idfModel = idf.fit(featurizedData)</span><br></pre></td></tr></table></figure>

</li>
<li><p>用数据进行预测，调用IDFModel的transform()方法</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> rescaledData = idfModel.transform(featurizedData)</span><br><span class="line"><span class="comment">// 取前三个</span></span><br><span class="line">rescaledData.select(<span class="string">&quot;features&quot;</span>,<span class="string">&quot;label&quot;</span>).take(<span class="number">3</span>).foreach(println)</span><br></pre></td></tr></table></figure>
<p>词频变了，单词重要性变了，即得到每一个单词对应的TF-IDF度量值</p>
<img src="images/image-20210107101555327.png" alt="image-20210107101555327" style="zoom:50%;">



</li>
</ol>
<h4 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h4><p>词嵌入方法，计算每个单词在给定语料库环境下的分布式词向量，一定程度上刻画语义</p>
<p>本质上是评估器，需要训练得到word2vecmodel，从而进行映射转成固定大小向量</p>
<h5 id="过程-2"><a href="#过程-2" class="headerlink" title="过程"></a>过程</h5><ol>
<li><p>首先导入Word2Vec所需要的包，并创建三个词语序列，每个代表一个文档</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.ml.feature.<span class="type">Word2Vec</span></span><br><span class="line"><span class="keyword">val</span> documentDF = spark.createDataFrame(<span class="type">Seq</span>(</span><br><span class="line">|<span class="string">&quot;Hi I heard about Spark&quot;</span>.split(<span class="string">&quot; &quot;</span>),</span><br><span class="line">|<span class="string">&quot;I wish Java could use case classes&quot;</span>.split(<span class="string">&quot; &quot;</span>),</span><br><span class="line">|<span class="string">&quot;Logistic regression models are neat&quot;</span>.split(<span class="string">&quot; &quot;</span>)</span><br><span class="line">|).map(<span class="type">Tuple1</span>.apply)).toDF(<span class="string">&quot;text&quot;</span>)</span><br></pre></td></tr></table></figure>

</li>
<li><p>新建Word2Vec</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 特征向量维度为3，单词出现0次以上列入统计</span></span><br><span class="line"><span class="keyword">val</span> word2Vec = <span class="keyword">new</span> <span class="type">Word2Vec</span>().</span><br><span class="line">|setInputCol(<span class="string">&quot;text&quot;</span>).</span><br><span class="line">|setOutputCol(<span class="string">&quot;result&quot;</span>).</span><br><span class="line">|setVectorSize(<span class="number">3</span>).</span><br><span class="line">|setMinCount(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

</li>
<li><p>读入训练数据，用fit()方法生成一个Word2VecModel</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> model = word2Vec.fit(documentDF)</span><br></pre></td></tr></table></figure>

</li>
<li><p>利用Word2VecModel把文档转变成特征向量</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> result = model.transform(documentDF)</span><br><span class="line">result.select(<span class="string">&quot;result&quot;</span>).take(<span class="number">3</span>).foreach(println)</span><br></pre></td></tr></table></figure>
<img src="images/image-20210107102904893.png" alt="image-20210107102904893" style="zoom:67%;">



</li>
</ol>
<h4 id="CountVectorizer"><a href="#CountVectorizer" class="headerlink" title="CountVectorizer"></a>CountVectorizer</h4><p>通过技术把文档转向量，如果不存在先验字典会通过评估器进行训练，生成CountVectorizerModel</p>
<blockquote>
<p>产生文档关于词语的稀疏表示，传递给其他算法进一步使用</p>
</blockquote>
<h5 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h5><p>CountVectorizer训练中会根据语料库中的词频排序从高到低进行选择</p>
<ul>
<li><p>vocabsize</p>
<p>指定词汇表最大含量（前n个词频最大的）</p>
</li>
<li><p>minDF</p>
<p>指定词汇表中的词语至少在多少个不同文档中出现</p>
</li>
</ul>
<h5 id="过程-3"><a href="#过程-3" class="headerlink" title="过程"></a>过程</h5><h6 id="无先验"><a href="#无先验" class="headerlink" title="无先验"></a>无先验</h6><ol>
<li><p>首先导入CountVectorizer所需要的包</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.ml.feature.&#123;<span class="type">CountVectorizer</span>,<span class="type">CountVectorizerModel</span>&#125;</span><br></pre></td></tr></table></figure>

</li>
<li><p>构建相关数据</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> df = spark.createDataFrame(<span class="type">Seq</span>(</span><br><span class="line">|(<span class="number">0</span>,<span class="type">Array</span>(<span class="string">&quot;a&quot;</span>,<span class="string">&quot;b&quot;</span>,<span class="string">&quot;c&quot;</span>)),</span><br><span class="line">|(<span class="number">1</span>,<span class="type">Array</span>(<span class="string">&quot;a&quot;</span>,<span class="string">&quot;b&quot;</span>,<span class="string">&quot;b&quot;</span>,<span class="string">&quot;c&quot;</span>,<span class="string">&quot;a&quot;</span>))</span><br><span class="line">|)).toDF(<span class="string">&quot;id&quot;</span>,<span class="string">&quot;words&quot;</span>)</span><br></pre></td></tr></table></figure>

</li>
<li><p>通过CountVectorizer设定超参数，训练一个CountVectorizerModel</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> cvModel: <span class="type">CountVectorizerModel</span> = <span class="keyword">new</span> <span class="type">CountVectorizer</span>().</span><br><span class="line">|setInputCol(<span class="string">&quot;words&quot;</span>).</span><br><span class="line">|setOutputCol(<span class="string">&quot;features&quot;</span>).</span><br><span class="line">|setVocabSize(<span class="number">3</span>).</span><br><span class="line">|setMinDF(<span class="number">2</span>).</span><br><span class="line">|fit(df)</span><br></pre></td></tr></table></figure>

</li>
<li><p>通过CountVectorizerModel的vocabulary成员获得到模型的词汇表</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">cvModel.vocabulary</span><br></pre></td></tr></table></figure>

</li>
<li><p>使用这一模型对DataFrame进行变换，得到文档的向量化表示</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">cvModel.transform(df).show(<span class="literal">false</span>)</span><br></pre></td></tr></table></figure>


</li>
</ol>
<h6 id="含先验"><a href="#含先验" class="headerlink" title="含先验"></a>含先验</h6><p>可以指定先验的词汇表，不需要训练</p>
<ol>
<li><p>用先验的词汇表直接生成得到一个模型</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">cal cvm = <span class="keyword">new</span> <span class="type">CountVectorizerModel</span>(<span class="type">Array</span>(<span class="string">&quot;a&quot;</span>,<span class="string">&quot;b&quot;</span>,<span class="string">&quot;c&quot;</span>)).</span><br><span class="line">|setInoutCol(<span class="string">&quot;words&quot;</span>).</span><br><span class="line">|setOutputCol(<span class="string">&quot;features&quot;</span>)</span><br><span class="line">cvm.transform(df).select(<span class="string">&quot;features&quot;</span>).foreach&#123;println&#125;</span><br></pre></td></tr></table></figure>


</li>
</ol>
<h3 id="分类和回归"><a href="#分类和回归" class="headerlink" title="分类和回归"></a>分类和回归</h3><h4 id="逻辑斯蒂回归分类器"><a href="#逻辑斯蒂回归分类器" class="headerlink" title="逻辑斯蒂回归分类器"></a>逻辑斯蒂回归分类器</h4><ol>
<li><p>导入需要的包</p>
<img src="images/image-20210107112820800.png" alt="image-20210107112820800" style="zoom:50%;">
</li>
<li><p>隐式转换问题</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="comment">// 样例类定义</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Iris</span>(<span class="params">features:org.apache.spark.ml.linalg.<span class="type">Vector</span>,label:<span class="type">String</span></span>)</span></span><br></pre></td></tr></table></figure>

</li>
<li><p>读取数据</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> data = spark.spark.<span class="type">Context</span>.textFile(<span class="string">&quot;file:///usr/local/spark/iris.txt&quot;</span>).map(_.split(<span class="string">&quot;,&quot;</span>)).map(p=&gt;<span class="type">Iris</span>(<span class="type">Vectors</span>.dense(p(<span class="number">0</span>).toDouble,p(<span class="number">1</span>).toDouble,p(<span class="number">2</span>).toDouble,p(<span class="number">3</span>).toDouble),p(<span class="number">4</span>).toString())).toDF()</span><br><span class="line">data.show()</span><br></pre></td></tr></table></figure>

</li>
<li><p>得到数据注册成一个表Iris，通过SQL语句进行数据查询</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 注册临时表</span></span><br><span class="line">data.createOrReplaceTempView(<span class="string">&quot;iris&quot;</span>)</span><br><span class="line"><span class="comment">// 查询sql</span></span><br><span class="line"><span class="keyword">val</span> df = spark.sql(<span class="string">&quot;select * from iris where label != &#x27;Iris-setosa&#x27;&quot;</span>)</span><br><span class="line"><span class="comment">// 标签放前</span></span><br><span class="line">df.map(t=&gt;t(<span class="number">1</span>)+<span class="string">&quot;:&quot;</span>+t(<span class="number">0</span>)).collect().foreach(println)</span><br></pre></td></tr></table></figure>

</li>
<li><p>分别获取标签列和特征列，进行索引，并进行重命名</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 字符串转为数值形式</span></span><br><span class="line"><span class="keyword">val</span> labelIndexer = <span class="keyword">new</span> <span class="type">StringIndexer</span>().setInputCol(<span class="string">&quot;label&quot;</span>).setOutputCol(<span class="string">&quot;indexedLabel&quot;</span>).fit(df)</span><br><span class="line"><span class="comment">// 特征向量也进行转换为数值型</span></span><br><span class="line"><span class="keyword">val</span> featureIndexer = <span class="keyword">new</span> <span class="type">VectorIndexer</span>().setInputCol(<span class="string">&quot;features&quot;</span>).setOutputCol(<span class="string">&quot;indexedFeatures&quot;</span>).fit(df)</span><br></pre></td></tr></table></figure>

</li>
<li><p>数据集划分</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> <span class="type">Array</span>(trainingData,testData) = df.randomSplit(<span class="type">Array</span>(<span class="number">0.7</span>,<span class="number">0.3</span>))</span><br></pre></td></tr></table></figure>

</li>
<li><p><strong>设置logistic的参数</strong></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 索引后的标签indexedLabel 特征列indexedFeatures，最大迭代10</span></span><br><span class="line"><span class="keyword">val</span> lr = <span class="keyword">new</span> <span class="type">LogisticRegression</span>().setLabelCol(<span class="string">&quot;indexedLabel&quot;</span>).setFeaturesCol(<span class="string">&quot;indexedFeatures&quot;</span>).setMaxIter(<span class="number">10</span>).setRegParam(<span class="number">0.3</span>).setElasticNetParam(<span class="number">0.8</span>)</span><br></pre></td></tr></table></figure>

</li>
<li><p>设置一个labelConverter，把预测的类别重新转化成字符型</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> labelConverter = <span class="keyword">new</span> <span class="type">IndexToString</span>().setInputCol(<span class="string">&quot;prediction&quot;</span>).setOutputCol(<span class="string">&quot;predictedLabel&quot;</span>).setLabels(labelIndexer.labels)</span><br></pre></td></tr></table></figure>
<p><strong>labelIndexer.labels</strong>：包含的数据型</p>
</li>
<li><p>构建pipeline，设置stage，调用fit()来训练模型</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> <span class="type">IrPipeline</span> = <span class="keyword">new</span> <span class="type">Pipeline</span>().setStages(<span class="type">Array</span>(labelIndexer,featureIndexer,lr,labelConverter))</span><br><span class="line"><span class="keyword">val</span> <span class="type">IrPiprlineModel</span> = <span class="type">IrPipeline</span>.fit(trainingData)</span><br></pre></td></tr></table></figure>

</li>
<li><p>PipelineModel调用transform()进行预测</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> <span class="type">IrPredictions</span> = <span class="type">IrPipelineModel</span>.transform(testData)</span><br><span class="line"><span class="type">IrPredictions</span>.select(<span class="string">&quot;predictedLabel&quot;</span>,<span class="string">&quot;label&quot;</span>,<span class="string">&quot;features&quot;</span>,<span class="string">&quot;probability&quot;</span>).collect().foreach &#123;<span class="keyword">case</span> <span class="type">Row</span>(predictedLabel:<span class="type">String</span>,label:<span class="type">String</span>,features:<span class="type">Vector</span>,prob:<span class="type">Vector</span>)=&gt;println(<span class="string">s&quot;(<span class="subst">$label</span>,<span class="subst">$features</span>)--&gt;prob=<span class="subst">$prob</span>,predictedLabel=<span class="subst">$predictedLabel</span>&quot;</span>)&#125;</span><br></pre></td></tr></table></figure>
<img src="images/image-20210107115337108.png" alt="image-20210107115337108" style="zoom:50%;">
</li>
<li><p>模型评估，计算预测准确率和错误率</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> evaluator = <span class="keyword">new</span> <span class="type">MulticlassClassificationEvaluator</span>().setLabelCol(<span class="string">&quot;indexedLabel&quot;</span>).setPredictionCol(<span class="string">&quot;prediction&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> <span class="type">IrAccuarcy</span> = evaluator.evaluate(<span class="type">IrPredictions</span>)</span><br><span class="line">println(<span class="string">&quot;Test Error = &quot;</span>+(<span class="number">1.0</span> - <span class="type">IrAccuarcy</span>))</span><br></pre></td></tr></table></figure>

</li>
<li><p>通过model来获取训练得到的逻辑斯蒂模型</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> <span class="type">IrModel</span> = <span class="type">IrPipelineModel</span>.stages(<span class="number">2</span>).asInstanceOf[<span class="type">LogisticRegressionModel</span>]</span><br></pre></td></tr></table></figure>

</li>
<li><p>打印参数</p>
<p><img src="images/image-20210107115822876.png" alt="image-20210107115822876"></p>
</li>
</ol>
<h4 id="决策树分类器"><a href="#决策树分类器" class="headerlink" title="决策树分类器"></a>决策树分类器</h4><p>树形结构，训练数据训练</p>
<h5 id="概述-12"><a href="#概述-12" class="headerlink" title="概述"></a>概述</h5><p>特征选择、决策树生成、决策树剪枝</p>
<ul>
<li><p>通常特征选择的准则是信心增益，选择信息增益最大的特征</p>
<img src="images/image-20210107120050779.png" alt="image-20210107120050779" style="zoom:50%;">
</li>
<li><p>决策树生成</p>
</li>
<li><p>决策树剪枝</p>
<p>对未知测试数据的分类没有那么准确：过拟合现象</p>
</li>
</ul>
<h5 id="过程-4"><a href="#过程-4" class="headerlink" title="过程"></a>过程</h5><ol>
<li><p>导入需要的包</p>
<img src="images/image-20210107120155383.png" alt="image-20210107120155383" style="zoom: 67%;">
</li>
<li><p>隐式、读取</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="comment">// 样例类定义</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Iris</span>(<span class="params">features:org.apache.spark.ml.linalg.<span class="type">Vector</span>,label:<span class="type">String</span></span>)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> data = spark.spark.<span class="type">Context</span>.textFile(<span class="string">&quot;file:///usr/local/spark/iris.txt&quot;</span>).map(_.split(<span class="string">&quot;,&quot;</span>)).map(p=&gt;<span class="type">Iris</span>(<span class="type">Vectors</span>.dense(p(<span class="number">0</span>).toDouble,p(<span class="number">1</span>).toDouble,p(<span class="number">2</span>).toDouble,p(<span class="number">3</span>).toDouble),p(<span class="number">4</span>).toString())).toDF()</span><br><span class="line">data.show()</span><br></pre></td></tr></table></figure>

</li>
<li><p>得到数据注册成一个表Iris，通过SQL语句进行数据查询</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 注册临时表</span></span><br><span class="line">data.createOrReplaceTempView(<span class="string">&quot;iris&quot;</span>)</span><br><span class="line"><span class="comment">// 查询sql</span></span><br><span class="line"><span class="keyword">val</span> df = spark.sql(<span class="string">&quot;select * from iris&quot;</span>)</span><br><span class="line"><span class="comment">// 标签放前</span></span><br><span class="line">df.map(t=&gt;t(<span class="number">1</span>)+<span class="string">&quot;:&quot;</span>+t(<span class="number">0</span>)).collect().foreach(println)</span><br></pre></td></tr></table></figure></li>
<li><p>分别获取标签列和特征列，进行索引，并进行重命名</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 字符串转为数值形式</span></span><br><span class="line"><span class="keyword">val</span> labelIndexer = <span class="keyword">new</span> <span class="type">StringIndexer</span>().setInputCol(<span class="string">&quot;label&quot;</span>).setOutputCol(<span class="string">&quot;indexedLabel&quot;</span>).fit(df)</span><br><span class="line"><span class="comment">// 特征向量也进行转换为数值型 设最大分类</span></span><br><span class="line"><span class="keyword">val</span> featureIndexer = <span class="keyword">new</span> <span class="type">VectorIndexer</span>().setInputCol(<span class="string">&quot;features&quot;</span>).setOutputCol(<span class="string">&quot;indexedFeatures&quot;</span>).setMaxCategories(<span class="number">4</span>).fit(df)</span><br></pre></td></tr></table></figure>

</li>
<li><p>数据集划分</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> <span class="type">Array</span>(trainingData,testData) = df.randomSplit(<span class="type">Array</span>(<span class="number">0.7</span>,<span class="number">0.3</span>))</span><br></pre></td></tr></table></figure></li>
<li><p>进一步处理特征和标签，以及数据分组</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> labelConverter = <span class="keyword">new</span> <span class="type">IndexToString</span>().setInputCol(<span class="string">&quot;prediction&quot;</span>).setOutputCol(<span class="string">&quot;predictedLabel&quot;</span>).setLabels(labelIndexer.labels)</span><br></pre></td></tr></table></figure>

</li>
<li><p>构建决策树分类模型</p>
<ol>
<li><p>导入所需要的包</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.ml.classification.<span class="type">DecisionTreeClassificationModel</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.ml.classification.<span class="type">DecisionTreeClassifier</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.ml.evaluation.<span class="type">MulticlassClassificationEvaluator</span></span><br></pre></td></tr></table></figure>

</li>
<li><p>基本配置</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 训练决策树模型，通过setter方法来设置决策树参数，也可以用ParamMap来设置，可以设置的参数可以通过explainParams()获取</span></span><br><span class="line"><span class="keyword">val</span> dtClassifier = <span class="keyword">new</span> <span class="type">DecisionTreeClassifier</span>().setLabelCol(<span class="string">&quot;indexedLabel&quot;</span>).setFeaturesCol(<span class="string">&quot;indexedFeatures&quot;</span>)</span><br></pre></td></tr></table></figure>

</li>
</ol>
</li>
<li><p>定义评估器</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> pipelinedClassifier = <span class="keyword">new</span> <span class="type">Pipeline</span>().setStages(<span class="type">Array</span>(labelIndexer,featureIndexer,dtClassifier,labelConverter))</span><br></pre></td></tr></table></figure>

</li>
<li><p>训练模型，并进行预测</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> modelClassifier = pipelinedClassifier.fit(trainingData)</span><br><span class="line"><span class="keyword">val</span> predictionsClassifier = modelClassifier.transform(testData)</span><br><span class="line"><span class="comment">// 预测结果</span></span><br><span class="line">predictionsClassifier.select(<span class="string">&quot;predictedLabel&quot;</span>,<span class="string">&quot;label&quot;</span>,<span class="string">&quot;features&quot;</span>.show(<span class="number">20</span>))</span><br></pre></td></tr></table></figure>

</li>
<li><p>评估决策树分类器模型</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> evaluatorClassifier = <span class="keyword">new</span> <span class="type">MulticlassClassificationEvaluator</span>().setLabelCol(<span class="string">&quot;indexedLabel&quot;</span>),setPredictionCol(<span class="string">&quot;prediction&quot;</span>).setMetricName(<span class="string">&quot;accuracy&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> accuracy = evaluatorClassifier.evaluate(predictionsClassifier)</span><br></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">println(<span class="string">&quot;Test Error=&quot;</span>+(<span class="number">1.0</span> - arruracy))</span><br></pre></td></tr></table></figure>

</li>
<li><p>获取整个模型并打印</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 获取整个模型</span></span><br><span class="line"><span class="keyword">val</span> treeModelClassifier = modelClassifier.stages(<span class="number">2</span>).asInstanceOf[<span class="type">DecisionTreeClassificationModel</span>]</span><br></pre></td></tr></table></figure>
<p>打印：</p>
<img src="images/image-20210107121901044.png" alt="image-20210107121901044" style="zoom: 67%;">





</li>
</ol>
<h2 id="图计算"><a href="#图计算" class="headerlink" title="图计算"></a>图计算</h2><blockquote>
<p>针对图结构数据的处理</p>
<p>图结构数据可以很好的表达数据之间的关联性，关联性计算是大数据计算的核心—-通过获得数据的关联性，可以从噪音很多的海量数据中抽取有用信息。如通过关联性得到相似用户推荐、通过图结构计算找到意见领袖</p>
</blockquote>
<h3 id="概述-13"><a href="#概述-13" class="headerlink" title="概述"></a>概述</h3><blockquote>
<p><strong>BSP模型</strong>叫整体同步并行计算模型或者简称为大同步模型</p>
<p>包括通过网络连接起来的处理器，并行分布式处理；计算是一系列的全局超步（迭代）；</p>
<p>一个超步的垂直结构如下</p>
<img src="images/image-20210116230136553.png" alt="image-20210116230136553" style="zoom: 67%;">

<ul>
<li><strong>局部计算</strong>：自身的计算任务只读取本地值（各个处理器异步独立执行互不干涉）</li>
<li><strong>通讯</strong>：不同处理器交换数据，使下次更好的迭代计算</li>
<li><strong>栅栏同步</strong>：不同的处理器各自允许，速度不同，速度快的会等待，被栅栏拦住</li>
</ul>
</blockquote>
<h3 id="Pregel"><a href="#Pregel" class="headerlink" title="Pregel"></a>Pregel</h3><p>基于BSP模型实现的并行图处理系统</p>
<h4 id="图计算模型"><a href="#图计算模型" class="headerlink" title="图计算模型"></a>图计算模型</h4><h5 id="有向图和顶点"><a href="#有向图和顶点" class="headerlink" title="有向图和顶点"></a>有向图和顶点</h5><img src="images/image-20210116230559940.png" alt="image-20210116230559940" style="zoom:67%;">

<ol>
<li>Pregel计算模型以有向图作为输入</li>
<li>有向图的每个顶点都有一个String类型的顶点ID</li>
<li>每个顶点都有一个可修改的用户自定义值与之关联</li>
<li>每条有向边都和其源顶点关联，并记录了其目标顶点ID</li>
<li>边上有一个可修改的用户自定义值与之关联</li>
</ol>
<blockquote>
<img src="images/image-20210116230924348.png" alt="image-20210116230924348" style="zoom:80%;">

<ul>
<li><p>在每个超步S中，图中所有顶点都会并行执行相同的用户自定义函数</p>
</li>
<li><p>每个顶点可以接收前一个超步(S-1)中发送给它的消息，修改其自身及其出射边的状态，并发送消息给其他顶点，甚至是修改整个图的拓扑结构</p>
</li>
<li><p>边并不是核心对象，在边上面不会有相应计算，只有顶点才会执行用户自定义函数进行相应计算</p>
</li>
</ul>
</blockquote>
<p>顶点之间的消息传递方法：</p>
<ul>
<li><p>远程读取</p>
</li>
<li><p>基于共享内存</p>
</li>
<li><p>而Pregel采用消息传递模型</p>
<blockquote>
<p>因为消息传递具有足够的表达能力</p>
<p>有助于提升系统的整体性能（远程读取有较高延迟，图结构不需要远程读取；而共享内存扩展性不好）</p>
</blockquote>
</li>
</ul>
<h5 id="计算过程"><a href="#计算过程" class="headerlink" title="计算过程"></a>计算过程</h5><ul>
<li><p>Pregel计算过程是由被称为”超步”的迭代组成的</p>
</li>
<li><p>在<strong>每个超步</strong>中，<strong>每个顶点</strong>上面都会并行执行用户自定义的函数</p>
</li>
</ul>
<p>读取前一个超步顶点发送的消息，然后处理消息后修改出射边状态，再把相关消息发送给下个超步的顶点（下个超步处理）</p>
<ul>
<li><p>在Pregel计算过程中，一个算法什么时候结束由所有顶点状态决定</p>
<img src="images/image-20210116231751907.png" alt="image-20210116231751907" style="zoom:50%;">

<blockquote>
<p>所有顶点非活跃状态，并且顶点之间无消息传递发生，则结束</p>
</blockquote>
<ol>
<li>在第0个超步，所有顶点处于活跃状态</li>
<li>一个顶点不需要继续执行进一步计算时就会把自己状态设置为停机，除非给其发送了消息被激活</li>
<li>Pregel计算框架必须根据条件判断来决定是否将非活跃状态点显式唤醒进入活跃状态</li>
</ol>
</li>
</ul>
<h4 id="C"><a href="#C" class="headerlink" title="C++###########"></a>C++###########</h4><h4 id="Pregel体系结构"><a href="#Pregel体系结构" class="headerlink" title="Pregel体系结构"></a>Pregel体系结构</h4><h5 id="Pregel执行过程和容错性"><a href="#Pregel执行过程和容错性" class="headerlink" title="Pregel执行过程和容错性"></a>Pregel执行过程和容错性</h5><h6 id="执行过程"><a href="#执行过程" class="headerlink" title="执行过程"></a>执行过程</h6><p>在Pregel计算框架中，大型图会被划分成许多个分区，每个分区包含了一部分顶点以及以其为起点的边。而顶点分配到哪个分区是由一个函数决定的，系统默认函数为hash(ID) mod N，其中N为所有分区总数，ID是这个顶点的标识符</p>
<blockquote>
<p><img src="images/image-20210117171718862.png" alt="image-20210117171718862"></p>
<ol>
<li>选择集群中的多台机器执行图计算任务，有一台机器被选为Master其他机器作为Worker</li>
<li>Master把一个图分成多个分区，并把分区分配到多个Worker，一个Worker会领到一个或多个分区，每个Worker知道所有其他Worker所分配到的分区情况</li>
<li>Master会把用户输入划分成多个部分，然后Master会为每个Worker分配用户输入的一部分。如果一个Worker从输入内容中加载到的顶点刚好是自己所分配到的分区中的顶点，就会立即更新相应的数据结构；否则该Worker会根据加载到的顶点的ID把它发送到其所属的分区所在Worker上。当所有输入都被加载后，图中所有顶点都会被标记为”活跃”状态</li>
<li>Master向每个Worker发送指令，Worker收到指令后，开始运行一个超步。当一个超步中所有工作都完成后，Worker通知Master，并把自己下一个超步还处于活跃状态的顶点数量报告给Master</li>
<li>计算过程结束后，Master给所有Worker发送指令，通知每个Worker对自己的计算结果进行持久化存储</li>
</ol>
</blockquote>
<h6 id="容错性"><a href="#容错性" class="headerlink" title="容错性"></a>容错性</h6><p><img src="images/image-20210117172720743.png" alt="image-20210117172720743"></p>
<h5 id="Worker、Master和Aggregator"><a href="#Worker、Master和Aggregator" class="headerlink" title="Worker、Master和Aggregator"></a>Worker、Master和Aggregator</h5><h6 id="Worker"><a href="#Worker" class="headerlink" title="Worker"></a>Worker</h6><blockquote>
<p>一般在执行过程中，信息存在内存中</p>
<p>信息有：顶点当前值，出射边列表，消息队列，标志位</p>
<p>前两个保存一份，后两个保存两份（一份为当前超步一份为下一个超步）</p>
</blockquote>
<blockquote>
<p>Worker对自己管辖分区中每个顶点进行遍历，并调用Compute函数</p>
</blockquote>
<blockquote>
<p>如果一个顶点V在超步S接收到消息表示V将会在下一个超步S+1中处于”活跃”状态</p>
<p>如果目标顶点在<strong>同台机器</strong>，直接把消息放入与目标顶点U对应的输入消息队列中；如果在<strong>远程机器</strong>，<strong>暂时缓存本地</strong>，当缓存中的消息数目达到一个事先设定的<em>阈值</em>时，这些缓存消息会被批量异步发送出去，传输到目标顶点所在的Woker上</p>
</blockquote>
<h6 id="Master"><a href="#Master" class="headerlink" title="Master"></a>Master</h6><ul>
<li><p>扮演管家角色，主要协调Worker执行各个任务</p>
</li>
<li><p>维护关于当前处于有效状态的所有Worker的各种信息，包括每个Worker的ID和地址信息，以及每个Worker被分配到的分区信息</p>
</li>
<li><p>Master中保存这些信息的数据结构的大小，<strong>只与分区的数量有关</strong>，而与顶点和边的数量无关</p>
</li>
</ul>
<p><strong>Master指令</strong>：Master向所有处于有效状态的Worker发送相同指令，然后等待Worker回应，在指定时间内某一个Worker没有回应说明该Worker已经失效，Master会进入恢复模式</p>
<blockquote>
<p>在每个超步中，图计算的各个任务(包括输入输出、计算保存、检查点恢复)，会在”路障”之前结束</p>
</blockquote>
<p><strong>Master在内部运行了一个HTTP服务器来显示图计算过程的各种信息</strong></p>
<h6 id="Aggregator"><a href="#Aggregator" class="headerlink" title="Aggregator"></a>Aggregator</h6><ul>
<li>在执行图计算过程的某个超步S中，每个Worker会利用一个Aggregator对当前本地分区中包含的所有顶点的值进行归约，得到一个本地的局部规约值</li>
<li>在超步S结束时，所有Worker会将所有包含局部归约值的Aggregator的值进行最后汇总，得到全局值，提交给Master</li>
<li>在下一个超步S+1开始时，Master会将Aggregator的全局值发送给每个Worker</li>
</ul>
<p>应用：<a target="_blank" rel="noopener" href="https://www.icourse163.org/learn/XMU-1002335004?tid=1460162442#/learn/content?type=detail&amp;id=1236280069&amp;cid=1256005043&amp;replay=true">https://www.icourse163.org/learn/XMU-1002335004?tid=1460162442#/learn/content?type=detail&amp;id=1236280069&amp;cid=1256005043&amp;replay=true</a></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">JH</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://www.gtxhjh.cn/2021/01/28/BigData/bigdata/">https://www.gtxhjh.cn/2021/01/28/BigData/bigdata/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://www.gtxhjh.cn" target="_blank">阿花花花deCSNotes</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2021/01/28/Vue/vue/"><img class="prev-cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Vue</div></div></a></div><div class="next-post pull-right"><a href="/2021/01/27/hello-world/"><img class="next-cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Hello World</div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-info-avatar is-center"><img class="avatar-img" src="/img/avatar.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">JH</div><div class="author-info__description"></div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">22</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">16</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/jhhhh9"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/jhhhh9" target="_blank" title="Github"><i class="fab fa-github"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">更新CS笔记</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A6%82%E8%BF%B0"><span class="toc-number">1.</span> <span class="toc-text">大数据概述</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A6%82%E5%BF%B5%E5%92%8C%E5%BD%B1%E5%93%8D"><span class="toc-number">1.1.</span> <span class="toc-text">大数据概念和影响</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%89%B9%E7%82%B9%EF%BC%9A"><span class="toc-number">1.1.1.</span> <span class="toc-text">大数据特点：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BD%B1%E5%93%8D"><span class="toc-number">1.1.2.</span> <span class="toc-text">大数据影响</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF"><span class="toc-number">1.2.</span> <span class="toc-text">大数据关键技术</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B8%8E%E4%BA%91%E8%AE%A1%E7%AE%97%E3%80%81%E7%89%A9%E8%81%94%E7%BD%91"><span class="toc-number">1.3.</span> <span class="toc-text">大数据与云计算、物联网</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%91%E8%AE%A1%E7%AE%97"><span class="toc-number">1.3.1.</span> <span class="toc-text">云计算</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%99%9A%E6%8B%9F%E5%8C%96%E6%8A%80%E6%9C%AF"><span class="toc-number">1.3.1.1.</span> <span class="toc-text">虚拟化技术</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BA%91%E8%AE%A1%E7%AE%97%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%BF%83"><span class="toc-number">1.3.1.2.</span> <span class="toc-text">云计算数据中心</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%89%A9%E8%81%94%E7%BD%91-IoT"><span class="toc-number">1.3.2.</span> <span class="toc-text">物联网 IoT</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B1%82%E6%AC%A1%E6%9E%B6%E6%9E%84"><span class="toc-number">1.3.2.1.</span> <span class="toc-text">层次架构</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B3%E7%B3%BB"><span class="toc-number">1.3.3.</span> <span class="toc-text">关系</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%9E%B6%E6%9E%84Hadoop"><span class="toc-number">2.</span> <span class="toc-text">大数据处理架构Hadoop</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A6%82%E8%BF%B0"><span class="toc-number">2.1.</span> <span class="toc-text">概述</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%89%B9%E7%82%B9"><span class="toc-number">2.1.1.</span> <span class="toc-text">特点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%89%88%E6%9C%AC"><span class="toc-number">2.1.2.</span> <span class="toc-text">版本</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%90%84%E7%BB%84%E4%BB%B6%E3%80%81%E5%8A%9F%E8%83%BD"><span class="toc-number">2.2.</span> <span class="toc-text">各组件、功能</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A1%B9%E7%9B%AE%E7%BB%93%E6%9E%84"><span class="toc-number">2.2.1.</span> <span class="toc-text">项目结构</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9FHDFS"><span class="toc-number">3.</span> <span class="toc-text">分布式文件系统HDFS</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A6%82%E8%BF%B0-1"><span class="toc-number">3.1.</span> <span class="toc-text">概述</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#HDFS%E5%AE%9E%E7%8E%B0%E7%9B%AE%E6%A0%87"><span class="toc-number">3.1.1.</span> <span class="toc-text">HDFS实现目标</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B1%80%E9%99%90%E6%80%A7"><span class="toc-number">3.1.2.</span> <span class="toc-text">局限性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5"><span class="toc-number">3.1.3.</span> <span class="toc-text">相关概念</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9D%97"><span class="toc-number">3.1.3.1.</span> <span class="toc-text">块</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%A4%E5%A4%A7%E7%BB%84%E4%BB%B6"><span class="toc-number">3.1.3.2.</span> <span class="toc-text">两大组件</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%85%83%E6%95%B0%E6%8D%AE"><span class="toc-number">3.1.3.2.1.</span> <span class="toc-text">元数据</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%90%8D%E7%A7%B0%E8%8A%82%E7%82%B9"><span class="toc-number">3.1.3.2.2.</span> <span class="toc-text">名称节点</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#FsImage"><span class="toc-number">3.1.3.2.2.1.</span> <span class="toc-text">FsImage</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#EditLog"><span class="toc-number">3.1.3.2.2.2.</span> <span class="toc-text">EditLog</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86"><span class="toc-number">3.1.3.2.2.3.</span> <span class="toc-text">如何处理</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E5%90%8D%E7%A7%B0%E8%8A%82%E7%82%B9"><span class="toc-number">3.1.3.2.2.4.</span> <span class="toc-text">第二名称节点</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E8%8A%82%E7%82%B9"><span class="toc-number">3.1.3.2.3.</span> <span class="toc-text">数据节点</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84"><span class="toc-number">3.2.</span> <span class="toc-text">体系结构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#HDFS%E5%91%BD%E5%90%8D%E7%A9%BA%E9%97%B4%E7%AE%A1%E7%90%86"><span class="toc-number">3.2.1.</span> <span class="toc-text">HDFS命名空间管理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%9A%E4%BF%A1%E5%8D%8F%E8%AE%AE"><span class="toc-number">3.2.2.</span> <span class="toc-text">通信协议</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B1%80%E9%99%90%E6%80%A7%EF%BC%881-0%EF%BC%89"><span class="toc-number">3.2.3.</span> <span class="toc-text">局限性（1.0）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AD%98%E5%82%A8%E5%8E%9F%E7%90%86"><span class="toc-number">3.3.</span> <span class="toc-text">存储原理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%86%97%E4%BD%99%E6%95%B0%E6%8D%AE%E4%BF%9D%E5%AD%98"><span class="toc-number">3.3.1.</span> <span class="toc-text">冗余数据保存</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E4%BF%9D%E5%AD%98%E7%AD%96%E7%95%A5"><span class="toc-number">3.3.2.</span> <span class="toc-text">数据保存策略</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E6%81%A2%E5%A4%8D"><span class="toc-number">3.3.3.</span> <span class="toc-text">数据恢复</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%90%8D%E7%A7%B0%E8%8A%82%E7%82%B9%E7%9A%84%E5%87%BA%E9%94%99"><span class="toc-number">3.3.3.1.</span> <span class="toc-text">名称节点的出错</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E8%8A%82%E7%82%B9%E7%9A%84%E5%87%BA%E9%94%99"><span class="toc-number">3.3.3.2.</span> <span class="toc-text">数据节点的出错</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E6%9C%AC%E8%BA%AB%E7%9A%84%E5%87%BA%E9%94%99"><span class="toc-number">3.3.3.3.</span> <span class="toc-text">数据本身的出错</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%86%99"><span class="toc-number">3.4.</span> <span class="toc-text">数据读写</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%BB%E8%BF%87%E7%A8%8B"><span class="toc-number">3.4.1.</span> <span class="toc-text">读过程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%86%99%E8%BF%87%E7%A8%8B"><span class="toc-number">3.4.2.</span> <span class="toc-text">写过程</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#HDFS%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4"><span class="toc-number">3.5.</span> <span class="toc-text">HDFS常用命令</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%96%87%E4%BB%B6%E6%93%8D%E4%BD%9C"><span class="toc-number">3.5.1.</span> <span class="toc-text">文件操作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%A9%E7%94%A8Web%E7%95%8C%E9%9D%A2%E7%AE%A1%E7%90%86HDFS"><span class="toc-number">3.5.2.</span> <span class="toc-text">利用Web界面管理HDFS</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#HDFS%E5%B8%B8%E7%94%A8JAVA-API"><span class="toc-number">3.6.</span> <span class="toc-text">HDFS常用JAVA API</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E7%9A%84%E9%83%A8%E7%BD%B2"><span class="toc-number">3.6.1.</span> <span class="toc-text">应用程序的部署</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%83%E4%B9%A0"><span class="toc-number">3.6.2.</span> <span class="toc-text">练习</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%86%99%E5%85%A5%E6%96%87%E4%BB%B6"><span class="toc-number">3.6.2.1.</span> <span class="toc-text">写入文件</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%A4%E6%96%AD%E6%96%87%E4%BB%B6%E6%98%AF%E5%90%A6%E5%AD%98%E5%9C%A8"><span class="toc-number">3.6.2.2.</span> <span class="toc-text">判断文件是否存在</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AF%BB%E5%8F%96%E6%96%87%E4%BB%B6"><span class="toc-number">3.6.2.3.</span> <span class="toc-text">读取文件</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93HBase"><span class="toc-number">4.</span> <span class="toc-text">分布式数据库HBase</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A6%82%E8%BF%B0-2"><span class="toc-number">4.1.</span> <span class="toc-text">概述</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#HBase%E5%92%8C%E4%BC%A0%E7%BB%9F%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E8%81%94%E7%B3%BB%E5%92%8C%E5%8C%BA%E5%88%AB"><span class="toc-number">4.1.1.</span> <span class="toc-text">HBase和传统关系型数据库的联系和区别</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#HBase%E8%AE%BF%E9%97%AE%E6%8E%A5%E5%8F%A3"><span class="toc-number">4.1.2.</span> <span class="toc-text">HBase访问接口</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9E%8B"><span class="toc-number">4.2.</span> <span class="toc-text">数据模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%9D%90%E6%A0%87"><span class="toc-number">4.2.1.</span> <span class="toc-text">数据坐标</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A6%82%E5%BF%B5%E8%A7%86%E5%9B%BE"><span class="toc-number">4.2.2.</span> <span class="toc-text">概念视图</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%89%A9%E7%90%86%E8%A7%86%E5%9B%BE"><span class="toc-number">4.2.3.</span> <span class="toc-text">物理视图</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86"><span class="toc-number">4.3.</span> <span class="toc-text">实现原理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8A%9F%E8%83%BD%E7%BB%84%E4%BB%B6"><span class="toc-number">4.3.1.</span> <span class="toc-text">功能组件</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BA%93%E5%87%BD%E6%95%B0"><span class="toc-number">4.3.1.1.</span> <span class="toc-text">库函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#master%E6%9C%8D%E5%8A%A1%E5%99%A8"><span class="toc-number">4.3.1.2.</span> <span class="toc-text">master服务器</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#region%E6%9C%8D%E5%8A%A1%E5%99%A8"><span class="toc-number">4.3.1.3.</span> <span class="toc-text">region服务器</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5%E8%A1%A8%E4%B8%8ERegion"><span class="toc-number">4.3.2.</span> <span class="toc-text">核心概念表与Region</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Region%E5%AE%9A%E4%BD%8D"><span class="toc-number">4.3.3.</span> <span class="toc-text">Region定位</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BF%90%E8%A1%8C%E6%9C%BA%E5%88%B6"><span class="toc-number">4.4.</span> <span class="toc-text">运行机制</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84"><span class="toc-number">4.4.1.</span> <span class="toc-text">系统架构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Region%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86"><span class="toc-number">4.4.2.</span> <span class="toc-text">Region服务器工作原理</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%94%A8%E6%88%B7%E8%AF%BB%E5%86%99%E6%95%B0%E6%8D%AE%E8%BF%87%E7%A8%8B"><span class="toc-number">4.4.2.1.</span> <span class="toc-text">用户读写数据过程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BC%93%E5%AD%98%E5%88%B7%E6%96%B0"><span class="toc-number">4.4.2.2.</span> <span class="toc-text">缓存刷新</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Store%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86"><span class="toc-number">4.4.3.</span> <span class="toc-text">Store工作原理</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#StoreFile%E5%90%88%E5%B9%B6"><span class="toc-number">4.4.3.1.</span> <span class="toc-text">StoreFile合并</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#HLog%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86"><span class="toc-number">4.4.4.</span> <span class="toc-text">HLog工作原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#zookeper"><span class="toc-number">4.4.5.</span> <span class="toc-text">zookeper</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#HBase%E5%BA%94%E7%94%A8%E6%96%B9%E6%A1%88"><span class="toc-number">4.5.</span> <span class="toc-text">HBase应用方案</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95"><span class="toc-number">4.5.1.</span> <span class="toc-text">性能优化方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A3%80%E6%B5%8B%E6%80%A7%E8%83%BD"><span class="toc-number">4.5.2.</span> <span class="toc-text">检测性能</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9E%84%E5%BB%BAsql%E5%BC%95%E6%93%8E%E5%92%8CHBase%E4%BA%8C%E7%BA%A7%E7%B4%A2%E5%BC%95"><span class="toc-number">4.5.3.</span> <span class="toc-text">构建sql引擎和HBase二级索引</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#sql%E5%BC%95%E6%93%8E"><span class="toc-number">4.5.3.1.</span> <span class="toc-text">sql引擎</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#HBase%E4%BA%8C%E7%BA%A7%E7%B4%A2%E5%BC%95-%E8%BE%85%E5%8A%A9%E7%B4%A2%E5%BC%95"><span class="toc-number">4.5.3.2.</span> <span class="toc-text">HBase二级索引(辅助索引)</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#HBase-%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="toc-number">4.6.</span> <span class="toc-text">HBase 数据库</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E8%A1%A8"><span class="toc-number">4.6.1.</span> <span class="toc-text">创建表</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B7%BB%E5%8A%A0%E6%95%B0%E6%8D%AE"><span class="toc-number">4.6.2.</span> <span class="toc-text">添加数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%A0%E9%99%A4%E6%95%B0%E6%8D%AE"><span class="toc-number">4.6.3.</span> <span class="toc-text">删除数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9F%A5%E7%9C%8B%E6%95%B0%E6%8D%AE"><span class="toc-number">4.6.4.</span> <span class="toc-text">查看数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%A0%E9%99%A4%E8%A1%A8"><span class="toc-number">4.6.5.</span> <span class="toc-text">删除表</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9F%A5%E8%AF%A2%E8%A1%A8%E5%8E%86%E5%8F%B2%E6%95%B0%E6%8D%AE"><span class="toc-number">4.6.6.</span> <span class="toc-text">查询表历史数据</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9F%A5%E8%AF%A2%E5%8E%86%E5%8F%B2%E7%89%88%E6%9C%AC"><span class="toc-number">4.6.6.1.</span> <span class="toc-text">查询历史版本</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#HBase-JAVA-API"><span class="toc-number">4.7.</span> <span class="toc-text">HBase JAVA API</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#NoSQL%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="toc-number">5.</span> <span class="toc-text">NoSQL数据库</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A6%82%E8%BF%B0-3"><span class="toc-number">5.1.</span> <span class="toc-text">概述</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8E%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E6%AF%94%E8%BE%83"><span class="toc-number">5.1.1.</span> <span class="toc-text">与关系数据库比较</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%9B%E5%A4%A7%E7%B1%BB%E5%9E%8B%E3%80%81%E4%B8%89%E5%A4%A7%E5%9F%BA%E7%9F%B3"><span class="toc-number">5.1.2.</span> <span class="toc-text">四大类型、三大基石</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9B%9B%E5%A4%A7%E7%B1%BB%E5%9E%8B"><span class="toc-number">5.1.2.1.</span> <span class="toc-text">四大类型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%89%E5%A4%A7%E5%9F%BA%E7%9F%B3"><span class="toc-number">5.1.2.2.</span> <span class="toc-text">三大基石</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#CAP"><span class="toc-number">5.1.2.2.1.</span> <span class="toc-text">CAP</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#BASE-Basically-Avaible-Soft-state%E3%80%81Eventual-consistency"><span class="toc-number">5.1.2.2.2.</span> <span class="toc-text">BASE-Basically Avaible Soft state、Eventual consistency</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%E5%AE%9E%E7%8E%B0%E4%B8%80%E8%87%B4%E6%80%A7"><span class="toc-number">5.1.2.2.2.1.</span> <span class="toc-text">实现一致性</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#NoSQL%E5%92%8CNewSQL%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8C%BA%E5%88%AB"><span class="toc-number">5.1.3.</span> <span class="toc-text">NoSQL和NewSQL数据库区别</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%96%87%E6%A1%A3%E6%95%B0%E6%8D%AE%E5%BA%93MongoDB%E4%B8%BA%E5%AE%9E%E4%BE%8B%E4%BB%8B%E7%BB%8DNoSQL%E6%95%B0%E6%8D%AE%E5%BA%93%E7%BC%96%E7%A8%8B%E5%AE%9E%E6%88%98"><span class="toc-number">5.1.4.</span> <span class="toc-text">文档数据库MongoDB为实例介绍NoSQL数据库编程实战</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%8E%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E6%AF%94%E8%BE%83"><span class="toc-number">5.1.4.1.</span> <span class="toc-text">与关系数据库的比较</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="toc-number">5.1.4.2.</span> <span class="toc-text">数据库</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%96%87%E6%A1%A3"><span class="toc-number">5.1.4.3.</span> <span class="toc-text">文档</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%91%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="toc-number">6.</span> <span class="toc-text">云数据库</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A6%82%E8%BF%B0-4"><span class="toc-number">6.1.</span> <span class="toc-text">概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#UMP%E7%B3%BB%E7%BB%9F"><span class="toc-number">6.2.</span> <span class="toc-text">UMP系统</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A6%82%E8%BF%B0-5"><span class="toc-number">6.2.1.</span> <span class="toc-text">概述</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9E%B6%E6%9E%84"><span class="toc-number">6.2.2.</span> <span class="toc-text">架构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8A%9F%E8%83%BD"><span class="toc-number">6.2.3.</span> <span class="toc-text">功能</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#MapReduce"><span class="toc-number">7.</span> <span class="toc-text">MapReduce</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A6%82%E8%BF%B0-6"><span class="toc-number">7.1.</span> <span class="toc-text">概述</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E5%B8%83%E5%BC%8F%E5%B9%B6%E8%A1%8C%E7%BC%96%E7%A8%8B"><span class="toc-number">7.1.1.</span> <span class="toc-text">分布式并行编程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B"><span class="toc-number">7.1.2.</span> <span class="toc-text">模型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AD%96%E7%95%A5%EF%BC%9A%E5%88%86%E8%80%8C%E6%B2%BB%E4%B9%8B"><span class="toc-number">7.1.2.1.</span> <span class="toc-text">策略：分而治之</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9E%B6%E6%9E%84-1"><span class="toc-number">7.1.2.2.</span> <span class="toc-text">架构</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#map"><span class="toc-number">7.1.2.3.</span> <span class="toc-text">map</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#reduce"><span class="toc-number">7.1.2.4.</span> <span class="toc-text">reduce</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84-1"><span class="toc-number">7.2.</span> <span class="toc-text">体系结构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#client"><span class="toc-number">7.2.1.</span> <span class="toc-text">client</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#jobtracker"><span class="toc-number">7.2.2.</span> <span class="toc-text">jobtracker</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tasktrackr-%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6%E5%99%A8"><span class="toc-number">7.2.3.</span> <span class="toc-text">tasktrackr-任务调度器</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A1%A1%E9%87%8F%E8%B5%84%E6%BA%90%E7%8A%B6%E6%80%81-slot%E6%A7%BD"><span class="toc-number">7.2.3.1.</span> <span class="toc-text">衡量资源状态-slot槽</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#task-schedule"><span class="toc-number">7.2.4.</span> <span class="toc-text">task schedule</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B"><span class="toc-number">7.3.</span> <span class="toc-text">工作流程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%A7%E8%A1%8C%E7%9A%84%E5%90%84%E4%B8%AA%E9%98%B6%E6%AE%B5"><span class="toc-number">7.3.1.</span> <span class="toc-text">执行的各个阶段</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#split-%E5%88%86%E7%89%87"><span class="toc-number">7.3.1.1.</span> <span class="toc-text">split-分片</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#map%E3%80%81reduce%E5%88%86%E9%85%8D"><span class="toc-number">7.3.1.2.</span> <span class="toc-text">map、reduce分配</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#shuffle%E8%BF%87%E7%A8%8B%E5%8E%9F%E7%90%86"><span class="toc-number">7.4.</span> <span class="toc-text">shuffle过程原理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#map%E7%AB%AFshuffle"><span class="toc-number">7.4.1.</span> <span class="toc-text">map端shuffle</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BE%93%E5%85%A5%E6%95%B0%E6%8D%AE%E5%92%8C%E6%89%A7%E8%A1%8Cmap%E4%BB%BB%E5%8A%A1"><span class="toc-number">7.4.1.1.</span> <span class="toc-text">输入数据和执行map任务</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%86%99%E5%85%A5%E7%BC%93%E5%AD%98"><span class="toc-number">7.4.1.2.</span> <span class="toc-text">写入缓存</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%BA%A2%E5%86%99"><span class="toc-number">7.4.1.3.</span> <span class="toc-text">溢写</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%88%86%E5%8C%BA"><span class="toc-number">7.4.1.3.1.</span> <span class="toc-text">分区</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%8E%92%E5%BA%8F"><span class="toc-number">7.4.1.3.2.</span> <span class="toc-text">排序</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%90%88%E5%B9%B6%EF%BC%88%E9%9D%9E%E5%BF%85%E9%A1%BB%EF%BC%89"><span class="toc-number">7.4.1.3.3.</span> <span class="toc-text">合并（非必须）</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BD%92%E5%B9%B6"><span class="toc-number">7.4.1.4.</span> <span class="toc-text">归并</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#reduce%E7%AB%AFshuffle"><span class="toc-number">7.4.2.</span> <span class="toc-text">reduce端shuffle</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B"><span class="toc-number">7.5.</span> <span class="toc-text">应用程序执行过程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E4%BE%8B"><span class="toc-number">7.6.</span> <span class="toc-text">实例</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E7%8E%B0%E8%87%AA%E7%84%B6%E8%BF%9E%E6%8E%A5"><span class="toc-number">7.6.1.</span> <span class="toc-text">实现自然连接</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C"><span class="toc-number">7.7.</span> <span class="toc-text">实验</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Hadoop%E5%8F%91%E5%B1%95"><span class="toc-number">8.</span> <span class="toc-text">Hadoop发展</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#HDFS-HA%E3%80%81HDFS-Federation"><span class="toc-number">8.1.</span> <span class="toc-text">HDFS HA、HDFS Federation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#HDFS-HA"><span class="toc-number">8.1.1.</span> <span class="toc-text">HDFS HA</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#HDFS-Fedration"><span class="toc-number">8.1.2.</span> <span class="toc-text">HDFS Fedration</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Yarn"><span class="toc-number">8.2.</span> <span class="toc-text">Yarn</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84-2"><span class="toc-number">8.2.1.</span> <span class="toc-text">体系结构</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#ResourceManager"><span class="toc-number">8.2.1.1.</span> <span class="toc-text">ResourceManager</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%B0%83%E5%BA%A6%E5%99%A8"><span class="toc-number">8.2.1.1.1.</span> <span class="toc-text">调度器</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E7%AE%A1%E7%90%86%E5%99%A8"><span class="toc-number">8.2.1.1.2.</span> <span class="toc-text">应用程序管理器</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#ApplicationMaster"><span class="toc-number">8.2.1.2.</span> <span class="toc-text">ApplicationMaster</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#NodeManager"><span class="toc-number">8.2.1.3.</span> <span class="toc-text">NodeManager</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B-1"><span class="toc-number">8.2.2.</span> <span class="toc-text">工作流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%91%E5%B1%95%E7%9B%AE%E6%A0%87"><span class="toc-number">8.2.3.</span> <span class="toc-text">发展目标</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B6%E4%BB%96%E7%BB%84%E4%BB%B6"><span class="toc-number">8.3.</span> <span class="toc-text">其他组件</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Pig"><span class="toc-number">8.3.1.</span> <span class="toc-text">Pig</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Tez"><span class="toc-number">8.3.2.</span> <span class="toc-text">Tez</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Spark"><span class="toc-number">8.3.3.</span> <span class="toc-text">Spark</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Kafka"><span class="toc-number">8.3.4.</span> <span class="toc-text">Kafka</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93Hive"><span class="toc-number">9.</span> <span class="toc-text">数据仓库Hive</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A6%82%E8%BF%B0-7"><span class="toc-number">9.1.</span> <span class="toc-text">概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SQL%E8%BD%ACMapReduce%E4%BD%9C%E4%B8%9A%E5%8E%9F%E7%90%86"><span class="toc-number">9.2.</span> <span class="toc-text">SQL转MapReduce作业原理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Impala"><span class="toc-number">9.3.</span> <span class="toc-text">Impala</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84-1"><span class="toc-number">9.3.1.</span> <span class="toc-text">系统架构</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Impalad"><span class="toc-number">9.3.1.1.</span> <span class="toc-text">Impalad</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#State-Store"><span class="toc-number">9.3.1.2.</span> <span class="toc-text">State Store</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#CLI"><span class="toc-number">9.3.1.3.</span> <span class="toc-text">CLI</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9F%A5%E8%AF%A2%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B"><span class="toc-number">9.3.2.</span> <span class="toc-text">查询执行过程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8EHive%E5%AF%B9%E6%AF%94"><span class="toc-number">9.3.3.</span> <span class="toc-text">与Hive对比</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C"><span class="toc-number">9.4.</span> <span class="toc-text">基本操作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B"><span class="toc-number">9.4.1.</span> <span class="toc-text">基本数据类型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B-1"><span class="toc-number">9.4.1.1.</span> <span class="toc-text">基本数据类型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%8D%E6%9D%82%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B"><span class="toc-number">9.4.1.2.</span> <span class="toc-text">复杂数据类型</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4"><span class="toc-number">9.4.2.</span> <span class="toc-text">操作命令</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Spark-1"><span class="toc-number">10.</span> <span class="toc-text">Spark</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%94%9F%E6%80%81%E7%B3%BB%E7%BB%9F"><span class="toc-number">10.1.</span> <span class="toc-text">生态系统</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BF%90%E8%A1%8C%E6%9E%B6%E6%9E%84"><span class="toc-number">10.2.</span> <span class="toc-text">运行架构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A6%82%E8%BF%B0%E5%92%8C%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1"><span class="toc-number">10.2.1.</span> <span class="toc-text">概述和架构设计</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BF%90%E8%A1%8C%E6%9E%B6%E6%9E%84-1"><span class="toc-number">10.2.1.1.</span> <span class="toc-text">运行架构</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BC%98%E7%82%B9%EF%BC%9A"><span class="toc-number">10.2.1.1.1.</span> <span class="toc-text">优点：</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BF%90%E8%A1%8C%E8%BF%87%E7%A8%8B"><span class="toc-number">10.2.1.2.</span> <span class="toc-text">运行过程</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Spark%E8%BF%90%E8%A1%8C%E5%9F%BA%E6%9C%AC%E6%B5%81%E7%A8%8B"><span class="toc-number">10.2.2.</span> <span class="toc-text">Spark运行基本流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RDD"><span class="toc-number">10.2.3.</span> <span class="toc-text">RDD</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A6%82%E8%BF%B0-8"><span class="toc-number">10.2.3.1.</span> <span class="toc-text">概述</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%89%B9%E6%80%A7"><span class="toc-number">10.2.3.2.</span> <span class="toc-text">特性</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BE%9D%E8%B5%96%E5%85%B3%E7%B3%BB%E5%92%8C%E8%BF%90%E8%A1%8C%E8%BF%87%E7%A8%8B"><span class="toc-number">10.2.3.3.</span> <span class="toc-text">依赖关系和运行过程</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SparkSQL"><span class="toc-number">10.3.</span> <span class="toc-text">SparkSQL</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%83%A8%E7%BD%B2%E5%92%8C%E5%BA%94%E7%94%A8"><span class="toc-number">10.4.</span> <span class="toc-text">部署和应用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#shell"><span class="toc-number">10.5.</span> <span class="toc-text">shell</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%AF%E5%8A%A8"><span class="toc-number">10.5.1.</span> <span class="toc-text">启动</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8A%A0%E8%BD%BDtext%E6%96%87%E4%BB%B6"><span class="toc-number">10.5.2.</span> <span class="toc-text">加载text文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%80%E5%8D%95RDD%E6%93%8D%E4%BD%9C"><span class="toc-number">10.5.3.</span> <span class="toc-text">简单RDD操作</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#action"><span class="toc-number">10.5.3.1.</span> <span class="toc-text">action</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#transformation"><span class="toc-number">10.5.3.2.</span> <span class="toc-text">transformation</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%80%E5%87%BA"><span class="toc-number">10.5.4.</span> <span class="toc-text">退出</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%8B%AC%E7%AB%8B%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E7%BC%96%E7%A8%8B"><span class="toc-number">10.6.</span> <span class="toc-text">独立应用程序编程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8sbt%E5%AF%B9Scala%E7%8B%AC%E7%AB%8B%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E8%BF%9B%E8%A1%8C%E7%BC%96%E8%AF%91%E6%89%93%E5%8C%85"><span class="toc-number">10.6.1.</span> <span class="toc-text">使用sbt对Scala独立应用程序进行编译打包</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8Maven%E5%AF%B9Java%E7%8B%AC%E7%AB%8B%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E8%BF%9B%E8%A1%8C%E7%BC%96%E8%AF%91%E6%89%93%E5%8C%85"><span class="toc-number">10.6.2.</span> <span class="toc-text">使用Maven对Java独立应用程序进行编译打包</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8Maven%E5%AF%B9Scala%E7%8B%AC%E7%AB%8B%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E8%BF%9B%E8%A1%8C%E7%BC%96%E8%AF%91%E6%89%93%E5%8C%85"><span class="toc-number">10.6.3.</span> <span class="toc-text">使用Maven对Scala独立应用程序进行编译打包</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RDD%E7%BC%96%E7%A8%8B"><span class="toc-number">10.7.</span> <span class="toc-text">RDD编程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%EF%BC%9A"><span class="toc-number">10.7.1.</span> <span class="toc-text">创建：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%93%8D%E4%BD%9C%EF%BC%9A"><span class="toc-number">10.7.2.</span> <span class="toc-text">操作：</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#transformation-1"><span class="toc-number">10.7.2.1.</span> <span class="toc-text">transformation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#action-1"><span class="toc-number">10.7.2.2.</span> <span class="toc-text">action</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8C%81%E4%B9%85%E5%8C%96"><span class="toc-number">10.7.3.</span> <span class="toc-text">持久化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E5%8C%BA-1"><span class="toc-number">10.7.4.</span> <span class="toc-text">分区</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%86%E5%8C%BA%E4%BD%9C%E7%94%A8%E5%92%8C%E5%8E%9F%E5%88%99"><span class="toc-number">10.7.4.1.</span> <span class="toc-text">分区作用和原则</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%86%E5%8C%BA%E8%AE%BE%E7%BD%AE%E6%96%B9%E6%B3%95"><span class="toc-number">10.7.4.2.</span> <span class="toc-text">分区设置方法</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%94%AE%E5%80%BC%E5%AF%B9RDD"><span class="toc-number">10.7.5.</span> <span class="toc-text">键值对RDD</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%EF%BC%9A-1"><span class="toc-number">10.7.5.1.</span> <span class="toc-text">创建：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B8%B8%E8%A7%81%E8%BD%AC%E6%8D%A2%E6%93%8D%E4%BD%9C"><span class="toc-number">10.7.5.2.</span> <span class="toc-text">常见转换操作</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%86%99-1"><span class="toc-number">10.7.6.</span> <span class="toc-text">数据读写</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%86%99"><span class="toc-number">10.7.6.1.</span> <span class="toc-text">文件系统数据读写</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%9C%AC%E5%9C%B0%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F"><span class="toc-number">10.7.6.1.1.</span> <span class="toc-text">本地文件系统</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#HDFS"><span class="toc-number">10.7.6.1.2.</span> <span class="toc-text">HDFS</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#json%E6%96%87%E4%BB%B6%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%86%99"><span class="toc-number">10.7.6.2.</span> <span class="toc-text">json文件数据读写</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AF%BB%E5%86%99HBase%E6%95%B0%E6%8D%AE"><span class="toc-number">10.7.6.3.</span> <span class="toc-text">读写HBase数据</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark-SQL"><span class="toc-number">10.8.</span> <span class="toc-text">Spark SQL</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#DataFrame"><span class="toc-number">10.8.1.</span> <span class="toc-text">DataFrame</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%9B%E5%BB%BADataFrame"><span class="toc-number">10.8.1.1.</span> <span class="toc-text">创建DataFrame</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%98%BE%E7%A4%BA"><span class="toc-number">10.8.1.2.</span> <span class="toc-text">显示</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BF%9D%E5%AD%98DataFrame"><span class="toc-number">10.8.1.3.</span> <span class="toc-text">保存DataFrame</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%B6%E4%BB%96"><span class="toc-number">10.8.1.4.</span> <span class="toc-text">其他</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%8ERDD%E8%BD%AC%E6%8D%A2%E5%BE%97%E5%88%B0DataFrame"><span class="toc-number">10.8.2.</span> <span class="toc-text">从RDD转换得到DataFrame</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%A9%E7%94%A8%E5%8F%8D%E5%B0%84%E6%9C%BA%E5%88%B6%E6%8E%A8%E6%96%ADRDD%E6%A8%A1%E5%BC%8F"><span class="toc-number">10.8.2.1.</span> <span class="toc-text">利用反射机制推断RDD模式</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BC%96%E7%A8%8B%E6%96%B9%E5%BC%8F%E5%AE%9A%E4%B9%89RDD%E6%A8%A1%E5%BC%8F"><span class="toc-number">10.8.2.2.</span> <span class="toc-text">编程方式定义RDD模式</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8Spark-SQL-%E8%AF%BB%E5%86%99%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="toc-number">10.8.3.</span> <span class="toc-text">使用Spark SQL 读写数据库</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark-Streaming"><span class="toc-number">10.9.</span> <span class="toc-text">Spark Streaming</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A6%82%E8%BF%B0-9"><span class="toc-number">10.9.1.</span> <span class="toc-text">概述</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B5%81%E6%95%B0%E6%8D%AE%E5%9F%BA%E6%9C%AC%E7%90%86%E5%BF%B5%EF%BC%9A"><span class="toc-number">10.9.1.1.</span> <span class="toc-text">流数据基本理念：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BC%A0%E7%BB%9F%E7%9A%84%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B%EF%BC%9A"><span class="toc-number">10.9.1.2.</span> <span class="toc-text">传统的数据处理流程：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B5%81%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%B5%81%E7%A8%8B"><span class="toc-number">10.9.1.3.</span> <span class="toc-text">流数据处理流程</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%AE%9E%E6%97%B6%E9%87%87%E9%9B%86"><span class="toc-number">10.9.1.3.1.</span> <span class="toc-text">数据实时采集</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97"><span class="toc-number">10.9.1.3.2.</span> <span class="toc-text">数据实时计算</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%AE%9E%E6%97%B6%E6%9F%A5%E8%AF%A2%E6%9C%8D%E5%8A%A1"><span class="toc-number">10.9.1.3.3.</span> <span class="toc-text">实时查询服务</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Spark-Streaming-1"><span class="toc-number">10.9.2.</span> <span class="toc-text">Spark Streaming</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E6%8A%BD%E8%B1%A1%EF%BC%9ADStream"><span class="toc-number">10.9.2.1.</span> <span class="toc-text">数据抽象：DStream</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%8EStorm%E5%8C%BA%E5%88%AB"><span class="toc-number">10.9.2.2.</span> <span class="toc-text">与Storm区别</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Spark%E6%9E%B6%E6%9E%84"><span class="toc-number">10.9.2.3.</span> <span class="toc-text">Spark架构</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#DStream%E6%93%8D%E4%BD%9C"><span class="toc-number">10.9.3.</span> <span class="toc-text">DStream操作</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Input-DStream%E7%B1%BB%E5%9E%8B%EF%BC%9A"><span class="toc-number">10.9.3.1.</span> <span class="toc-text">Input DStream类型：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Spark-Streaming-%E7%A8%8B%E5%BA%8F"><span class="toc-number">10.9.3.2.</span> <span class="toc-text">Spark Streaming 程序</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#streaming-Context"><span class="toc-number">10.9.3.3.</span> <span class="toc-text">streaming Context</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E8%BE%93%E5%85%A5%E6%BA%90"><span class="toc-number">10.9.4.</span> <span class="toc-text">基本输入源</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%96%87%E4%BB%B6%E6%B5%81"><span class="toc-number">10.9.4.1.</span> <span class="toc-text">文件流</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A5%97%E6%8E%A5%E5%AD%97%E6%B5%81"><span class="toc-number">10.9.4.2.</span> <span class="toc-text">套接字流</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8NC%E4%BA%A7%E7%94%9F%E6%95%B0%E6%8D%AE"><span class="toc-number">10.9.4.2.1.</span> <span class="toc-text">使用NC产生数据</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8Socket%E7%BC%96%E7%A8%8B%E5%AE%9E%E7%8E%B0%E8%87%AA%E5%AE%9A%E4%B9%89%E6%95%B0%E6%8D%AE%E6%BA%90"><span class="toc-number">10.9.4.2.2.</span> <span class="toc-text">使用Socket编程实现自定义数据源</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#RDD%E9%98%9F%E5%88%97%E6%B5%81"><span class="toc-number">10.9.4.3.</span> <span class="toc-text">RDD队列流</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%AB%98%E7%BA%A7%E6%95%B0%E6%8D%AE%E6%BA%90"><span class="toc-number">10.9.5.</span> <span class="toc-text">高级数据源</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#kafka"><span class="toc-number">10.9.5.1.</span> <span class="toc-text">kafka</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%A6%82%E8%BF%B0-10"><span class="toc-number">10.9.5.1.1.</span> <span class="toc-text">概述</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#Broker"><span class="toc-number">10.9.5.1.1.1.</span> <span class="toc-text">Broker</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#Topic"><span class="toc-number">10.9.5.1.1.2.</span> <span class="toc-text">Topic</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#Partition"><span class="toc-number">10.9.5.1.1.3.</span> <span class="toc-text">Partition</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#Producer"><span class="toc-number">10.9.5.1.1.4.</span> <span class="toc-text">Producer</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#consumer"><span class="toc-number">10.9.5.1.1.5.</span> <span class="toc-text">consumer</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#Consumer-Group"><span class="toc-number">10.9.5.1.1.6.</span> <span class="toc-text">Consumer Group</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%E6%9E%B6%E6%9E%84%E5%9B%BE"><span class="toc-number">10.9.5.1.1.7.</span> <span class="toc-text">架构图</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#kafka%E5%90%AF%E5%8A%A8"><span class="toc-number">10.9.5.1.2.</span> <span class="toc-text">kafka启动</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%94%9F%E4%BA%A7%E8%80%85%E7%A8%8B%E5%BA%8F"><span class="toc-number">10.9.5.2.</span> <span class="toc-text">生产者程序</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B6%88%E8%B4%B9%E8%80%85%E7%A8%8B%E5%BA%8F"><span class="toc-number">10.9.5.3.</span> <span class="toc-text">消费者程序</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%BC%96%E5%86%99%E6%97%A5%E5%BF%97%E6%A0%BC%E5%BC%8F%E8%AE%BE%E7%BD%AE%E7%A8%8B%E5%BA%8F"><span class="toc-number">10.9.5.3.1.</span> <span class="toc-text">编写日志格式设置程序</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BD%AC%E6%8D%A2%E6%93%8D%E4%BD%9C"><span class="toc-number">10.9.6.</span> <span class="toc-text">转换操作</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#DStream%E6%97%A0%E7%8A%B6%E6%80%81%E8%BD%AC%E6%8D%A2"><span class="toc-number">10.9.6.1.</span> <span class="toc-text">DStream无状态转换</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#DStream%E6%9C%89%E7%8A%B6%E6%80%81%E8%BD%AC%E6%8D%A2"><span class="toc-number">10.9.6.2.</span> <span class="toc-text">DStream有状态转换</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E8%BD%AC%E6%8D%A2%E6%93%8D%E4%BD%9C"><span class="toc-number">10.9.6.2.1.</span> <span class="toc-text">滑动窗口转换操作</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#updateStateByKey%E6%93%8D%E4%BD%9C"><span class="toc-number">10.9.6.2.2.</span> <span class="toc-text">updateStateByKey操作</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BE%93%E5%87%BA%E6%93%8D%E4%BD%9C"><span class="toc-number">10.9.7.</span> <span class="toc-text">输出操作</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BE%93%E5%87%BA%E5%88%B0%E6%96%87%E6%9C%AC%E6%96%87%E4%BB%B6"><span class="toc-number">10.9.7.1.</span> <span class="toc-text">输出到文本文件</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BE%93%E5%87%BA%E5%88%B0mysql"><span class="toc-number">10.9.7.2.</span> <span class="toc-text">输出到mysql</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Structured-Streaming"><span class="toc-number">10.9.8.</span> <span class="toc-text">Structured Streaming</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark-MLlib"><span class="toc-number">10.10.</span> <span class="toc-text">Spark MLlib</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A6%82%E8%BF%B0-11"><span class="toc-number">10.10.1.</span> <span class="toc-text">概述</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%B5%81%E6%B0%B4%E7%BA%BF"><span class="toc-number">10.10.2.</span> <span class="toc-text">机器学习流水线</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#pipeline%E6%A6%82%E8%BF%B0"><span class="toc-number">10.10.2.1.</span> <span class="toc-text">pipeline概述</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#DataFrame-1"><span class="toc-number">10.10.2.1.1.</span> <span class="toc-text">DataFrame</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Transformer%E8%BD%AC%E6%8D%A2%E5%99%A8"><span class="toc-number">10.10.2.1.2.</span> <span class="toc-text">Transformer转换器</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%E8%BD%AC%E6%8D%A2%E5%99%A8%E5%AE%9E%E7%8E%B0%E7%9A%84%E6%96%B9%E6%B3%95%E4%B8%BATransform"><span class="toc-number">10.10.2.1.2.1.</span> <span class="toc-text">转换器实现的方法为Transform()</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Estimator%E8%AF%84%E4%BC%B0%E5%99%A8"><span class="toc-number">10.10.2.1.3.</span> <span class="toc-text">Estimator评估器</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%E8%AF%84%E4%BC%B0%E5%99%A8%E5%AE%9E%E7%8E%B0%E7%9A%84%E6%96%B9%E6%B3%95%E4%B8%BAfit"><span class="toc-number">10.10.2.1.3.1.</span> <span class="toc-text">评估器实现的方法为fit()</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#PipeLine"><span class="toc-number">10.10.2.1.4.</span> <span class="toc-text">PipeLine</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%E6%9E%84%E5%BB%BAPipeline%E6%B5%81%E6%B0%B4%E7%BA%BF%EF%BC%9A"><span class="toc-number">10.10.2.1.4.1.</span> <span class="toc-text">构建Pipeline流水线：</span></a></li></ol></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9E%84%E5%BB%BA"><span class="toc-number">10.10.2.2.</span> <span class="toc-text">构建</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BB%BB%E5%8A%A1%E6%8F%8F%E8%BF%B0"><span class="toc-number">10.10.2.2.1.</span> <span class="toc-text">任务描述</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%BF%87%E7%A8%8B"><span class="toc-number">10.10.2.2.2.</span> <span class="toc-text">过程</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%B5%8B%E8%AF%95%E6%95%B0%E6%8D%AE"><span class="toc-number">10.10.2.2.3.</span> <span class="toc-text">测试数据</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%89%B9%E5%BE%81%E6%8A%BD%E5%8F%96%E3%80%81%E8%BD%AC%E5%8C%96%E5%92%8C%E9%80%89%E6%8B%A9"><span class="toc-number">10.10.3.</span> <span class="toc-text">特征抽取、转化和选择</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#TF-IDF"><span class="toc-number">10.10.3.1.</span> <span class="toc-text">TF-IDF</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#TF"><span class="toc-number">10.10.3.1.1.</span> <span class="toc-text">TF</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#IDF"><span class="toc-number">10.10.3.1.2.</span> <span class="toc-text">IDF</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%BF%87%E7%A8%8B-1"><span class="toc-number">10.10.3.1.3.</span> <span class="toc-text">过程</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Word2Vec"><span class="toc-number">10.10.3.2.</span> <span class="toc-text">Word2Vec</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%BF%87%E7%A8%8B-2"><span class="toc-number">10.10.3.2.1.</span> <span class="toc-text">过程</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#CountVectorizer"><span class="toc-number">10.10.3.3.</span> <span class="toc-text">CountVectorizer</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%8F%82%E6%95%B0"><span class="toc-number">10.10.3.3.1.</span> <span class="toc-text">参数</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%BF%87%E7%A8%8B-3"><span class="toc-number">10.10.3.3.2.</span> <span class="toc-text">过程</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%E6%97%A0%E5%85%88%E9%AA%8C"><span class="toc-number">10.10.3.3.2.1.</span> <span class="toc-text">无先验</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%E5%90%AB%E5%85%88%E9%AA%8C"><span class="toc-number">10.10.3.3.2.2.</span> <span class="toc-text">含先验</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E7%B1%BB%E5%92%8C%E5%9B%9E%E5%BD%92"><span class="toc-number">10.10.4.</span> <span class="toc-text">分类和回归</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%80%BB%E8%BE%91%E6%96%AF%E8%92%82%E5%9B%9E%E5%BD%92%E5%88%86%E7%B1%BB%E5%99%A8"><span class="toc-number">10.10.4.1.</span> <span class="toc-text">逻辑斯蒂回归分类器</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91%E5%88%86%E7%B1%BB%E5%99%A8"><span class="toc-number">10.10.4.2.</span> <span class="toc-text">决策树分类器</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%A6%82%E8%BF%B0-12"><span class="toc-number">10.10.4.2.1.</span> <span class="toc-text">概述</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%BF%87%E7%A8%8B-4"><span class="toc-number">10.10.4.2.2.</span> <span class="toc-text">过程</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%BE%E8%AE%A1%E7%AE%97"><span class="toc-number">10.11.</span> <span class="toc-text">图计算</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A6%82%E8%BF%B0-13"><span class="toc-number">10.11.1.</span> <span class="toc-text">概述</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Pregel"><span class="toc-number">10.11.2.</span> <span class="toc-text">Pregel</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9B%BE%E8%AE%A1%E7%AE%97%E6%A8%A1%E5%9E%8B"><span class="toc-number">10.11.2.1.</span> <span class="toc-text">图计算模型</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%9C%89%E5%90%91%E5%9B%BE%E5%92%8C%E9%A1%B6%E7%82%B9"><span class="toc-number">10.11.2.1.1.</span> <span class="toc-text">有向图和顶点</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8B"><span class="toc-number">10.11.2.1.2.</span> <span class="toc-text">计算过程</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#C"><span class="toc-number">10.11.2.2.</span> <span class="toc-text">C++###########</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Pregel%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84"><span class="toc-number">10.11.2.3.</span> <span class="toc-text">Pregel体系结构</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Pregel%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B%E5%92%8C%E5%AE%B9%E9%94%99%E6%80%A7"><span class="toc-number">10.11.2.3.1.</span> <span class="toc-text">Pregel执行过程和容错性</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B"><span class="toc-number">10.11.2.3.1.1.</span> <span class="toc-text">执行过程</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%E5%AE%B9%E9%94%99%E6%80%A7"><span class="toc-number">10.11.2.3.1.2.</span> <span class="toc-text">容错性</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Worker%E3%80%81Master%E5%92%8CAggregator"><span class="toc-number">10.11.2.3.2.</span> <span class="toc-text">Worker、Master和Aggregator</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#Worker"><span class="toc-number">10.11.2.3.2.1.</span> <span class="toc-text">Worker</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#Master"><span class="toc-number">10.11.2.3.2.2.</span> <span class="toc-text">Master</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#Aggregator"><span class="toc-number">10.11.2.3.2.3.</span> <span class="toc-text">Aggregator</span></a></li></ol></li></ol></li></ol></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2021/02/24/JVM-4/" title="JVM-4"><img src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="JVM-4"/></a><div class="content"><a class="title" href="/2021/02/24/JVM-4/" title="JVM-4">JVM-4</a><time datetime="2021-02-24T09:29:43.000Z" title="发表于 2021-02-24 17:29:43">2021-02-24</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/02/23/JVM-3-6/" title="第三章_6-对象的实例化内存布局与访问定位"><img src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="第三章_6-对象的实例化内存布局与访问定位"/></a><div class="content"><a class="title" href="/2021/02/23/JVM-3-6/" title="第三章_6-对象的实例化内存布局与访问定位">第三章_6-对象的实例化内存布局与访问定位</a><time datetime="2021-02-23T08:05:56.000Z" title="发表于 2021-02-23 16:05:56">2021-02-23</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/02/23/JVM-3-5/" title="第三章_5-方法区"><img src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="第三章_5-方法区"/></a><div class="content"><a class="title" href="/2021/02/23/JVM-3-5/" title="第三章_5-方法区">第三章_5-方法区</a><time datetime="2021-02-23T08:05:43.000Z" title="发表于 2021-02-23 16:05:43">2021-02-23</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/02/23/JVM-3-4/" title="第三章_4-堆"><img src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="第三章_4-堆"/></a><div class="content"><a class="title" href="/2021/02/23/JVM-3-4/" title="第三章_4-堆">第三章_4-堆</a><time datetime="2021-02-23T08:05:32.000Z" title="发表于 2021-02-23 16:05:32">2021-02-23</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/02/19/volatile%E4%B8%8E%E4%BC%AA%E5%85%B1%E4%BA%AB%E7%9A%84%E7%90%86%E8%A7%A3/" title="volatile与伪共享的理解"><img src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="volatile与伪共享的理解"/></a><div class="content"><a class="title" href="/2021/02/19/volatile%E4%B8%8E%E4%BC%AA%E5%85%B1%E4%BA%AB%E7%9A%84%E7%90%86%E8%A7%A3/" title="volatile与伪共享的理解">volatile与伪共享的理解</a><time datetime="2021-02-19T05:47:46.000Z" title="发表于 2021-02-19 13:47:46">2021-02-19</time></div></div></div></div></div></div></main><footer id="footer" style="background: #CCFF99"><div id="footer-wrap"><div class="copyright">&copy;2021 By JH</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text"><a target="_blank" rel="noopener" href="https://beian.miit.gov.cn"><img class="icp-icon" src="https://cdn.jsdelivr.net/gh/sviptzk/StaticFile_HEXO@0c02ff8/butterfly/img/icp.png"><span>浙ICP备20004337号-2</span></a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>function loadValine () {
  function initValine () {
    let initData = {
      el: '#vcomment',
      appId: 'OnzLMbp9lfRgDFkeJmhYlqQI-MdYXbMMI',
      appKey: 'hLFiA6vj8fQnQojdG3GOHsyS',
      placeholder: '可匿名评论，但您的评论必须经人工审核通过后才会显示，并可收到相关回复邮件通知，因此邮箱为必填项',
      avatar: 'monsterid',
      meta: 'nick,mail,link'.split(','),
      pageSize: '10',
      lang: 'zh-CN',
      recordIP: false,
      serverURLs: '',
      emojiCDN: '',
      emojiMaps: "",
      enableQQ: true,
      path: window.location.pathname,
    }

    if (true) { 
      initData.requiredFields= ('mail'.split(','))
    }
    
    if (false) {
      const otherData = false
      initData = Object.assign(initData, otherData)
    }
    
    const valine = new Valine(initData)
    window.valine = new Valine({
      el:'#vcomment',
      master:'3B8AFE518CBEB7296252D7E7B6487BC5',
      friends:'',
      enableQQ: true,
      requiredFields: requiredFields,
      emojiMaps: {
                "1":"https://cdn.jsdelivr.net/gh/lete114/CDN/emoji/1.gif",
                "001":"https://cdn.jsdelivr.net/gh/lete114/CDN/emoji/001.png",
                "002":"https://cdn.jsdelivr.net/gh/lete114/CDN/emoji/002.png",
                "003":"https://cdn.jsdelivr.net/gh/lete114/CDN/emoji/003.png",
                "0104":"https://cdn.jsdelivr.net/gh/lete114/CDN/emoji/104.jpg",
                "0105":"https://cdn.jsdelivr.net/gh/lete114/CDN/emoji/105.jpg",
                "0106":"https://cdn.jsdelivr.net/gh/lete114/CDN/emoji/106.gif"
            }
    });
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/gh/HCLonely/Valine@latest/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !false) {
  if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/fireworks.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>